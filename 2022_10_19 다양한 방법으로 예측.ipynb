{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09982386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "svc = SVC()\n",
    "knn = KNeighborsClassifier()\n",
    "lr = LogisticRegression()\n",
    "voting_model = VotingClassifier(estimators=[('knn',knn),('lr',lr),('svc',svc)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c456d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female' 'male']\n",
      "['C' 'Q' 'S']\n",
      "['baby' 'old' 'young']\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.drop(['class', 'alive', 'embark_town', 'who', 'adult_male', 'alone'], axis=1, inplace=True)\n",
    "titanic['family'] = titanic.sibsp + titanic.parch\n",
    "titanic.drop(['sibsp', 'parch'], axis=1, inplace=True)\n",
    "df1 = titanic.copy()\n",
    "df1.embarked.fillna(value='S', inplace=True)\n",
    "age_md = df1.groupby(['pclass', 'sex']).age.agg(['median'])\n",
    "m1_med = df1.loc[(df1.pclass == 1) & (df1.sex == 'male'), 'age'].median()\n",
    "m2_med = df1.loc[(df1.pclass == 2) & (df1.sex == 'male'), 'age'].median()\n",
    "m3_med = df1.loc[(df1.pclass == 3) & (df1.sex == 'male'), 'age'].median()\n",
    "f1_med = df1.loc[(df1.pclass == 1) & (df1.sex == 'female'), 'age'].median()\n",
    "f2_med = df1.loc[(df1.pclass == 2) & (df1.sex == 'female'), 'age'].median()\n",
    "f3_med = df1.loc[(df1.pclass == 3) & (df1.sex == 'female'), 'age'].median()\n",
    "df1.loc[(df1.pclass == 1) & (df1.sex == 'male'), 'age'].fillna(m1_med, inplace=True)\n",
    "df1.loc[(df1.pclass == 1) & (df1.sex == 'male')&(df1.age.isna()), 'age'] = m1_med\n",
    "df1.loc[(df1.pclass == 2) & (df1.sex == 'male')&(df1.age.isna()), 'age'] = m2_med\n",
    "df1.loc[(df1.pclass == 3) & (df1.sex == 'male')&(df1.age.isna()), 'age'] = m3_med\n",
    "df1.loc[(df1.pclass == 1) & (df1.sex == 'female')&(df1.age.isna()), 'age'] = f1_med\n",
    "df1.loc[(df1.pclass == 2) & (df1.sex == 'female')&(df1.age.isna()), 'age'] = f2_med\n",
    "df1.loc[(df1.pclass == 3) & (df1.sex == 'female')&(df1.age.isna()), 'age'] = f3_med\n",
    "df1.drop('deck', axis=1, inplace=True)\n",
    "df1.age_new = 0\n",
    "df1.loc[df1.age >= 50, 'age_new'] = 'old'\n",
    "df1.loc[(df1.age < 50) & (df1.age >= 10), 'age_new'] = 'young'\n",
    "df1.loc[df1.age < 10, 'age_new'] = 'baby'\n",
    "for i in ['sex', 'embarked', 'age_new']:\n",
    "    globals()[f'df1_{i}_encoder'] = LabelEncoder()\n",
    "    globals()[f'df1_{i}_encoder'].fit(df1[i])\n",
    "    df1[i] = globals()[f'df1_{i}_encoder'].transform(df1[i])\n",
    "print(df1_sex_encoder.classes_)\n",
    "print(df1_embarked_encoder.classes_)\n",
    "print(df1_age_new_encoder.classes_)\n",
    "df1.drop('embarked', axis=1, inplace=True)\n",
    "X = df1.drop('survived', axis=1)\n",
    "y = df1.survived\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdaf9da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>family</th>\n",
       "      <th>age_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass  sex   age     fare  family  age_new\n",
       "0         3    1  22.0   7.2500       1        2\n",
       "1         1    0  38.0  71.2833       1        2\n",
       "2         3    0  26.0   7.9250       0        2\n",
       "3         1    0  35.0  53.1000       1        2\n",
       "4         3    1  35.0   8.0500       0        2\n",
       "..      ...  ...   ...      ...     ...      ...\n",
       "886       2    1  27.0  13.0000       0        2\n",
       "887       1    0  19.0  30.0000       0        2\n",
       "888       3    0  21.5  23.4500       3        2\n",
       "889       1    1  26.0  30.0000       0        2\n",
       "890       3    1  32.0   7.7500       0        2\n",
       "\n",
       "[891 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "844f7b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>family</th>\n",
       "      <th>age_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>38.5</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass  sex   age      fare  family  age_new\n",
       "0         3    1  34.5    7.8292       0        2\n",
       "1         3    0  47.0    7.0000       1        2\n",
       "2         2    1  62.0    9.6875       0        1\n",
       "3         3    1  27.0    8.6625       0        2\n",
       "4         3    0  22.0   12.2875       2        2\n",
       "..      ...  ...   ...       ...     ...      ...\n",
       "413       3    1  24.0    8.0500       0        2\n",
       "414       1    0  39.0  108.9000       0        2\n",
       "415       3    1  38.5    7.2500       0        2\n",
       "416       3    1  24.0    8.0500       0        2\n",
       "417       3    1  24.0   22.3583       2        2\n",
       "\n",
       "[418 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f613dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>family</th>\n",
       "      <th>age_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass  sex   age     fare  embarked  family  age_new\n",
       "0         0       3    1  22.0   7.2500         2       1        2\n",
       "1         1       1    0  38.0  71.2833         0       1        2\n",
       "2         1       3    0  26.0   7.9250         2       0        2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ff6b224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>family</th>\n",
       "      <th>age_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>38.5</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass  sex   age      fare  family  age_new\n",
       "0         3    1  34.5    7.8292       0        2\n",
       "1         3    0  47.0    7.0000       1        2\n",
       "2         2    1  62.0    9.6875       0        1\n",
       "3         3    1  27.0    8.6625       0        2\n",
       "4         3    0  22.0   12.2875       2        2\n",
       "..      ...  ...   ...       ...     ...      ...\n",
       "413       3    1  24.0    8.0500       0        2\n",
       "414       1    0  39.0  108.9000       0        2\n",
       "415       3    1  38.5    7.2500       0        2\n",
       "416       3    1  24.0    8.0500       0        2\n",
       "417       3    1  24.0   22.3583       2        2\n",
       "\n",
       "[418 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#테스트 파일 전처리\n",
    "test = pd.read_csv('./test.csv')\n",
    "test['family'] = test.SibSp + test.Parch\n",
    "test.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "test.drop(['Cabin'], axis=1, inplace=True)\n",
    "test.drop(['Name', 'Ticket', 'PassengerId'], axis=1, inplace=True)\n",
    "test.Fare.fillna(value=test[(test.family == 0)&(test.Pclass == 3)].Fare.mean(), inplace=True)\n",
    "test_age_md = test.groupby(['Pclass', 'Sex']).Age.agg(['median'])\n",
    "m1_med = test.loc[(test.Pclass == 1) & (test.Sex == 'male'), 'Age'].median()\n",
    "m2_med = test.loc[(test.Pclass == 2) & (test.Sex == 'male'), 'Age'].median()\n",
    "m3_med = test.loc[(test.Pclass == 3) & (test.Sex == 'male'), 'Age'].median()\n",
    "f1_med = test.loc[(test.Pclass == 1) & (test.Sex == 'female'), 'Age'].median()\n",
    "f2_med = test.loc[(test.Pclass == 2) & (test.Sex == 'female'), 'Age'].median()\n",
    "f3_med = test.loc[(test.Pclass == 3) & (test.Sex == 'female'), 'Age'].median()\n",
    "test.loc[(test.Pclass == 1) & (test.Sex == 'male'), 'Age'].fillna(m1_med, inplace=True)\n",
    "test.loc[(test.Pclass == 1) & (test.Sex == 'male')&(test.Age.isna()), 'Age'] = m1_med\n",
    "test.loc[(test.Pclass == 2) & (test.Sex == 'male')&(test.Age.isna()), 'Age'] = m2_med\n",
    "test.loc[(test.Pclass == 3) & (test.Sex == 'male')&(test.Age.isna()), 'Age'] = m3_med\n",
    "test.loc[(test.Pclass == 1) & (test.Sex == 'female')&(test.Age.isna()), 'Age'] = f1_med\n",
    "test.loc[(test.Pclass == 2) & (test.Sex == 'female')&(test.Age.isna()), 'Age'] = f2_med\n",
    "test.loc[(test.Pclass == 3) & (test.Sex == 'female')&(test.Age.isna()), 'Age'] = f3_med\n",
    "test.loc[test.Age >= 50, 'age_new'] = 'old'\n",
    "test.loc[(test.Age < 50) & (test.Age >= 10), 'age_new'] = 'young'\n",
    "test.loc[test.Age < 10, 'age_new'] = 'baby'\n",
    "test.rename(columns={'Pclass':'pclass','Sex':'sex','Age':'age', 'Fare':'fare', 'Embarked':'embarked'}, inplace=True)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for i in ['sex', 'embarked', 'age_new']:\n",
    "    globals()[f'new_test_{i}_encoder'] = LabelEncoder()\n",
    "    globals()[f'new_test_{i}_encoder'].fit(test[i])\n",
    "    test[i] = globals()[f'new_test_{i}_encoder'].transform(test[i])\n",
    "test.drop('embarked', axis=1, inplace=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74d5bee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train,y_train)\n",
    "knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8aacfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = knn.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d240b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1acd4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "49569a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 정확도: 0.8000\n",
      "KNeighborsClassifier 정확도: 0.7333\n",
      "보팅 분류기의 정확도:  0.8222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "voting_model = VotingClassifier(estimators=[ ('LogisticRegression', logistic_regression),\n",
    "                                            ('KNN', knn) ], voting='soft')\n",
    "\n",
    "classifiers = [logistic_regression, knn]\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    class_name = classifier.__class__.__name__\n",
    "    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))\n",
    "voting_model.fit(X_train, y_train)\n",
    "pred = voting_model.predict(X_test)\n",
    "print('보팅 분류기의 정확도: {0: .4f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f73873d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 정확도: 0.8000\n",
      "KNeighborsClassifier 정확도: 0.7111\n"
     ]
    }
   ],
   "source": [
    "classifiers = [logistic_regression, knn]\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    class_name = classifier.__class__.__name__\n",
    "    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))\n",
    "voting_model.fit(X, y)\n",
    "pred = voting_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eea521e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,ExtraTreesClassifier\n",
    "ran_clf = RandomForestClassifier(n_estimators=400, max_depth=5)\n",
    "ran_clf.fit(X_train, y_train)\n",
    "pred = ran_clf.predict(X_test)\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6f991a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_clf.fit(X, y)\n",
    "pred = ran_clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "47689cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tit = pd.read_csv('test.csv')                          \n",
    "tit.drop(list(tit.columns)[1:], axis = 1, inplace=True) \n",
    "tit['Survived'] = pred\n",
    "tit.set_index('PassengerId', inplace=True)\n",
    "tit.to_csv('tit_test(3).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fb6427b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ce807a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8444444444444444"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(n_estimators=200)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "pred = ran_clf.predict(X_test)\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a065df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier()\n",
    "bag_clf.fit(X, y)\n",
    "pred1 = ran_clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd1640c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tit = pd.read_csv('test.csv')                          \n",
    "tit.drop(list(tit.columns)[1:], axis = 1, inplace=True) \n",
    "tit['Survived'] = pred1\n",
    "tit.set_index('PassengerId', inplace=True)\n",
    "tit.to_csv('tit_test(5).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c9cb28cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>family</th>\n",
       "      <th>age_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass  sex   age     fare  family  age_new\n",
       "0         3    1  22.0   7.2500       1        2\n",
       "1         1    0  38.0  71.2833       1        2\n",
       "2         3    0  26.0   7.9250       0        2\n",
       "3         1    0  35.0  53.1000       1        2\n",
       "4         3    1  35.0   8.0500       0        2\n",
       "..      ...  ...   ...      ...     ...      ...\n",
       "886       2    1  27.0  13.0000       0        2\n",
       "887       1    0  19.0  30.0000       0        2\n",
       "888       3    0  21.5  23.4500       3        2\n",
       "889       1    1  26.0  30.0000       0        2\n",
       "890       3    1  32.0   7.7500       0        2\n",
       "\n",
       "[891 rows x 6 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a30c378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 정확도: 0.8778\n",
      "KNeighborsClassifier 정확도: 0.7111\n",
      "보팅 분류기의 정확도:  0.8222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "voting_model = VotingClassifier(estimators=[ ('LogisticRegression', logistic_regression),\n",
    "                                            ('KNN', knn) ], voting='soft')\n",
    "\n",
    "classifiers = [logistic_regression, knn]\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    class_name = classifier.__class__.__name__\n",
    "    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))\n",
    "voting_model.fit(X_train, y_train)\n",
    "pred_vm = voting_model.predict(X_test)\n",
    "print('보팅 분류기의 정확도: {0: .4f}'.format(accuracy_score(y_test, pred_vm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a38e7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_model.fit(X,y)\n",
    "pred_vm = voting_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ce747120",
   "metadata": {},
   "outputs": [],
   "source": [
    "tit = pd.read_csv('test.csv')                          \n",
    "tit.drop(list(tit.columns)[1:], axis = 1, inplace=True) \n",
    "tit['Survived'] = pred_vm\n",
    "tit.set_index('PassengerId', inplace=True)\n",
    "tit.to_csv('tit_test(6).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "64e0201e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(C=1.0, degree=1, kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "pred_svc = svc.predict(X_test)\n",
    "accuracy_score(y_test, pred_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ef78c65e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc.fit(X, y)\n",
    "pred_svc = svc.predict(test)\n",
    "tit = pd.read_csv('test.csv')                          \n",
    "tit.drop(list(tit.columns)[1:], axis = 1, inplace=True) \n",
    "tit['Survived'] = pred_svc\n",
    "tit.set_index('PassengerId', inplace=True)\n",
    "tit.to_csv('tit_test(7).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8589b98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAAIuCAYAAADnru5GAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACqUUlEQVR4nOzdf3zP9f7/8dtzfm1+bShU5yTFUUL6ZcZs721+hqEQJVSaOJS+55TOh1ZEpQ6lomh+RnX6QSTE2HQWCvmdI522OhLlV/JrP5/fP968826Tbba93u/tfr1cXpfs9Xq9X6/H652Xx2uP1/OHsdYiIiIiIiIiIiLOC3A6ABERERERERERcVOhRkRERERERETER6hQIyIiIiIiIiLiI1SoERERERERERHxESrUiIiIiIiIiIj4CBVqRERERERERER8hAo1IiIiIiIiIiI+QoUaEREREREREREfoUKNiIiIiIiIiIiPUKFGRERERERERMRHqFAjIiIiIiIiIuIjVKgREREREREREfERKtSIiIiIiIiIiPgIFWpERERERERERHyECjUiIiIiIiIiIj5ChRoRERERERERER+hQo2IiIiIiIiIiI9QoUZERERERERExEeoUCMiIiIiIiIi4iNUqBERERERERER8REq1IiIiIiIiIiI+AgVakREREREREREfIQKNSIiIiIiIiIiPkKFGhERERERERERH6FCjYiIiIiIiIiIj1ChRkRERERERETER6hQIyIiIiIiIiLiI1SoERERERERERHxESrUiIiIiIiIiIj4CBVqRERERERERER8hAo1IiIiIiIiIiI+orzTAYiIiPiLoKCg/adPn67jdBylRWBg4IFTp07VdToOEREREV9irLVOxyAiIuIXjDFWebPoGGOw1hqn4xARERHxJer6JCIiIiIiIiLiI1SoERERERERERHxESrUiIiIlGHz58+nVatWdOnShWPHjnltS05Opl69erhcLvr37+9QhCIiIiJliwo1IiIipUhmZiaZmZn53vf111/n008/5Z577mHatGm59rnnnntITk5m7ty5RR2qiIiIiORBhRoRERGHrF27ltDQUKKjo5k5cyYAY8eOxeVyER0dTVpaGh999BGPPvooOTk5dOzYkf/97395HistLY3/+7//Izo6ml9++SVf5//6669p2rQp5cuXp23btqxfvz7XPm+//TZt2rTh7bffLvyFioiIiEi+aXpuERERhyxdupQJEybgcrmw1rJ9+3Z++OEHkpOT2bVrF88++yzTpk3jvffeIy4ujq5du/LnP/851zESEhKoXLky999/P8888wwA33//fa7uSldccQXz58/3/Hz06FGqV68OQHBwMEeOHPHa/5ZbbuE///kPGRkZtG3blrZt23LppZcWx1chIiIiImeoUCMiIuKQoUOHMm7cOGbMmMHw4cNJS0sjOTkZl8sFwGWXXQbA4MGD6dSpE1OmTMl1jHnz5pGVlcXgwYNp06aNZ/2VV15JcnLyH54/JCTEMy7NsWPHCAkJ8dpetWpVACpUqEBERAR79uxRoUZERESkmBlrrdMxiIiI+AVjjC3KvHnq1CmCgoLYt28f999/P8899xwJCQm88sorgHsMmXLlynHbbbfRrl07jh8/zpNPPpnrOKmpqUybNo3169cTGxvLX//6Vw4cOHDBFjWZmZnExMSQlJTEBx98QFpaGo899phn+7Fjx6hevTrZ2dm0a9eOefPmcfnllxfZ9RtjsNaaIjugiIiISCmgFjUiIiIOmTZtGgsWLOD48eOMHDmSG264gbp16+JyuTDG0LdvX06ePEmPHj0YPHgwPXv2ZOfOnVx//fVex6lfvz7PPfccGRkZLFiwgBMnTuSrRU2FChV44IEHaNOmDTVq1OCtt94CYMSIEUycOJF3332X6dOnExAQQN++fYu0SCMiIiIieVOLGhERkXwq6hY1ZZ1a1IiIiIjkplmfRERERERERER8hAo1IiIiIiIiIiI+QoUaEREREREREREfoUKNiIiID9i/fz/jx48/7/bhw4cX6rirV68mLCyMqKgo9u7dm+c+kyZNIjw8HIAtW7bgcrlwuVzUr1+fl156CYBGjRp51n/11VeFikVERERELkyDCYuIiOSTPw4mHBUVxeLFi/nqq6+YO3cuU6ZM8dqenp5OXFwc//3vf0lJSfHa1q1bNyZOnEiDBg0IDw/Ptf1iaTBhERERkdzUokZERKSEpaenExsbS8eOHenTpw+zZ88mLS2Nfv36ARAWFsawYcNo3rw5y5cvB/C0eCmIkydPEhQURLVq1QgNDc2zJUxCQgIDBgzItf7EiRPs37+fBg0aAHD48GEiIiIYPHgwp0+fLnAsIiIiIpI/KtSIiIiUsA8//JBWrVqxfPlyatSokWv7oUOHiI+P5+OPP2batGl5HiMjI8PTFensEhMT47XPkSNHqF69uufn7Oxsr+2ZmZmsWbOG6OjoXMdftmwZHTt29PyckpLCp59+Sr169Zg+fXqBrldERERE8q+80wGIiIiUNampqTRr1gyA5s2b59p+6aWXUrt2bQCOHj2a5zEqVqxIcnLyH56nRo0aHDt2zPNzQID3+5k333yTu+66K8/PLly4kMcee8zzc82aNQHo0aMHL7744h+eV0REREQKTy1qRERESlj9+vXZvn07ANu2bcu13Zjfhm0535g4+WlRU7lyZU6dOsXx48f54osvaNy4sdf23bt389prr9GxY0d27tzJK6+8Arhb2uzatYsbbrjBc6709HQAPvvsM6655ppCXrmIiIiIXIha1IiIiJSw7t2706tXLzp06EDVqlWpUKFCgY+RnxY1AKNGjaJdu3YEBgYyZ84cAJ577jnuueceJkyY4NkvPDzcM7PU6tWrvbpDHTlyhE6dOlG1alVq1KjBvHnzChyviIiIiOSPZn0SERHJp6Kc9SkrK4vy5cszZMgQ+vfvT1hYWJEc159o1icRERGR3NSiRkRExAGdO3fm+PHjNGjQoEwWaUREREQkb2pRIyIikk9F2aJG1KJGREREJC8aTFhERMQPhYeHF9uxzw5OfOONN9K9e3cAxo8fz+WXX87o0aM9+40ZM4awsDDCwsJYtWpVscUjIiIiUpao65OIiIh4OTtI8Ysvvki1atUAGDRoEK1atfIqyPTv358nn3ySo0ePEhsbm2vWKREREREpOLWoERERKSZr164lNDSU6OhoZs6cSWZmJjExMURERHDHHXeQnZ1NWloa0dHR9O7dmxtvvJEFCxbQvn17wsPDOXHiBGlpaURFRdGtWzdatmxJamqq1zk+//xzXC4XrVu3ZtasWQAMGDCAyMhIoqKiyMnJKXT8ixcvplu3bgDUqVPHa9pwcE8zDlCpUqVc20RERESkcNSiRkREpJgsXbqUCRMm4HK5ODu2zZIlSwgKCmL06NGsXr2ahg0bcuTIERITE3nnnXeYM2cOK1as4JlnnuGTTz7hpptuYv/+/axcuZJNmzYxYcIEXn/9dc854uPjWbx4MdWqVaNdu3bcdddd7N27lzVr1mCt9SqgZGRk0L59e68Yy5Url2e3pZ9++gljDJdeeukFr/Opp55i8ODBhf2aREREROQcKtSIiIgUk6FDhzJu3DhmzJjB8OHDady4MXFxcfzwww8cOHCAhg0b0rBhQxo3bkxAQACXX345TZo0AeDyyy/nyJEjADRt2pTy5cvTvHlzvvnmG69zbN26ldjYWAAOHjzIwYMHGTBgAP369aNevXo8/fTTBAS4G9BWrFjR063pQhYtWuRpTfNHFi5cyKFDh7jrrrvy+7WIiIiIyB9QoUZERKSY1KhRg6lTp7Jv3z7uv/9+Bg0axF/+8hfeeustRo0a5Wllc26rl3P/fHb7jh07yM7OZuvWrVxzzTVe57jxxht5//33qVKlCpmZmQQEBNC3b1/69+9PXFwcGzZsIDQ0FChYi5oPP/yQqVOn/uH1bdu2jSlTpvDxxx8X4FsRERERkT+iQo2IiEgxmTZtGgsWLOD48eOMHDmS0NBQxo8fz8aNGwkODqZhw4b5Ok7t2rXp3r07P//8M/Pnz/faNmbMGGJjY8nJyaFmzZrMmDGD2NhYsrOzqV69Ok2bNvXsm98WNceOHePo0aPUq1fPs27GjBlMnTqVw4cPc+TIEaZMmcKjjz7KgQMH6NChA8HBwSxatCh/X4yIiIiInJc5+7ZORERE/pgxxpZ03kxLS2P06NHMmzevRM9bEowxWGs1CrGIiIjIOTTrk4iIiIiIiIiIj1CLGhERkXxyokVNaaYWNSIiIiK5qUWNiIhIMXvqqadITEwstuMPHDiQ0NBQfvnlF8+6hx56iH79+gHuwYhbtWpFmzZtuPfeez2DFAcHB+NyuXC5XBw+fDjPY6emptKmTRsiIiK46667yM7OZv/+/Z7PXXfddYwYMQLAs+7GG2+ke/fuAIwePZqQkBCysrKK7fpFREREShMNJiwiIlIKzJ8/n+DgYAAOHDhAWloa1atXB6BRo0asXbsWgHvvvZeNGzdy66230rRp0wsOLhwSEsJHH31ESEgIo0aNYunSpXTt2tXzuYcffpguXboAeNa9+OKLVKtWDYBx48aRkpJSxFcrIiIiUnqpRY2IiEghPfDAA+zatQuAl19+mffee4/ly5cTGRnJLbfcwty5c732nz17NgkJCYC7lU1ycjLWWoYMGUJ0dDSdO3fmyJEjFx3Xiy++yPDhwz0/V6hQwfPnSpUq8ec//xmAXbt20aZNGx5//HHO16WrRo0ahISEAFC+fHnKlSvntf3TTz/F5XJ5rVu8eDHdunW76OsQERERKYtUqBERESmknj178v777wOwfPlybrvtNiIiIlizZg3r169n+vTpFzzGkiVLuPLKK1m9ejXDhg3j9ddf99o+duxYT5eis8vKlSvPe7zDhw/z888/55r6e/HixTRp0oSffvqJWrVqAbBnzx4+/fRTjhw5wkcfffSHce7bt4/ExETat2/vWbdx40aaNWtG+fK/NdD96aefMMZw6aWXXvDaRURERCQ3FWpEREQKKSYmhqSkJH766SeqVatGlSpV2LRpE23btiUmJoadO3d67W/Mb+Pmnm3BsmvXLt555x1cLhfjx4/PNVZMfHw8ycnJXku7du3OG9PkyZMZNmxYrvWxsbHs2LGDK664giVLlgBQs2ZNjDF0796dHTt2nPeY6enpDBgwgDfeeMOrKLNw4UJuv/12r30XLVqk1jQiIiIiF0Fj1IiIiBRS+fLlueqqq3jhhRc8g+c+//zzJCQkcMUVV+Rq1RIcHMz27dsB2L59O1FRUTRq1Ij+/fvzt7/9DYDMzEyvz4wdO5bVq1d7rRs1atR5izWpqan84x//4NSpU+zZs4d3332Xbt26UalSJQCqV69OUFAQJ06cIDAwkHLlyvHZZ5/RtGlTAH744QeuuOIKr2PGxcUxdOhQGjdu7LV+xYoVjB492mvdhx9+yNSpU//wexMRERGR81OhRkRE5CL07NmT3r178+OPPwLQo0cPunXrRvPmzalRo4bXvjExMbzwwgts2LDB0zIlNjaWhx56iOjoaABGjBhBbGys5zPx8fHEx8fnO56z4+KkpaUxevRoevfuzaJFi5g0aRIADRs2pH379mzbto377ruPKlWqcPXVVzNmzBgA7r77bq8BhtetW8eCBQv47rvvmDx5Mg8//DA9evRg9+7d1KtXj6CgIM++x44d4+jRo9SrVy/f8YqIiIiIN3O+wQNFRETEmzHG+mLe/Pvf/8769ev5+OOPPTM/FcahQ4d4+eWXPUWbojB69Gjef/99du7cmWsgYmMM1lpzno+KiIiIlEkq1IiIiOSTrxZq/JUKNSIiIiK5aTBhEREREREREREfoUKNiIiIiIiIiIiPUKFGRETEh7lcLrKyskrkXL/++itdu3aldevWnkGJRURERKRkqVAjIiJyAcYt7/mwS5E33niDvn378umnn5KQkEBGRkaxn9MYU/jRj0VERERKIRVqREREzsMYE2SMeQDYAUwq7vPl5OQwaNAgIiMj6dSpk9e25cuXExkZyS233OJp7TJlyhRatmxJVFQUX375JQsXLqRFixZER0ezdOnSAp9/3bp1tG3blnLlynHDDTewe/fuIrmuC0g1xkw2xjQoiZOJiIiI+LryTgcgIiLia4wxlwN/BR4APgceAlYDOcV53kWLFlG7dm0SEhLIyfE+VUREBB07diQrKwuXy0X//v1ZtGgRSUlJBAUFYa1l0qRJvPvuu1x11VX8fnaqsWPHsnr1aq91o0aNol273xoKHT16lOrVqwMQHBzMkSNHiulKvTTD/V2vM8asBV4CkjW9loiIiJRVKtSIiIicYYy5BRgBdAbmA+HW2q/P2V6s5//6669p1aoVAAEB3o1eN23axJgxY8jMzGTnzp0AjBkzhiFDhlCxYkWefvppRo8ezbhx48jKymLUqFE0bNjQ8/n4+Hji4+P/8PwhISEcO3aMwMBAjh07RkhISNFeYB6stXuBfxhjngbuAaYC6caYl4B3rLWniz0IERERER+irk8iIlKmGWPKG2PuMMakAB8AW4CrrbXDzi3SlIRGjRqxfv16gFwtap5//nkSEhJITEwkONg9rEvz5s2ZPXs2LpeL2bNnU69ePRISEoiLi2PSJO+eWmPHjsXlcnktK1eu9NonLCyMVatWkZ2dzZYtW2jUqFExXq03a+1Ja+004HrgcaAPkGaMecoYU6fEAhERERFxmFHLYhERKYuMMSHA/cBw4AfcXW4WWmvPO8WSMaZYe+Tk5OTwwAMPsGfPHqpWrcrSpUtxuVwkJiYyd+5cJk+eTPPmzdm2bRubN29mwIABpKamkp6ezqxZs5gzZw7r16/n+PHjTJw4EZfLVaDzHzt2jLvuuovDhw8TFxfHwIEDi+U6zzLGYK09bzMlY0xj3N3O7gQ+BCZba7cUa1AiIiIiDlOhRkREyhRjTEPcv/zfDSzD/cv/F/n8rIZOKUIXKtScs18tIA73WDZ7cBfVllhrs4s3QhEREZGSp0KNiIiUesY9uEwU7vFnwoA3gKlnxkcpyHFUqClC+S3UnLN/BaAn8AhQE3gZmGWt/bWYQhQREREpcSrUiIhIqWWMCQT64i7QVMDdEmOetfZkIY+nQk0RKmih5pzPGdwFtxFADDAHeMVam1q0EYqIiIiUPA0mLCIipY4xpq4xZgzwHdAbeAy43lo7vbBFGvEd1m2ttbY3cBOQDWw0xnxgjGljint6LhEREZFipEKNiIiUGsaYG40xc4BdQG3AZa3tZK39RE1hSidr7XfW2keBesBqYAbuos09xpiKzkYnIiIiUnDq+iQiIn7NGFMO6Iq7G0wD4FXgDWvtoWI4l+o9RaiwXZ8ucMwAoBPucWwaA1OBadban4vyPCIiIiLFRYUaERHxS8aY6sB9uGdw+hl4EfjAWptZXOcMCgraf/r06TrFdfyyJjAw8MCpU6fqFtfxjTFNgYeBO4APcM/wtb24ziciIiJSFFSoERERv2KMuRoYDgwAVgIvWWvXORuV+DJjzKXAg8BQYAfuQaWXWWtznIxLREREJC8q1IiIiM87MzhsG9zdWdrgHodkirX2e0cDE79ijKmEe3DpR4CqwGRgjrX2uKOBiYiIiJxDhRoREfFZZ36xvhP3+DNVcP9iPVe/WMvFOFP4C8ddsIkAZgKvqvAnIiIivkCFGhER8TnGmNrAYNRVRYrZma50w3B3pUvE/XdtvUaNFhEREaeoUCMiIj7jzOCvI4DbgfdxD/66w9GgpEw4Mzj1vbgHpz6Iu2DzfnEOTi0iIiKSFxVqRETEUWemU74Nd4FG0ymLo85M994F99/HhhTjdO8iIiIieVGhRkREHGGMqQoMxN2C4Vfc02u/a63NcDIukbOMMc1xF2y6Af/C3cJrl5MxiYiISOkX4HQAIiJSthhj6hljXgC+A6KA+4FbrLXzVKQRX2Kt3WKtHQhcB+wHkowxy4wxHc4MSCwiIiJS5NSiRkREit2ZX2rDcM+yEw3Mxj3LTqqTcYkUhDEmEOiD++9xBdyzkL1prT3paGAiIiJSqqhQIyIixcYYUxHoibv7SE3gZWCWtfZXJ+MSuRhnCo8u3H+vWwFvAFOstT84GJaIiIiUEirUiIhIkTPG1MI9vfZfgd24Z9D52Fqb7WRcIkXNGNMA9zhL/YDlwEvW2i+cjUpERET8mcaoERGRImOMaWyMmQZ8g3vGnM7W2mhr7WIVaaQ0stZ+Y619CLga2Ai8a4z5zBjTyxhT3uHwRERExA+pRY2IiFyUM9Nrt8c9bscNwGvA69baA44GJuKAM8WZWNz3w5W4p/dOsNYecTQwERER8Rsq1IiISKEYY6oA9wAPA+m4uze9ba1NdzIuEV9hjLkF9/3RGXgLeNla+7WzUYmIiIivU9cnEREpEGPMn4wxzwJpQEdgCHCjtXa2ijQiv7HWbrTW3gM0AY4CKcaYJcaYtpreW0RERM5HLWpERCRfjDGhuGe56QDMw9064BtHgxLxI8aYINyDDj8MWNyt0N6y1p5yMi4RERHxLSrUiIjIeZ0Zb+MO3AWaurin155prf3FybhE/NmZ1jQxuMexuQWYDky11v7oaGAiIiLiE1SoERGRXIwxNYAHgGG4uzi9BCzSzE0iRcsY0wj39N53AR/hnt77S2ejEhERESdpjBoREfEwxjQyxkwFvsU9rkYPa22EtXaBijQiRc9au9ta+1fc03vvAD40xnxqjLndGFPO4fBERETEAWpRIyJSxp3phtEWd/emW4FpqBuGiCOMMRWAHrjvx8tQd0MREZEyR4UaEZEy6szApnfj/oVQA5uK+JjfDeA9F3jFWvtfR4MSERGRYqdCjYhIGWOMuRwYCsQBX+Au0KyySggiPskY8yfgr8Ag4DPc9+wa3bMiIiKlkwo1IiJlhDHmZtxv5zsDb+GeXvtrR4MSkXwzxlQB7sF9H5/CXbB5x1qb7mBYIiIiUsRUqBERKcXODEbaHfcvdvWAV4AEa+0RB8MSkYtgjAkA2uOe3vsG4DXgdWvtAUcDExERkSKhQo2ISClkjAkG7geGA/twv3lfaK3NcjIuESlaxpjrcU/v3RtYCEy21m51NioRERG5GJqeW0SkFDHGNDDGvAykArcAd1prW1tr31ORRqT0sdbutNYOBhoCe4ClxpjVxphYTe8tIiLin9SiRkTEz52ZXtuFu3tTK+AN3NNr73UwLBFxgDGmItATd7eoGsBkYLa19ldHAxMREZF8U6FGRMRPGWMCgb64CzQVcXdvetNae9LBsETEB5wp4IbhLthEA7NxT++d5mBYIiIikg8q1IiI+BljTB1gCPAgsBl3gWaltTbHybhExDcZY+oBw4D7gCTc/2Z8pum9RUREfJMKNSIifsIY0xx365luwL9wT6/9lZMxiYj/MMZUBQYADwPHcBds3rXWZjgZl4iIiHhToUZExIedGQy0C+4CTUPgVeANa+0hJ+MSEf91Znrv23D/u3IdMBWYZq096GRcIiIi4qZCjYiIDzLGVMPdTeEh4BDwIvC+tTbT0cBEpFQxxjTFXbC5HXgf9/TeOxwNSkREpIzT9NwiIj7EGFPfGDMJSMM9g1M/INRa+7aKNCJS1Ky126219wONgO+BlcaYFcaY2860vBEREZESphY1IiIOOzM7Sxvcb7UjgJnAq9ba752MS0TKHmNMJaA37tmiquCe3nuOtfaEo4GJiIiUISrUiIg4xBhTEbgTd4GmGu6BPedaa487GJaISF4F5BnAFBWQRUREip8KNSIiJcwYcynuqbWHADtxF2iWaXptEfFFxpirgeG4Z4xaCbxorV3vbFQiIiKllwo1IiIl5MygnQ8DdwAf4B60c7uzUYmI5I8xpjpwL+5Bzn/GXWT+QONniYiIFC0VakREitGZwTg74e4+cD2/TYP7s5NxiYgUljGmHNAF9zg21wCvAm9Yaw87GpiIiEgpoUKNiEgxMMZUxd1N4GHgOO7ptf9lrc1wNDARkSJkjLkR979z3YB3cLcU/I+zUYmIiPg3TbsoIlKEjDFXGmOexz29djRwP3CztfZNFWlEpLSx1m621g4ErgMOAMnGmGXGmPZnBiQWERGRAlKLGhGRi3Tml5Ew3N2bYoA5wCvW2lQn4xIRKWnGmECgL+5/D8vjnt57nrX2pJNxiYiI+BMVakRECskYUwHoifsXkktw/0Iy21p7zMm4REScdqaA7cI9jk1L4A1gqrX2ByfjEhER8Qcq1IiIFJAxphYQB/wV2IN75pMl1tpsJ+MSEfFFxpiGuKf37gcsA16y1m5wNioRERHfpTFqRETyyRhznTHmdeAb4C9AF2ttlLV2kYo0IiJ5s9busdY+BFwNbALeM8Z8ZozpaYwp73B4IiIiPkctakRE/sCZ5vsdcHdvag68DrxmrT3gYFgiIn7rTHGmG+5/V68EXgESrLVHHQxLRETEZ6hQIyKSB2NMZeAe3NPOZuKeXvsda+1pRwMTESlFjDG34C7Y3AbMB1621u5xNCgRERGHqeuTiMg5jDF/MsY8C3yH+xeHoUBza+1sFWlERIqWtXajtbYf0AT4BfjMGPORMSZG03uLiEhZpRY1IiKAMaYF7re6HYE3cU+v/Y2jQYmIlDHGmCDgbtz/HufgHqz9LRXKRUSkLFGhRkTKrDPjJNyO+xeCy4GXgRnW2l+cjEtEpKw705qmLe5/n28BpuEeH+xHJ+MSEREpCSrUiBSjoKCg/adPn67jdBz+KDAw8MCpU6fqFsexjTE1gEG4p4tNw/3GdrG1Nqs4ziciIoVnjGmEe7ywvsBiYLK19sviPKfyd+EVZ/4WESkrVKgRKUbGGKt7rHCMMVhri3R8gjMP+w8BdwFLgJestZuK8hwiIlI8jDE1cRfZhwGp/FZkzy6Gcyl/F1Jx5G8RkbJGhRqRYqQHvcIrqge9M83nY3A3n2/Bb83n913ssUVEpOQZYyoAPYBHgDq4u63OtNYeK8JzKH8Xkgo1IiIXT4UakWKkB73Cu9gHvTMDUt6Fu0BjcL95nW+tPVUkAYqIiOOMMS1xd4vqAMzFPb33t0VwXOXvQlKhRkTk4ml6bhE/M3DgQL75xvnJiMaMGUNYWBhhYWGsWrUq1/ZGjRrhcrlwuVx89dVXJRaXMeYyY8zTuMeeOfvGtam1NkFFGhGR0sVau95a2xe4ATgNfGGMWWiMiXR6eu/Zs2cze/bsfO9f3Pl9woQJtG3bFpfLRU5Ojte2HTt2EB4eTuvWrdm2bVuxxSAiIvlT3ukARMR3nDhxgipVquRr3/79+/Pkk09y9OhRYmNjiYmJ8dp+6aWXkpycXAxR5s0YcxPu1jNdgLeBCGvt7hILQEREHGOt/R/w+JlC/T24u7meNMa8BPzLWpvuZHzFoSA5e8OGDRw/fpzExMQ8tz/xxBO8/fbbBAQEMHToUBYtWlSUoYqISAGpRY2Ij0hOTqZr16506tSJ6OhoDh8+DMDo0aNp3bo10dHRHD161LP/li1biIyMpGXLljzzzDMALFy4kBYtWhAdHc3SpUtZu3YtoaGhREdHM3PmzDzPe+rUKebMmUO7du0K9GBWv359ACpVqkReLy0PHz5MREQEgwcP5vTp0/k+bkEYY8oZY243xnwKfAhsB66x1v5VRRoRkbLHWnvCWvs60BgYBdwNpBlj4o0xtYv7/BkZGXTr1o2OHTvyySefeNaPHTsWl8tFdHQ0aWlpwPnze1JSEnfffTeZmZm5jm+tZeXKlfTu3Zv4+Ph8x/XRRx9x6NAhoqKiGDt2bK7thw8f5s9//jNXXHEFv/zyS/4vWEREioVa1Ij4kNOnT7Ny5Ur+9a9/MX36dDp06MC3337LZ599xu/7yjdq1Ijk5GSMMURFRfHII4/wwQcf8O6773LVVVdhreWJJ55gwoQJuFyuXJ9PS0vj5ZdfZseOHdxxxx0sWLCAatWqATB06NBc3ZVeeeUVmjZtmivmp556isGDB+dan5KSQs2aNXnmmWeYPn06Dz300EV9N8aYVsCfrbX/MsYEA/fhnsHpR9zjzyzQ9NoiIgJgrc0BlgHLjDHX484Xu40xC3HP+LfNGFMXGAhMKKoBaT788ENatGjBqFGjiIuLA2D79u388MMPJCcns2vXLp599lkefPDBPPP7mjVrWLVqFXPmzKFChQqe9b/++iuvv/46y5YtIyIigokTJ/LnP/8ZgOnTp/PWW295xXH//fdzzz33eH4+cOAAtWrVIikpiT59+vDll19y0003ebaf2xXq992iRESk5KlQI+JDbrzxRgCaN2/OypUrqV+/Pq1atQLI1WolNTWVv/3tb5w8eZLdu3fz008/MXr0aMaNG0dWVhajRo1i6NChjBs3jhkzZjB8+HBatGjh+fyGDRtITEzk4Ycfpk+fPl7Np6dOnZqveBcuXMihQ4e46667cm2rWbMmAD169ODFF18s2BfxO8aYa4AFwN+MMZNxN2tfDvSx1n5+UQcXEZFSzVq7ExhsjBkFxOEu3vwHmArcCWQB/yyKc3377beeXH7zzTcDsGvXLpKTk3G5XABcdtllfP3113nm9zFjxrBq1SqvIg3Avn37mDNnDr169WLQoEFcccUVnm1xcXGeotD5BAcHExkZCUBUVBS7du3yKtQEBATk+WcREXGG/iUW8SFbt271/Peaa66hUaNGrF+/3rP93Ldur732GiNHjmTNmjU0aNAAay316tUjISGBuLg4Jk2aRI0aNZg6dSoTJkzgySef9DpXr169+PzzzwkICKB79+48+OCDfP3114C7Rc3ZgYDPLtu3b/f6/LZt25gyZQpTpkzJdR0ZGRmkp7uHA/jss8+45pprCv2dnGk9swo4gLvlzEmgmbX2LhVpREQkv6y1B621zwD1gRnA40AIMNoY07MozlG/fn1PLt+8eTPgbgHbvn17kpOTSU5OZu7cuefN77Nnz2bIkCEcPHjQ67iNGjVi+/bttGzZkhEjRnDHHXd4BvKfPn16rpz95ptven2+VatWnkGCt2zZ4um+fFbNmjXZu3cv+/btIzg4uCi+ChERuQhqUSPiQypUqEDHjh05ffo0H3zwAbVq1aJevXq0bt2aSpUqsWDBAs++nTt3ZtiwYTRu3JiKFSsC7m5I69ev5/jx40ycOJFp06axYMECjh8/zsiRI3OdLygoiHvvvZd7772XLVu28OOPP/KXv/wlXy1qHn30UQ4cOECHDh0IDg5m0aJFLF++nOzsbG655RY6depE1apVqVGjBvPmzbuYr+VdoB7uQs3/gEustXsv5oAiIlJ2WWszjDHHcbeksUAV4G1jzAcX2wWqe/fu9OzZkw4dOlCjRg0AbrjhBurWrYvL5cIYQ9++fYmLi8szv1955ZW8/PLL9OvXj/fff5+qVat6jm2MoUOHDnTo0IF9+/bx2WefAflrUdOlSxceeOABIiMjufbaa2nVqhX79+9nxowZjBo1ijFjxtCnTx+stXm+gBERkZJliqhLrojkwRiT72e+5ORkEhMTGTduXDFH5R+MMVhrjTGmAlAbqHZmOWGtLbn5vkVEpNQxxtQErgV+BY4Dh621v5yzvaiGrSlzzuZvp+MQEfFnalEjIj7NWpsJ/OB0HCIiUnpYaw8Da52OQ0REJC9qUSNSjPRGrvD0Rk5ERJyi/F14yt8iIhdPgwmL+JH9+/czfvz4824fPnx4oY67evVqwsLCiIqKYu9e7+FfduzYQatWrWjTpg333nuvZ8DD4cOH43K5uO+++8jOzgbcgx2eHcjw99N7i4iIlFW+kr8zMzMJCwujatWqfPPNN559n3vuOcLDw7njjjs4ceJEoWIREZGioxY1IsXIX97IRUVFsXjxYr766ivmzp3rNZBgZmamZ5rQe++9l6FDhwKQkJDAtGnTmDhxIg0aNKBbt26Eh4eTkpJSJDHpjZyIiDiltObvW265hZ9++omRI0cyevRoGjRowI8//sigQYP4+OOP+eCDD9i7dy8PP/xwoWNS/hYRuXhqUSPio9LT04mNjaVjx4706dOH2bNnk5aWRr9+/QAICwtj2LBhNG/enOXLlwMQHh5e4POcPHmSoKAgqlWrRmhoaK6WMGcf8gAqVarEn//8Z7799luaNWsGQPPmzVm3bh0Ahw8fJiIigsGDB3P69OlCXbeIiIg/8+X8bYyhTp06Xvt9//33NG7cGPDO6SIi4hwVakR81IcffkirVq1Yvny5Z4rPcx06dIj4+Hg+/vhjpk2blucxMjIyPF2Rzi4xMTFe+xw5coTq1at7fj7bjelcixcvpkmTJvz000/UqlWLRo0asWbNGsDd7PrIkSMApKSk8Omnn1KvXj2mT59e6GsXERHxV76cv/Ny9dVX88UXX5CVleWV00VExDma9UnER6Wmpnq1Wvm9Sy+9lNq1awNw9OjRPI9RsWJFkpOT//A8NWrU4NixY56fAwJy129jY2OJjY1l+PDhLFmyhB49etCkSROioqJo0qSJ5+1czZo1AejRowcvvvjihS5RRESk1PH1/J1XPP369aNt27a0aNEiV4sbEREpeWpRI+Kj6tevz/bt2wHYtm1bru3G/Nb9+3z96PPzRq5y5cqcOnWK48eP88UXX3iaP5+Vnp7u+XP16tUJCgoCID4+nqSkJGrVqkXnzp3JyMjw7PvZZ59xzTXXFOKqRURE/Juv5++8PPDAAyQnJ9O4cWM6d+584YsUEZFipRY1Ij6qe/fu9OrViw4dOlC1alWvvub5lZ83cgCjRo2iXbt2BAYGMmfOHMA9A8Q999zDxo0bmTRpEgANGzakffv25OTkEB0dTbly5YiJiSE0NJQDBw7QqVMnqlatSo0aNZg3b16B4xUREfF3vpy/AXr37k1KSgp79uzhscceo1u3bvTq1YvDhw/TrFkzJk6cWOB4RUSkaGnWJ5FidLGzRmRlZVG+fHmGDBlC//79CQsLK8LofJtmjRAREacofxee8reIyMVTixoRH9a5c2eOHz9OgwYNytRDnoiIiD9T/hYRkYuhFjUixehi38iVZXojJyIiTlH+LjzlbxGRi6fBhEVEREREREREfIQKNSKlWHh4eLEdu2vXrrRp04aYmBj27t0LwMCBAwkNDcXlcvHWW28B8NRTT3HDDTfgcrk8gxqKiIjI+RVn/gbYt28fgYGBfPPNN0De+XvEiBGeGadq1KhRrPGIiIg3jVEjIoXy8ssvU79+fVauXMmLL77omSVi/vz5NGjQwGvfiRMn0rZtWyfCFBERkd956aWXaNmypde63+fvl156CYDNmzdrJigRkRKmFjUiDlu7di2hoaFER0czc+ZMMjMziYmJISIigjvuuIPs7GzS0tKIjo6md+/e3HjjjSxYsID27dsTHh7OiRMnSEtLIyoqim7dutGyZUtSU1O9zvH555/jcrlo3bo1s2bNAmDAgAFERkYSFRVFTk5OgeOuX78+AOXLl6dcuXKAu196//796dq1K999951n35EjR9K2bVu2bNlSyG9JRETEt/hr/v7555/59ddfueqqqzzrzpe/ARYuXMjtt99e8C9IREQKz1qrRYuWYlrct9gfGzVqlE1KSrLWWpuTk2NzcnLsyZMnPdtWrFhhU1NTbfPmzW12dradP3++jY2NtdZaO378ePvBBx/Y1NRUe+2119rMzEy7fv16O3jwYGutta1bt7bWWtu+fXv7yy+/2JycHBsTE2NPnz5to6OjPec8V3p6uo2MjPRazu77e1lZWTYmJsZ+88031lprDx06ZK219t///re94447vNZ9/fXXNjw8/ILfx1lnvjvH/x9q0aJFi5ayt5Tm/P1///d/dseOHXbAgAF2z5491tq88/dZt956qz1x4sQFv4+zlL+1aNGi5eIXdX0ScdjQoUMZN24cM2bMYPjw4TRu3Ji4uDh++OEHDhw4QMOGDWnYsCGNGzcmICCAyy+/nCZNmgBw+eWXc+TIEQCaNm1K+fLlad68uafP+Vlbt24lNjYWgIMHD3Lw4EEGDBhAv379qFevHk8//TQBAe4GdhUrViQ5OTlfsf/tb3+jf//+XHPNNQDUrFkTcPetf/zxx73WNWzY8CK+JREREd/ij/n76NGj/O9//+P666/3Wp9X/gbYs2cPV1xxBZUrVy78FyUiIgWmQo2Iw2rUqMHUqVPZt28f999/P4MGDeIvf/kLb731FqNGjcJa9/Sgxvw20+W5fz67fceOHWRnZ7N161ZP4eSsG2+8kffff58qVaqQmZlJQEAAffv2pX///sTFxbFhwwZCQ0MByMjIoH379l6fL1euHKtWrfJaN2PGDE9T6bOOHTtG9erV2b17NyEhIV7rDh48SFZW1kV+WyIiIr7BH/P37t272bNnDx07dmT79u3s3buXxMTEPPM3uLs99ejRo2i+MBERyTcVakQcNm3aNBYsWMDx48cZOXIkoaGhjB8/no0bNxIcHJzvlii1a9eme/fu/Pzzz8yfP99r25gxY4iNjSUnJ4eaNWsyY8YMYmNjyc7Opnr16jRt2tSzb35b1AwdOpQWLVrgcrmIjIxkzJgx3H333Rw5cgRjDK+99hoAjz76KDt27CAnJ4fnnnsu/1+MiIiID/PH/B0aGsq6desA90xPo0ePBsgzfwMsWbKERYsW5es6RESk6Jiz1XwRKXrGGFsS91haWhqjR49m3rx5xX6ukmKMwVprLryniIhI0VL+LjzlbxGRi6dZn0REREREREREfIRa1IgUo5J6I1ca6Y2ciIg4Rfm78JS/RUQunlrUiPigp556isTExGI7/sCBAwkNDeWXX34BYMKECbRt2xaXy0VOTg5ZWVn06dOHqKgoHnvssfMeZ9++fdx0000EBgZ6BgrOycmhX79+REZG0rZtWw4ePAjA5s2badeuHVFRUXz88ccADBo0iAYNGhTbdYqIiJSkkszfK1asIDw8nLCwMEaNGuXZZ+7cucTExOByufjhhx8AWLlyJdHR0bhcLjZt2nTe4w8fPhyXy8V9991HdnY2AN26dSMkJMTrupS/RUSKlwo1ImXU/PnzCQ4OZsOGDRw/fpzExESSk5MJCAhg4cKF3HDDDSQlJXHq1Cm2bt2a5zFq1qzJqlWraNmypWfdli1bqFixImvWrOHee+/1DIw4btw4Fi1aRFJSEp07dwYgISGBunXrFv/FioiIlBJn83dUVBQpKSmsW7eOtWvX8vPPP/PDDz+wZs0aVq1aRXJyMldccQWnTp1i2rRprFy5kuTkZG6++eY8j7thwwYyMjJITk7m+uuvZ8mSJQC8/vrrjBgxwmtf5W8RkeKlQo1ICXrggQfYtWsXAC+//DLvvfcey5cvJzIykltuuYW5c+d67T979mwSEhIA91u65ORkrLUMGTKE6OhoOnfuzJEjRy4qpo8++ohDhw4RFRXF2LFjAfj2229p1qwZAM2bN/fMEPF7gYGB1KhRw2vdFVdc4Zl+9OjRo9SqVYtvv/2W06dP07NnT7p3786BAwcuKmYREZGS5Iv5u0KFCgBkZ2dTt25dqlevzieffEJ2djYxMTEMHz6c7Oxs1q1bR0BAAJ06deKee+7hxIkTeR7vfLn/sssuu6g4RUSk4FSoESlBPXv25P333wdg+fLl3HbbbURERLBmzRrWr1/P9OnTL3iMJUuWcOWVV7J69WqGDRvG66+/7rV97NixuFwur2XlypXnPd6BAwcICQkhKSmJr776ii+//JJGjRqxZs0aAJKSkgr0MHnJJZeQnp7Oddddx2uvvcbtt9/OgQMH2L17N++//z6DBw9m/Pjx+T6eiIiI03wxfwNMnz6dRo0aUatWLSpVqsSBAwfIyMhg1apVVK5cmUWLFnHgwAF+/PFHli1bRqtWrZg2bVqexzo3969evfqiC0kiIlJ4KtSIlKCYmBiSkpL46aefqFatGlWqVGHTpk20bduWmJgYdu7c6bX/2ZYpAGcHNdy1axfvvPMOLpeL8ePHc/jwYa/PxMfHk5yc7LW0a9fuvDEFBwcTGRkJQFRUFLt27aJr166cOnWKmJgYKlWqRJ06dfJ9jStWrCA4OJhdu3bx1FNP8c9//pPg4GBuvfVWKleuTHR0tOetpIiIiD/wxfwNEBcXx+7du9m7dy+bN2/2yuln821wcDDh4eGUK1fuD3Nw8+bNadKkCVFRURw7dqxAuV9ERIqWCjUiJah8+fJcddVVvPDCC3Tv3h2A559/noSEBBITEwkODvbaPzg4mB9//BGA7du3A+43Xv379yc5OZmUlBSeeeYZr88U9I1cq1at2LZtG+AeX6Z+/fqUK1eOV155hVWrVlGuXDnat28P4BmU8I9Ya6lZsybgbl3zyy+/0LBhQ3766Seys7M95xAREfEXvpi/09PTAShXrhxVqlQhKCgoz5x+6623eooz5+bgvHJ6fHw8SUlJ1KpVyzOenIiIlLzyTgcgUtb07NmT3r17ex7gevToQbdu3WjevHmu8V5iYmJ44YUX2LBhA+XLu2/X2NhYHnroIaKjowEYMWIEsbGxns/Ex8cTHx+f73i6dOnCAw88QGRkJNdeey2tWrXihx9+4O677yYgIID+/fvzpz/9iaysLAYOHOj10JiZmUmnTp3YunUrHTp04JlnnqF9+/bMnDnTM4PUrFmzqFChAg888AAul4uAgABmz55d2K9PRETEEb6Wv2fNmsU777xDVlYWUVFRXHvttQAEBQXhcrm45JJLeOSRR6hYsSKRkZFERERQuXJl3nrrLQDuvvtukpOTPcfLyckhOjqacuXKERMTQ2hoKAAPPfQQS5YsYfHixTz44IPExcUV8JsTEZGCMmebY4pI0TPGWF+8x/7+97+zfv16Pv7441xvAc9n06ZNbN26lfvuu6/I4hg0aBC7d+/m3//+d65txhistSaPj4mIiBSr0pS/83Lo0CFefvllxowZU6jPK3+LiBQvFWpEipGvPuj5Az3oiYiIU5S/C0/5W0Tk4mmMGhERERERERERH6FCjYiIiIiIiIiIj1ChRqQUcLlcZGVllci5li1bxrXXXkt4eHiJnE9ERKS0Ksn8/euvv9K1a1dat27N3LlzS+ScIiJSOCrUiEiBtGzZkq1btzodhoiIiBTAG2+8Qd++ffn0009JSEggIyPD6ZBEROQ8VKgR8TM5OTkMGjSIyMhIOnXq5LVt+fLlREZGcsstt3jelk2ZMoWWLVsSFRXFl19+ycKFC2nRogXR0dEsXbq0wOevUaMGlSpVKpJrERERKSuczt/r1q2jbdu2lCtXjhtuuIHdu3cXyXWJiEjRK+90ACJSMIsWLaJ27dokJCSQk5PjtS0iIoKOHTuSlZWFy+Wif//+LFq0iKSkJIKCgrDWMmnSJN59912uuuoqfj+jxdixY1m9erXXulGjRtGuXbtivy4REZHSzOn8ffToUapXrw5AcHAwR44cKaYrFRGRi6VCjYif+frrr2nVqhUAAQHejeI2bdrEmDFjyMzMZOfOnQCMGTOGIUOGULFiRZ5++mlGjx7NuHHjyMrKYtSoUTRs2NDz+fj4eOLj40vuYkRERMoIp/N3SEgIx44dIzAwkGPHjhESElK0FygiIkVGXZ9E/EyjRo1Yv349QK43cs8//zwJCQkkJiYSHBwMQPPmzZk9ezYul4vZs2dTr149EhISiIuLY9KkSV6fHzt2LC6Xy2tZuXJlyVyYiIhIKeZ0/g4LC2PVqlVkZ2ezZcsWGjVqVIxXKyIiF8P8vumkiBQdY4wt6nssJyeHBx54gD179lC1alWWLl2Ky+UiMTGRuXPnMnnyZJo3b862bdvYvHkzAwYMIDU1lfT0dGbNmsWcOXNYv349x48fZ+LEibhcrgKdf+PGjTz++ONs3LiRW265hSVLlhAYGFik1whgjMFaa4r8wCIiIhdQGvP3sWPHuOuuuzh8+DBxcXEMHDiwSK/vLOVvEZGLp0KNSDEqjge9skIPeiIi4hTl78JT/hYRuXjq+iQiIiIiIiIi4iNUqBERERERERER8REq1IiIiIiIiIiI+AgVakREREREREREfIQKNSIiIiIiIiIiPqK80wGIlGaBgYEHjDF1nI7DHwUGBh5wOgYRESmblL8LT/lbROTiaXpukRJgjIkA3gMes9bOcToeX2KMqQcsA1YAf7PWZjsckoiICKD8/UeUv0VEio8KNSLFzBjTB3gZuMtam+h0PL7IGFMDWAgcAvpZa085HJKIiJRxyt8XpvwtIlI8NEaNSDExbo8CzwNt9ZB3ftbaI0AHIB1YZYy5xOGQRESkjFL+zj/lbxGR4qFCjUgxMMaUA14F7gFaWWu3ORySz7PWpgP9gDXAWmNMA4dDEhGRMkb5u+CUv0VEip4GExYpYsaYKsDbQBDQxlr7i8Mh+Q1rbQ7wD2PMd8C/jTHdrbWfOx2XiIiUfsrfhaf8LSJStNSiRqQInZkhIgk4DHTWQ17hWGtfBx4AlhhjujscjoiIlHLK30VD+VtEpGioUCNSRIwxjYC1uGdAuNdam+FwSH7NWrsE6ARMNcYMdzoeEREpnZS/i5byt4jIxdOsTyJFwBjTGvgAGGWtneF0PKWJMaY+7ofnJbinR81xOCQRESkllL+Lj/K3iEjhqVAjcpGMMT2BqcA91tpPnI6nNDLG1AQ+BPYD/a21p52NSERE/J3yd/FT/hYRKRx1fRK5CMaYR4CXgA56yCs+1trDQHvAAonGmFoOhyQiIn5M+btkKH+LiBSOCjUihWCMKWeMeQm4H/f0nZsdDqnUO/MWri/ucQQ+M8Zc7XBIIiLiZ5S/S57yt4hIwWl6bpECMsYEAfOBGkC4tfaosxGVHWf6tz92ZvrPFGNMN2vtBqfjEhER36f87RzlbxGRglGLGpECMMZcCqwGTgId9ZDnDGvtFGAIsNQY09XpeERExLcpf/sG5W8RkfxRoUYkn4wxDXA3212Ne+DBdIdDKtOstYuALsB0Y8wQp+MRERHfpPztW5S/RUQuTLM+ieSDMaYlsBB40lo73el45DfGmGtwT/+5EPiHpv8UEZGzlL99l/K3iMj5qVAjcgHGmB7AdGCAtXap0/FIbsaYS4BFwPfAQL0tFRER5W/fp/wtIpI3dX0S+QPGmOHAq7j7s+shz0dZaw8CbYEKwApjTA2HQxIREQcpf/sH5W8RkbypUCOSB2NMgDFmIjAUaG2t3eR0TPLHrLWngN7AJtzTf17lbEQiIlLSlL/9j/K3iEhump5b5HeMMYHAm0BtoJW19ojDIUk+nenf/v/OTP/5mTEmVg/pIiJlg/K3/1L+FhHxphY1IucwxtQCEoEsoL0e8vyTtXYyMBxYboy5zel4RESkeCl/lw7K3yIibirUiJxhjLka9/SdKcDdGtDOv1lrFwCxwExjTJzT8YiISPFQ/i5dlL9FRDTrkwgAxpgWwIfAOGvtVIfDkSJkjGmIe/rPfwGjrf7RExEpNZS/Sy/lbxEpy1SokTLPGBMLJAD3W2s/cjoeKXrGmNrAYuAb4D5rbYbDIYmIyEVS/i79lL9FpKxS1ycp04wxQ4HXgc56yCu9rLU/AdFAVdz93kOcjUhERC6G8nfZoPwtImWVCjVSJp2ZvnMC8DAQbq3d4HRMUrystSeBO4CdQIox5kqHQxIRkQJS/i57lL9FpCxSoUbKnDPTd74FtMY9fee3DockJcRamw08BMwC1hpjmjsbkYiI5Jfyd9ml/C0iZY0KNVKmGGNqAp/g/rvf1lp7yOGQpIRZt4nAI8BKY0wHp2MSEZE/pvwtyt8iUpaoUCNlhjHmKuAzYAPQx1p72tmIxEnW2veAHsAcY8x9TscjIiJ5U/6Wcyl/i0hZoFmfpEwwxtyMe9aA56y1rzgdj/gOY0wj3NN/vgk8pek/RUR8h/K3nI/yt4iUZirUSKlnjOkMzAYesNZ+6Gw04ouMMXWAJbgHKozT9J8iIs5T/pYLUf4WkdJKXZ+kVDPGDAYSgK56yJPzsdYeAFxALeBjY0x1ZyMSESnblL8lP5S/RaS0UqFGSqUz03c+C/wdaGOtXe90TOLbrLUncPd53wP82xjzJ4dDEhEpc5S/paCUv0WkNFKhRkodY0wl3P2VI4Ewa+03DockfsJamwX8FZiPe/rPZg6HJCJSZih/S2Epf4tIaaNCjZQqxpgauKfvDAJirLUHHQ5J/MyZ6T+fB0YCicaYtk7HJCJS2il/y8VS/haR0kSFGik1jDH1gBRgC9DLWnvK2YjEn1lr3wZ6AfONMQOcjkdEpLRS/paipPwtIqWBZn2SUsEYcxPwEfCCtfYlh8ORUsQYcx2wFJgJjNP0nyIiRUf5W4qL8reI+DMVasTvGWM6AXOBB621Hzgdj5Q+xpjLcE//uRkYYq3NdDgkERG/p/wtxU35W0T8lbo+iV8zxgwCZgHd9JAnxcVa+yPuwS0vAz4yxlRzOCQREb+m/C0lQflbRPyVCjXil4zb08DjQIS1dq3TMUnpZq09DnQDvgc+NcZc7nBIIiJ+R/lbSpryt4j4IxVqxO8YYyoCc4D2QCtr7dcOhyRlxJnpPwcD7wHrjDHXOxySiIjfUP4Wpyh/i4i/Ke90ACIFYYwJBhYAvwJR1tqTDockZcyZwQifMcZ8DyQZY+601iY5HZeIiC9T/hanKX+LiD9RixrxG8aYP+OevnMXcIce8sRJ1tp5wJ3Av4wxdzsdj4iIr1L+Fl+i/C0i/kCFGvELxpgbgLXAbGC4tTbb2YhE4MybuGjcb+j+78zYC08YYyo4HZuIiC9Q/hZfpPwtIr5OXZ/E5xlj2gPzgGHW2nedjkfkXNbaHcaYMOBjoB5wLe63xu87GpiIiMOUv8WXKX+LiC9TixrxacaYe4E3cTeV1kOe+CRr7T7gQaA+UB0Y5mxEIiLOUv4Wf6D8LSK+yrjH1RLxLcYYAzwJ9Adus9b+x+GQRM7LGFMeSARuBk4AdYCW1trPHQ1MRKSEKX+LP1H+FhFfpa5P4nPO9A+eDjQBwqy1BxwOSeQPnZn203VmVpNI4CEgxNGgRERKmPK3+BvlbxHxVWpRIz7BGNMMOAYcxt03OB3oY6094WhgIiIicl7K3yIiIkVPY9SIr5gOtAH+DfwX6KGHPBEREZ+n/C0iIlLE1KJGHGeMuRFYCmQCU4EJVn8xxY8FBQXtP336dB2n4/B1gYGBB06dOlXX6ThEpHCUv6W0UN4uPOVykeKhQo04zhjzEdAR+BJoAIy01iY4G5VI4Rlj9LtKPhhjsNYap+MQkcJR/pbSQnm78JTLRYqHCjXiOGPM18C3wHxgtbX2B4dDErkoeuDLHz3cifg35W8pLZS3C0+5XKR4qFAjIlLE9MCXP3q4ExERX6C8XXjK5SLFQ4MJi4iUEGstnTt3JiIiguzsbEdjGTNmDGFhYYSFhbFq1SqvbbGxsYwePTrXZyZPnkxoaChhYWGsW7cOgB07dhAeHk7r1q3Ztm0bAGvWrCE0NJSWLVvy+uuvF//FiIiIlFEzZ86kfv369OvXz7Nuzpw5RERE0KJFC6ZOnZrrM6tXryYsLIyoqCj27t1bkuGKSH5Za7U4sAQGBu4HrJY/XgIDA/c7/f9Ki5aCLu5/WnP74Ycf7J133pnntt/Lzs7O137nOn78eL73/fbbb6211h45csS2adPGs37Lli22Xbt2dtSoUbk+07x5c5udnW337t1ru3fvbq21tnv37vb777+3e/futbGxsdZaa7t27Wq/++47m52dbVu0aHHeGM58T47//9KipSCL8rfyt5bSt5wvbzulIPn8559/tnv27LF33323Z11GRoa11tqsrCzbrFmzXJ9xuVz22LFjdv369Xbo0KEXFatyuRYtxbOoRY1DTp8+Xcfp//n+sGgEfilNHnvsMZKSkhg0aBBbtmwhMjKSli1b8swzzwAwe/Zs7rzzTjp37sy2bdsYO3YsLpeL6Oho0tLS8jzmqVOnmDNnDu3atWPRokX5jqV+/foAVKpUCWN+a7H88ssvM3To0Dw/06BBA9LT0zl69Ci1atUC4PDhw/z5z3/miiuu4JdffgHg+uuv55dffiE9PZ0qVarkOyYRf6D8rfwtArBv3z6ioqIIDw/35M309HRiY2Pp2LEjffr0Yfbs2QDFms8vueQSypcv77WuQoUKAGRkZHDdddd5bTt58iRBQUFUq1aN0NBQvvrqq3yfS0RKTvkL7yIiIkVh3LhxACQkJHDq1CmSk5MxxhAVFcUjjzwCQEhICP/617/Yvn07P/zwA8nJyezatYtnn32WadOmeY6VlpbGyy+/zI4dO7jjjjtYsGAB1apVA2Do0KG5HrxeeeUVmjZtmiump556isGDBwPwn//8h9q1axMSEpJn/DExMVx77bVkZWWxbNkyAHJycjzbz/65e/fudOnSBYD4+PgCf08iIiK+7pJLLmHlypWUL1+efv36sWfPHr788ktatWrF448/zpAhQwBKLJ//3tixY5k+fToPP/yw1/ojR45QvXp1z89Od8UWkbypUCMi4oDU1FT+9re/cfLkSXbv3s1PP/0EwM033wzArl27SE5OxuVyAXDZZZd5fX7Dhg0kJiby8MMP06dPH6+WK3n1R8/LwoULOXToEHfddRcAkyZNYuzYsfznP//Jte+xY8eYOXMme/bs4aeffiIuLo6lS5cSEPBbw8yzf3700UdJSUmhTp06tGvXjr59+1K5cuV8fjMiIiK+79ChQwwZMoSjR4+SlpbGvn37SE1NpVmzZgA0b94cKJl8npf4+HhGjhxJmzZtuO+++zwtYWvUqMGxY8c8+52bx0XEd+jO9HP79+9n/Pjx590+fPjwQh33QoOMPfLII7Rp08arSj98+HBcLhf33Xcf2dnZ7N+/H5fLhcvl4rrrrmPEiBGFikWkNHrttdcYOXIka9asoUGDBlhrgd8emBo1akT79u1JTk4mOTmZuXPnen2+V69efP755wQEBNC9e3cefPBBvv76a8D9Bu7svXd22b59u9fnt23bxpQpU5gyZYpn3XfffcfAgQN57LHHePvtt1mzZo1nW0BAAJUrV6ZixYoEBwdz4sQJAGrWrMnevXvZt28fwcHBAJQrV46QkBAqVqxIQEAAmZmZRfztifg/p/I3uIuy4eHhXudS/hYpmLfeeovu3buTnJxM69atsdZSv359T749O8B+cefzvKSnpwNQsWJFKleuTKVKlTzbKleuzKlTpzh+/DhffPEFjRs3LpLvQ0SKmNN9mMvqgo8NWvZ7fzTI2KZNm+wDDzxgrbX2wQcftF988YX94osvbFxcnLXW2n/+85/2ww8/9PrMQw89ZFeuXFngONAAZVr8cDnf/Z2amuoZ7G/ZsmX2+uuvt7169bJRUVE2NTXVzpo1y77xxhue/ceNG2cjIyOty+Wy06ZNy/OYZ23evNkmJyf/4T7nat++vW3SpImNjIz0DAJ8VlJSkmcw4c2bN9uEhARrrbXjx4+3LVu2tLfeeqv96KOPrLXWbt261bZu3dq2atXKbt682XNtLVq0sC1btrRjxow5bwy6v7X44+LP+dtaa0+fPm379+9vW7duba21yt9atNjC3ddffvmlbdKkie3WrZvt1q2bTUpKsqdPn7Zdu3a17du3t7fffrudN2+etbZ48/lHH31kW7dubevWrWtvv/12a621Tz75pI2MjLRhYWF26tSpnuOezecrV660LVu2tC6Xy3733XcFvvZz6V7XoqV4FmOtdaZCVMYZY2xBv/v09HR69epFRkYGISEhdOzYEZfLxejRo5k3bx5hYWHcfPPNpKSk8Nxzz9GxY0fCw8NJSUkp0HlOnjxJz549Wbp0KQBRUVEkJSV5tk+ZMoVLL72U3r1788EHH7Bv3z5q167NwYMH+etf/8qqVatYuXIlzz33nOczN954Ixs2bMg12NmFGGOw1poL7yniOwpzf5dFur/FH/lz/gZ3Dr/uuuuIj48nJSWFf/3rX8rfUuYVZd7OysqifPnyDBkyhP79+xMWFlYkx/VVutdFioe6PvmRDz/8kFatWrF8+XJq1KiRa/uhQ4eIj4/n448/9hqk7FwZGRm5mlDGxMR47XOhQcaOHj3q2R4cHMyRI0do1KiRp5vE6tWrOXLkiGf/jRs30qxZswI/5ImIiJQGvpK/MzMzWbNmDdHR0Z51yt8iRatz5860bt2akydPlvoijYgUH2VeP5LXAGXnuvTSS6lduzbgLqbkpWLFiiQnJ//heS40yFhISIhn+7FjxwgJCaF58+Y0adKEqKgomjRpQp06v83KuXDhQm6//fYLXZ6IiEip5Cv5+8033/QMHn6W8rdI0frkk0+cDkFESgG1qPEjeQ1Qdi5jfmt1eL7mm/l5I3ehQcbCwsJYtWoVAImJibRs2RJwjy6flJRErVq16Ny5s2f/FStW0L59+0JcsUjZ5suDjQI899xzhIeHc8cdd3gGFxaR3Hwlf+/evZvXXnuNjh07snPnTl555RVA+VukqDmRvz///HNatWpFmzZteOSRRwA4ePAgrVq1IjIyktjYWE6dOgXAY489RuvWrWnTpg179uwpVCwiUrzUosaPdO/enV69etGhQweqVq1KhQoVCnyM/LyRAxg1ahTt2rUjMDCQOXPmAO5fyu655x5uuukmAgMDadOmDTfccAMtWrQgJyeH6OhoypUrR0xMDKGhoYD7obBevXoEBQUVOFaRsq5u3bqMGjXqvNvP/pJVUE8//TQrVqzgq6++4tlnn/Wa+Qnc42ls3brV8/OGDRvIyMggOTmZiRMnsmTJElq0aMG///1vUlJS+OCDD0hISPCaBU5EfuMr+XvChAme/cLDwxk+fLjyt0gxcCJ/16tXj9WrVxMYGMjdd9/N9u3bady4MSkpKQQEBDBmzBiWLFlCTEwMGzdu5LPPPuOzzz5j6tSpvPjii4WKR0SKkdOjGZfVhULOGpGZmWmtdc+2tHbt2kIdw5+gkeS1+OFSmPv77EwRHTp0sHfeeaedNWuW1yxRLVu2tH/961/tDTfcYJctW2attZ5ZWwrixIkTtlOnTp6fXS5Xrn1effVVu2rVKs/x33nnHfvqq69aa61NTEy0I0eOtOvXr7d///vfrbXWfvPNN/bOO+8scCy6v7X446L8nT+6v7X401LY+9pa38rfZ/Xv39/u3LnTa90TTzxhN2/ebNPT023Pnj1tVlaWXbJkiX366acLHMu5dK9r0VI8i7o++RkNUCZSOvnbYKNXX301X3zxBVlZWbkGIBWR3JS/RUonX8nfZ23bto2DBw96uj5+8cUX3HLLLaxevZr69etTsWJFGjRoQKNGjfjrX//K/fffX9hLF5FipK5PfkYDlImUTv422Oill15Kv379aNu2LS1atPAagFREclP+FimdfCV/Axw+fJhhw4bx7rvveta1aNGCjRs3MnHiRGbOnEnHjh3ZsWMHX3/9NV9++SWjRo1i5syZF7hKESlpalFTxpw7OGhRGzp0KJdeeikJCQmedQ8//DCRkZGEhoby2WefATBixAjP24K83jyIlEX+ONjoAw88QHJyMo0bN/YagFREik9x5vHJkycTGhpKWFgY69atA6BPnz64XC7CwsLy/CVUpKzzlfydlZVFv379eOGFF6hbt67nuGdVr16doKAgrLWEhIQQEBDAJZdcwi+//FK4CxeRYqUWNVJknnjiCVq0aEFWVpZn3T//+U8qVKjAd999x9ChQ/n444956aWXANi8eTMTJ050KFoR3+KPg4326tWLw4cP06xZM93LIqXA7Nmz2bRpEz/++CPDhg1j4cKFvPPOO4B7qu5NmzY5HKGI7/GV/P3pp5+yYcMGRo4cCcCzzz5LuXLlePTRRwkICKBmzZq8+eabVK5cmWrVqtGmTRuysrKYPHlygeMVkRLg9CA5ZXXhDwYt++yzz2yLFi1sVFSUnTFjhs3IyLDR0dG2TZs29vbbb7dZWVk2NTXVRkVF2V69etnmzZvbDz74wLZr1862bt3aHj9+3KamplqXy2VjY2NtaGio/fbbb621vw1etn79ehsZGWlbtWplZ86caa11DzwWERFhXS6Xzc7OPm98f2TWrFn2jTfeyLV+x44d9h//+IfXuieeeMJ+8MEHf3g8NECZFj9c/uj+/iMabFSLFt9f8nN/+2se79mzpz158qTdsWOHvf/++7223XPPPXbTpk35Ppbuby3+tBQ2b59V1vL3uXSva9FSPIta1PigpUuXMmHCBFwuF9a6m0guWbKEoKAgRo8ezerVq2nYsCFHjhwhMTGRd955hzlz5rBixQqeeeYZPvnkE2666Sb279/PypUr2bRpExMmTOD111/3nCM+Pp7FixdTrVo12rVrx1133cXevXtZs2YN1lqvZpoZGRm0b9/eK8Zy5cqxatWqfF1Pjx49+OKLL3jzzTe91i9fvpzHH3+8sF+TSKnTuXNnjh8/ToMGDTTYqIgf89c8HhMTw7XXXktWVhbLli3zrM/KymL79u3cdNNNxfF1ifg95W8RKWoq1PigoUOHMm7cOGbMmMHw4cNp3LgxcXFx/PDDDxw4cICGDRvSsGFDGjduTEBAAJdffjlNmjQB4PLLL/fMvtK0aVPKly9P8+bN+eabb7zOsXXrVmJjYwE4ePAgBw8eZMCAAfTr14969erx9NNPewYpy29zzPNZuHAh//vf/+jVqxfr168HYM+ePVxxxRVUrly50McVKW002KhI6eCPefzYsWPMnDmTPXv28NNPPxEXF8fSpUsBSEpKwuVyFeE3JFK6KH+LSFFTocYH1ahRg6lTp7Jv3z7uv/9+Bg0axF/+8hfeeustRo0a5Xk7d+7bsrwGKtuxYwfZ2dls3bqVa665xuscN954I++//z5VqlQhMzOTgIAA+vbtS//+/YmLi2PDhg2ecSgupkVNeno6lSpVolq1alSpUsWzfuHChfTo0aOA34yIiIjv88c8HhAQQOXKlalYsSLBwcGcOHHCs23hwoX06dOniL4dERERuRAVanzQtGnTWLBgAcePH2fkyJGEhoYyfvx4Nm7cSHBwMA0bNszXcWrXrk337t35+eefmT9/vte2MWPGEBsbS05ODjVr1mTGjBnExsaSnZ1N9erVadq0qWff/LaoGT9+PG+99RbWWvbt20d8fDx33nknv/zyC1lZWTz77LOefZcsWcKiRYvy94WISL6Eh4eTkpJSLMceOnQo7733Hs8++yyDBg0C3Pf8lClTuO+++xg3bhwAa9as4bHHHsMYw8CBA3nwwQeLJR4RX+aPebxq1aq0b9+esLAwsrOziY+PB9xFo3Xr1vHqq68W7EsQkXwrzvzdu3dvDhw4QHZ2NjNmzKBRo0b06dOH/fv3k56ezqlTp9iyZQv79u2jX79+nD59mrFjx9K2bdtiiUdE8secfWsjJcsYY4vzu09LS2P06NHMmzev2M5REowxWGvNhfcU8R3FfX+fT3E+6P3444988sknZGVleQo1Bw4c4KuvvmLVqlWeQk1sbCyvvvoqf/rTnwgLC+Pzzz8/7zF1f4s/Kqn729/zuO5v8SdO5e2zijN/Z2ZmUqFCBdasWcO7777LlClTPNvOzuY2btw4HnroIfr06UOzZs3o0qVLvoc90L0uUjwCnA5ARKQsWbt2LaGhoURHRzNz5kwyMzOJiYkhIiKCO+64g+zsbNLS0oiOjqZ3797ceOONLFiwgPbt2xMeHs6JEydIS0sjKiqKbt260bJlS1JTU73O8fnnn+NyuWjdujWzZs0CYMCAAURGRhIVFUVOTk6B477ssstyratTp45Xdw2A66+/nl9++YX09HSv7o4iIiL+zF/z99npwo8fP06zZs28ti1cuJDbb78dgG3bthEWFkbVqlWpVq0av/76a2G+JhEpIur6VEpdddVVfvsWTqQ089fZYPKre/fudOnSxROHiBSO8riIb/HX/J2RkUF0dDT79u1j4cKFnvW/n80tOzvbc/zg4GCOHDlCtWrVivZLFJF8U6FGRKQE+eNsMAXx6KOPkpKSQp06dWjXrh19+/bV7G4iIuL3/DV/V6xYkZSUFDZt2kR8fLxnjMjfz+ZWrlw5z5+PHTtGSEhIYb8qESkC6vrkZ5566ikSExOL7fgDBw4kNDSUX375hdmzZ+NyuXC5XNSoUYMtW7bk+ZnNmzfTtGlTrrrqKs+6HTt20KpVK9q0acO9997refPwwgsvEB4ezt13301mZiYAXbp0ITw8vNiuScSXnJ0NZsKECTz55JN88skn/OUvf2HNmjXccccdRTYbzMcff0xycjKbN2+mbt269O3bl3nz5vHzzz+zYcMGz74ZGRme+/zsEhMTU+jrK1euHCEhIVSsWJGAgADPfS5S1pVk/j7roYceol+/fgAsX77cc49fdtllfPjhh394vEmTJnnlZuVvKev8MX9baz33a/Xq1QkKCvJs+/0MrM2aNWPdunWcOHGCY8eOUb169Yv9ykTkIqhFjeQyf/58goODGThwIAMHDiQrK4tbbrmFG264Ic/9GzRowPr16+nQoYNnXaNGjVi7di0A9957Lxs3buSqq64iKSmJlJQUJkyYwIcffkivXr1YsmSJHvSkzPDH2WAg71ndZsyYwdSpUzl8+DBHjhxhypQpjBw5krZt2xIQEECnTp0IDg4u0PcjIoV3Nn+De7DvtLQ0zy9bHTt2pGPHjgCEhob+4Ywu6enpbN261fPzzz//rPwtZZ4/5u/09HQ6duyIMQZjjGcg4bxmc3vsscfo378/p06dYsyYMfn8VkSk2FhrtTiwuL/63wwaNMh+9dVX1lprJ0+ebN999127bNkyGxERYW+++WY7Z84ca621Tz75pF25cqWdNWuWfeONNzzrkpKSbE5Ojn3wwQdtVFSUve222+zhw4dtQQ0YMMDu2bPHa92qVavs8OHDL/jZ1q1b57l+8ODB9scff7RLliyxEyZMsNZau3HjRvv//t//u+Bnz3xPjv//0qKlIMvv7++ilpqaau++++5iPUdJ0P2txR8Xf8nfI0eOtCtWrMj1b8V///tf27Vr1z881quvvmpXrVrlyc3K31pK+1Lcefus0pK/z6V7XYuW4lnU9clH9OzZk/fffx9wN0++7bbbiIiIYM2aNaxfv57p06df8BhLlizhyiuvZPXq1QwbNsxrcDKAsWPH5moiuXLlygsed8GCBV5NI/Nr8eLFNGnShJ9++olatWpx9OhRz5u9s4OUiYiI+DNfzN+HDx/m559/zvMN/4VyemZmJmvWrCE6OtqzTvlbRESkZKnrk4+IiYlhwoQJDB48mGrVqlGlShX+/e9/M2bMGDIzM9m5c6fX/nn1ed21axfvvPMOn3zyCVlZWYSFhXl9Jj4+vsCzsFhrSUlJYfLkyQW+ptjYWGJjYxk+fDhLliwhJCSEH374AdAgZSIXQ7PBiPgOX8zfkydPZtiwYXlu++ijj1iwYMF5P/vmm29y1113ea1T/hYpGsrfIpJfalHjI8qXL89VV13FCy+8QPfu3QF4/vnnSUhIIDExMdc4D8HBwfz4448AbN++HXCPC9O/f3+Sk5NJSUnhmWee8fpMYVrUbNiwgZtuuskzEnxWVhYHDhy44PWkp6d7/nx28LJbb72VNWvWAJCYmEjLli0veByRsqSkBxudMGECbdu2xeVykZOTw+eff+4ZBPyRRx4573HS0tKoU6cOLpfLa2rQlStXEh0djcvlYtOmTYAGG5XSzxfzd2pqKv/4xz8YMGAAq1ev5t133wVg//79VKxYkVq1ann2PVuAOWv37t289tprdOzYkZ07d/LKK68of4ucR0nm7RUrVhAeHk5YWBijRo0CYN++fdx0000EBgaSlZXl+dzw4cNxuVzcd999ZGdnn/f4wcHBnn9TDh8+DEC3bt0ICQnxuq5BgwbRoEGDYrpKEcmLWtT4kJ49e9K7d2/PA1yPHj3o1q0bzZs3p0aNGl77xsTE8MILL7BhwwbKl3f/b4yNjeWhhx7yNFceMWKEZ4o/KFyLmoULF3L77bd7fk5LS2PChAm88cYbnnX/+9//uPfee9mxYwdt27YlISGBrVu3MmnSJAAaNmxI+/btCQgIICIigvDwcK688kpGjBhRoFhE5OKdHWx0w4YNHD9+3OtBrF69eqxevZrAwEDuvvtutm/f7jVw4bnatWvn9Vbw1KlTTJs2jZUrV3pN8anBRqUs8LX8PXfuXMCds0ePHk3v3r0BWLRoEd26dfPsl5WVxcCBA72KPhMmTPD8OTw8nOHDhwMof4s45GzejoqK8rwciYqK4ueff6ZmzZqsWrXKqzvjhg0byMjIIDk5mYkTJ7JkyRKv+/5cTZs2zTUg8euvv860adO81iUkJCiXi5Q0pwfJKasLJTRoWUH97W9/s61bt7ZHjx7Nc/v7779vV61aVaTn7Ny5s+3Zs2ee29AAZVr8cPn9/e2Lg40+8cQTdsiQIdblctkxY8bk2rd///52586deR4nNTXVXn755TY8PNxOmjTJWusedLxXr162Xbt2tl+/fvb48eOe/TXYqJbStPhr/s7Lxo0b7YwZMwp9TuVvLaVl8Ye8fVZWVpbt06ePPX36tGddZGSkzczMtNZa+84779hXX33VWmttYmKiHTly5HmPX7NmTRseHm5Hjhxpc3JyPOvPXte5lMu1aCnZRV2fxMs///lPUlJSzjul7h133OE1wGBRWLJkCe+9916RHlPEl/jiYKMHDhwgJCSEpKQkvvrqK7788kvPtm3btnHw4EEaN26c52cvu+wyvv76a5KSkkhMTGTbtm0cOHCAH3/8kWXLltGqVatcb+NEpHhdKH/n5eabb+a+++4r9DmVv6W08sW8DTB9+nQaNWpErVq1qFSpUp77NGrUyNNVcfXq1X84+PeePXv49NNPOXLkCB999NEFr0lESo4KNSIixSwmJoakpCR++uknz2CjmzZtom3btsTExBRosFGXy8X48eM9fcnPio+PJzk52Wtp167deWMKDg4mMjIScDeh3rVrF+CeLWbYsGHMmDHjvJ+tVKkSVapUoXz58nTp0oUdO3YQHBxMeHg45cqVIzo62nM8ERERf+OLeRsgLi6O3bt3s3fvXjZv3pznPs2bN6dJkyZERUVx7Ngx6tSpc97j1axZE2MM3bt3Z8eOHX94bhEpWSrUlFIul8trULHitGzZMq699lr1XRU5D18cbLRVq1Zs27YNgC1btlC/fn2ysrLo168fL7zwAnXr1vXs+/vBRn/99VfPnz/77DOuueYabr31Vk9x5uzxRKTgSjJ///rrr3Tt2pXWrVt7xrUREd/M22cn6ihXrhxVqlQhKCjovPvGx8eTlJRErVq16Ny5M5A7l584ccIz0PDZXC4ivkOFGrloLVu2ZOvWrU6HIeLTevbsyWuvvUaXLl2A3wYbHTRoUJ6DjS5fvtxrMNHY2FjS0tKIjo4mOjqaZcuWeX2moG/munTpwldffUVkZCQ5OTm0atWK9957jw0bNjBy5EhcLhfr1q3zDDZ6rn//+9/cfPPNtGrVissvv5zQ0FAuvfRSIiMjiYiIYNasWTz44IMX+Y2JSHF744036Nu3L59++ikJCQlkZGQ4HZKIz/C1vD1r1ixcLhfh4eFcffXVXHvttWRmZtK2bVu2bt1Khw4d+Pzzz8nJycHlchETE0PFihUJDQ0F4O677/Y63p49e7j11ltp06YN//vf/+jZsycADz30EHPnzuWxxx7LVxcvESkmTg+SU1YXinAwwuzsbHv//ffbiIgI27FjR2vtb4OK5TXw2auvvmpDQ0Oty+WymzZtsgsWLLC33nqrjYqKsh9//HGh4zjfIGMXAw1QpsUPl6K8v4uSBhvVouXil9KUv3v27GkPHDhgrbV22LBhdtu2bUV2bbq/tfjTUprydl4OHjxo4+PjC/35+++/34aHh+e5Tfe6Fi3FsxhrrTMVojLOGGOL6rtfuHAhGzZs4JlnniEnJ4eAgABcLheJiYlkZGRQuXJlsrKycLlcpKSk0L59exYtWkRQUBDWWu655x7GjRvHVVdd5f5LcU4/27Fjx7J69Wqv840aNSrPin94eDgpKSlFck1nGWOw1poL7yniO4ry/i7NdH+LPypN+btdu3Z89NFHBAYGMnr0aNq3b09ERESRXJvub/EnytuFp3tdpHiUdzoAuXhff/01rVq1AiAgwLs326ZNmxgzZgyZmZmegc/GjBnDkCFDqFixIk8//TSjR49m3LhxZGVlMWrUKBo2bOj5fHx8PPHx8SV3MSIiImWE0/k7JCSEY8eOERgYyLFjxwgJCSnaCxQREZFC0Rg1pUCjRo1Yv349ADk5OV7b8hr4rHnz5syePRuXy8Xs2bOpV68eCQkJxMXFMWnSJK/PF2bqQBEpXhpsVKR0cDp/h4WFsWrVKrKzs9myZQuNGjUqxqsVEeVvEckvdX1ySFE2sczJyeGBBx5gz549VK1alaVLl3qaTs+dO5fJkyfTvHlztm3bxubNmxkwYACpqamkp6cza9Ys5syZw/r16zl+/DgTJ07E5XIV6PwbN27k8ccfZ+PGjdxyyy0sWbKEwMDAIrk2NacUf1TcTajP3t/lyxd/o8hJkyZRt25d7rzzTqKiokhMTKRixYpFcmzd3+KPSlP+PnbsGHfddReHDx8mLi4u18DhF0P3t/iTkur6VFry97l0r4sUDxVqHKK+sPmjf/zFHxX1L3JxcXHs2bOHypUrs2zZMs+DXmJiIs8++ywnTpzgoYceon///kyZMoU333yToKAgJk6cyHfffcezzz5L1apV+fvf/85tt91WoPP36tWLKVOmULt2bYYPH05cXBxNmzYtkmvT/S3+SPk7f3R/iz8pjvu6NOfvc+leFykeGqNGRMSHLVq0iNq1a5OQkJCra0RERAQdO3b0DDbav39/Fi1aRFJSkmew0UmTJvHuu+96Bhs9V34GGz169CjVq1cHIDg4mCNHjhTTlYqIiJQeyt8icjFUqBER8WEabFRERMT/KH+LyMXQYMIiIj5Mg42KiIj4H+VvEbkYGqPGIerjnj/q9yr+SION5o/ub/FHyt/5o/tb/ElxjVFTWvP3uXSvixQPFWocoge9/NE//uKPdH/nj+5v8Ue6v/NH97f4E93Xhad7XaR4aIwahwQGBh4wxtRxOg5fFxgYeMDpGERERM5S/s4f5W8REZHCU4saEZEipjdz+aO3cCIi4guUtwtPuVykeGgwYRERERERERERH6FCjYiIiIiIiIiIj1ChRkRERERERETER2gwYRGRIqbBRvNHg42KiIgvUN4uPOVykeKhwYRFRIqBMWYgMAHoaa39t8Ph+ARjTDXgPSAL6GOtPe5wSCIiIl6MMQOA54Fe1tpPnY7HFyh/i5Q8dX0SESlCxi0eeBJwqUjzG2vtr0BX4ACQbIyp63BIIiIigCd/PwE8hTt/q0hzhvK3SMlToUZEpIgYYyoACUAsEGat3eVwSD7HWpsJDAIWAWuNMdc6HJKIiJRxZ/L3G0B3lL/zpPwtUrI0Ro2ISBEwxlTH3Sw4E/ebODULPg/r7nP7tDHme9xv5nqp5ZGIiDjhnG492UCk8vf5KX+LlBy1qBERuUjGmMuBT4FUoLse8vLHWjsH6Ad8YIy50+l4RESkbDknf38HdFP+zh/lb5Hip0KNiMhFMMY0AdYB7wBDrLVZDofkV6y1iUBb4AVjzN+NMcbpmEREpPQzxlyPO3+/Czyo/F0wyt8ixUuzPomIFJIxJhp3gWaEtfYtp+PxZ8aYPwFLcb/ZfNham+1wSCIiUkoZY6KAfwGPWGvnOx2PP1P+FikeKtSIiBSCMaYfMBG401qb7HA4pYIxJhh4HzgJ9LXWnnQ4JBERKWWMMXcDk3BPM53kdDylgfK3SNFT1ycRkQI4M33nKGAcEK0iTdGx1v4CdAaOAEnGmNoOhyQiIqXEmfz9f8B43PlbRZoiovwtUvRUqBERySdjTHlgGnAH0Mpau9PhkEoda20GcC+wHFhnjPmLwyGJiIifO5O/Xwd6ofxdLJS/RYqWpucWEckHY0xV3AMOGtzTd/7qcEil1pnpP580xnwHfGqMucNa+5nTcYmIiP85k7//BZQDIpS/i4/yt0jRUYsaEZELMMbUBdYAPwCxesgrGdbamcAAYKEx5g6n4xEREf9yTv7+Eeiq/F0ylL9FLp4KNSIif8AYcx3u6Ts/BOKstZnORlS2WGs/AdoDk40xIxwOR0RE/MQ5+Xsh8IDyd8lS/ha5OJr1SUTkPIwxEcB7wKPW2rlOx1OWGWOuxD39ZyLwN03/KSIi56P87TuUv0UKR4UaEZE8GGP6AC/jnmZyldPxCBhjQnC/GT0M9LPWnnI2IhER8TXK375H+Vuk4NT1SUTkHGem73wMeB6I0UOe77DWHgU6AqeAVcaYS5yNSEREfIXyt+9S/hYpOBVqRETOODN95xTgbtzTd253OCT5HWttOnAPkASsNcY0cDgkERFxmPK371P+FikYTc8tIgIYY6oAbwOBQBtr7TGHQ5LzODP956gz03/+2xjT3Vr7udNxiYhIyVP+9h/K3yL5pxY1IlLmGWPq4H7DcwjorIc8/2CtnQ7cD3xkjOnucDgiIlLClL/9k/K3yIWpUCMiZZoxphHu6TuXAvdp+k7/Yq1dCnQCphhjhjsdj4iIlAzlb/+m/C3yxzTrk4iUWcaYcOB94B/W2llOxyOFZ4y5ClgGfAw8Zq3NcTYiEREpLsrfpYfyt0jeVKgRkTLJGNML98CD/ay1K5yORy6eMaYG8CFwAOhvrT3tbEQiIlLUlL9LH+VvkdzU9UlEypQz03f+DZgEtNdDXulhrT0CtAeygURjTC2HQxIRkSKi/F16KX+L5KZCjYiUGcaYcsBkYCDu6Tu3OBqQFLkz03/eDaQAnxljrnY4JBERuUjK36Wf8reIN03PLSJlgjGmMjAfqI57+s6jzkYkxeVM//bHjTHfAynGmG7W2g1OxyUiIgWn/F12KH+L/EYtakSk1DPGXAqsBn4FOukhr2yw1k4FBgMfG2O6Oh2PiIgUjPJ32aT8LaJCjYiUcsaYhrin71wJDLDWZjgckpQga+1HQGdgmjFmiNPxiIhI/ih/l23K31LWadYnESm1jDFhwELgCWvtG07HI84509d9Ge5ZJf6h6T9FRHyX8recpfwtZZUKNSJSKhljbgdex/0WbpnT8YjzzswisQj4HzDwzMCFIiLiQ5S/5feUv6UsUtcnESl1jDEPA68AHfWQJ2dZaw8BbYFywApjTA2HQxIRkXMof0telL+lLFKhRkRKDWNMgDHmRdwD0LWy1n7pdEziW6y1p4E+wAbc039e5WxEIiKi/C0XovwtZY2m5xaRUsEYEwS8CVwCtLbWHnE4JPFRZ/q3/90Y8x3uh71Ya+0mp+MSESmLlL8lv5S/pSxRixoR8XvGmEuARCAD6KCHPMkPa+0rwF+BZcaY25yOR0SkrFH+lsJQ/payQIUaEfFrxphrgLXAp0A/DTAnBWGt/RCIBWYYY+IcDkdEpMxQ/paLofwtpZ1mfRIRv2WMCcU9XeMYa+3rDocjfswY0wD39J/vAqOtkqOISLFR/paiovwtpZUKNSLil4wx3YAE4F5r7RKn4xH/Z4y5FFgM/Be4z1qb4XBIIiKljvK3FDXlbymN1PVJRPyOMWYY8BrQSQ95UlSstT8DMUBlYLkxJsTZiEREShflbykOyt9SGqlQIyJ+48z0nS8Aw3DPDLHR6ZikdLHWngR6AduBFGPMlQ6HJCLi95S/pbgpf0tpo0KNiPgFY0wg8DbQEmhlrU11OCQppay12dbah4EZwFpjTHOHQxIR8VvK31JSlL+lNFGhRkR8njGmJrDyzI/trLWHnYxHygZr7YvACGCFMaaDw+GIiPgd5W9xgvK3lAYq1IiITzPG1Mc9fec6oK+19rTDIUkZYq19H+gBzDHG3O90PCIi/kL5W5yk/C3+TrM+iYjPMsbcgnsU//HW2ilOxyNllzHmL7in/5wPPKnpP0VEzk/5W3yF8rf4KxVqRMQnGWO6ALOAQdbaRU7HI2KMqQ0sAXYBD2j6TxGR3JS/xdcof4s/UqFGRHyOMeZB4Emgu7X2c6fjETnLGFMFeAv3FKA9rbW/OBySiIjjjDGTgQXAdSh/iw9S/hZ/o0KNiPgMY0wA8AxwO9DJWvtfh0MSycUYUw6YDEQAt1lr9zockoiIY4wxlwDfALOB21D+Fh+l/C3+RIMJi4hPMMZUAuYBbXBP36mHPPFJ1tpsYDgwF/f0n80cDklExEmDgMNAJDATd4sFEZ+j/C3+RIUaEXGcMaYG8AlQEWhrrT3ocEgif8i6/RN4FEg0xrRzOiYREYeMAuoBIcDVQLqj0Yj8AeVv8Rfq+iQijjLG1MM9Gv9y4O/W2hyHQxIpEGNMG+B9YKS1drbD4YiIlChjzBDgE2vtt07HIlIQyt/iy1SoEZESZ4zpB3wHnAA+Ap631k52NiqRwjPGXAcsxT3TydPAq8BDZ5pZi4iIiA9S/hZfpUKNiJQoY0x5IBV4FhgDDLbWLnA2KpGLZ4ypi3v6z61AM+Apa+3HzkYlIhcjKCho/+nTp+s4HYevCwwMPHDq1Km6TschUhjK3+KLVKgRkRJljIkFXsQ92OAoYIVG3ZfS4MxsEh2Bh4ArgP9Zazs5G5WIXAxjjNWz8oUZY7DWGqfjECkM5W/xRSrUiEiJMsbsxj3Y4Cnc3Z8etdYudzYqkYtnjKkC/Av3zCeZQA2gmbV2u6OBiUihqVCTPyrUiD9T/hZfVN7pAESkzFkHTAQWWWsPOB2MSFGx1p4AupyZar4l7ilA9RueiIiID1P+Fl+kFjUiIiIiInm4mBY1+/fvZ8aMGYwaNYrw8HBSUlKKOLr8+/XXX+nevTuZmZlUr16dt99+m2rVqnm2Z2Vlce+995KamkqXLl14/PHHC3R8tagRESlaAU4HICIiIiJS2tStW5dRo0YV2/EzMzPJzMzM174VKlRg3rx5fPrpp3Tr1o3Zs2d7bV+8eDHXXXcdKSkppKSksH///mKIWERE8kuFGpESEBQUtN8YY7X88RIUFKQnQ/Frutd1T0vZtXbtWkJDQ4mOjmbmzJmkpaXRr18/wN1iJS4ujltuuYUlS5YAMGDAACIjI4mKiiInJ4eBAwcydOhQIiIiiI+PP+950tLS+L//+z+io6P55Zdf8hVbYGAgl112GQDly5enXLlyXtvXrVtH27ZtAYiKimLDhg0Fvn4Rf6FcrbzuDzRGjUgJOH36dB11M7wwY4ymQBW/pnvdm+5pKUuWLl3KhAkTcLlcWGv57rvvPNt+/vlnRo8eTa1atWjfvj0dOnRg7969rFmzBmstxrh7DbVu3ZqpU6fSpUsXfvjhB6644gqv4yckJFC5cmXuv/9+nnnmGQC+//57+vfv7xXLFVdcwfz583PFePz4caZPn86yZcu81h89epTq1asDEBwczJEjR4rmSxHxQcrVhae8XnJUqBERERERuUhDhw5l3LhxzJgxg+HDh1O7dm3Ptlq1anHllVcCUK5cOSpUqMCAAQPo168f9erV4+mnnwbgxhtvBKBp06akpqZ6FWrmzZtHVlYWgwcPpk2bNp71V155JcnJyReMz1rLfffdx/jx4wkJCfHaFhISwrFjxwA4duwYDRo0KNR3ICIiRUNdn0RERERELlKNGjWYOnUqEyZM4Mknn/TadvjwYfbu3cvJkyfJzs4mOzubvn37Mm/ePH7++WdPV6OtW7cCsGPHDq666iqvY7z11ltMnjyZjz/+GJfLxaRJk0hPT+f777/H5XJ5LXfffXeu+OLj42ndujXR0dG5toWFhbFq1SoAkpKSuPXWW4viKxERkUJSoUbEj+zfv5/x48efd/vw4cMLddzVq1cTFhZGVFQUe/fuzXOfSZMmER4e7nUul8vFfffdR3Z2tmf9l19+iTGGrKysQsUiIr9x4p7ft28fN910E4GBgbqPRQpg2rRpRERE0KVLFwYOHOi17ZJLLuGpp54iIiKCkSNH8uuvvxITE0Pr1q353//+R9OmTQFYs2YNbdq04YYbbuBPf/pTrnPUr1+f5557jhUrVnD55Zdz4sQJT4uac5ffd3vat28fEyZMYOHChbhcLl577TUARowYQXZ2Nl27dmXHjh2Eh4cTFhbmGc9GRC7Ml3L175/P9+/f7yngXnfddYwYMaJQsUjJ0/TcIiXAmMJP71kSoqKiWLx4MV999RVz585lypQpXtvT09OJi4vjv//9LykpKWzYsIGEhASmTZvGxIkTadCgAd26dQPg/vvvZ8uWLXz++eeUL1+w3pVG03uKn/P1e/2sP7rnT58+zalTp+jRoweJiYkFvo/PpXta/F1J3tMDBw5k9OjRftntSPe6+JPSmqv/6Pkc4OGHH6Zr166egcMLQ/d6yVGLGhEflZ6eTmxsLB07dqRPnz7Mnj3bawaJsLAwhg0bRvPmzVm+fDmAV4uX/Dp58iRBQUFUq1aN0NBQvvrqq1z7JCQkMGDAAM/P3377Lc2aNQOgefPmrFu3DoCdO3fy5z//mWrVqhU4DpGyzlfu+cDAQGrUqHHxFyQiIlLK+HKuPt/z+VmffvopLperwLGIM1SoEfFRH374Ia1atWL58uV5/tJ06NAh4uPj+fjjj5k2bVqex8jIyMjVbz0mJsZrnyNHjnhmegC8ujEBZGZmsmbNGq8+7Y0aNWLNmjWAu1nm2dkhXnzxRYYNG1a4CxYp43zlnhcRZ8yePdsvW9OIlCW+nKvP93wOsHHjRpo1a3ZRrWSlZOn/lIiPSk1N9aqK/96ll17qmVHi6NGjeR6jYsWKF5wJokaNGp6ZHgACArzrt2+++SZ33XWX17rmzZvTpEkToqKiaNKkCXXq1GHPnj0EBwdzySWXXODKRCQvvnLPi4iISN58OVfn9Xx+1sKFC7n99tsveAzxHXo6E/FR9evXZ/v27QBs27Yt13Zjfuseer5+tvmp2FeuXJlTp05x/PhxvvjiCxo3buy1fffu3bz22mt07NiRnTt38sorrwDu2SOSkpKoVasWnTt3Zvv27WzYsIGOHTuybds2HnzwwYu6fpGyxlfueREpGU4MQJqWlkadOnVwuVy0b9++UMcXKct8PVf//vn8rBUrVuie9zNqUSPio7p3706vXr3o0KEDVatWpUKFCgU+Rn4q9gCjRo2iXbt2BAYGMmfOHACee+457rnnHiZMmODZLzw8nOHDh5OTk0N0dDTlypUjJiaG0NBQAE+l3uVy8frrrxc4XpGyzFfu+dq1a9OpUye2bt1Khw4deOaZZzz3uIgUnbp16zJq1Kjzbj/7YqSgnn76aVasWMFXX33Fs88+m2uCgHbt2jFv3rxCHVukrPPlXH3rrbfm+Xy+e/du6tWrR1BQUIFjFedo1ieRElDY0eWzsrIoX748Q4YMoX///oSFhRVDdL5DI8mLv7vYmSRK2z2ve1r8XVHNDpOenk6vXr3IyMggJCSEjh074nK5GD16NPPmzSMsLIybb76ZlJQUnnvuOTp27Eh4eDgpKSkFOs/Jkyfp2bMnS5cuBdyzxiQlJXm2p6Wl0bp1a66++mpuv/12HnnkkYu+NtC9Lv5FubrwdK+XHLWoEfFhnTt35vjx4zRo0KBMJQGRskr3vEjpdHYA0scff5whQ4bk2n52ANLMzEyGDRtGx44dc+2TkZGRq+tCuXLlWLVqlefnCw1Aetlll/H1119TqVIlunXrRkxMjGe8DRHJH+VqKQkq1Ij4sE8++cTpEESkBOmeFymdfGUA0kqVKlGpUiUAunTpwo4dO1SoESkg5WopCRpMWKQMCA8PL7Zjv/nmm4SFhdGxY0f2798PwMqVK2nZsiVRUVH85z//KbZzi0jx3t9Dhw7l0ksvJSEhwbNO97dIwfnKAKS//vqr58+fffYZ11xzTaGvSUQKpzjzdu/evYmMjCQ8PJzdu3d71p86dYq6deuSmJjoWWet5YYbbvDK8eI71KJGRAotKyuLqVOnkpKSwpYtW5gwYQIvvvgiY8eOZdWqVRw7dowRI0bwr3/9y+lQRaQQnnjiCVq0aEFWVpZnne5vkYLzlQFIt27dyhNPPEGlSpUIDw/XQOEipcz8+fOpUKECa9as4eWXX/YMJj59+nSaNGnite/ixYs9LfnE96hFjYiPWLt2LaGhoURHRzNz5kwyMzOJiYkhIiKCO+64g+zsbNLS0oiOjqZ3797ceOONLFiwgPbt2xMeHs6JEydIS0sjKiqKbt260bJlS1JTU73O8fnnn+NyuWjdujWzZs0CYMCAAURGRhIVFUVOTk6BYj506BB/+tOfKFeuHDfccAPr16/3bKtSpQqXXXYZ//3vfy/+yxHxc/54f4N7PIu86P4WKZhKlSqxYMECPvnkE2rXrs3VV1/NVVdd5Zl96dxBg88WYwo6kPBZbdu2Zd26dSQlJXHllVcC8Pjjj3PFFVdw2223sWnTJtauXcvzzz9/cRclUor5a94+WwQ+fvy4p1tjRkYGn3/+ea6WPG+//TZ33nlnYb4eKQFqUSPiI5YuXcqECRNwuVyeZs9LliwhKCiI0aNHs3r1aho2bMiRI0dITEzknXfeYc6cOaxYsYJnnnmGTz75hJtuuon9+/ezcuVKNm3axIQJE7ymyY6Pj2fx4sVUq1aNdu3acdddd7F3717WrFmDtdar6XV+Bi285JJLSE1N5cSJE6xdu5bDhw97th04cIAjR47w/9u797Cq6vT//683oEIqiJZZNqmfRMvJT5Ymoiibg0lppJMdbBS1jJkccXI6OT+NxLJzNjYd1ME8VWNlNjmamoiYppiY4iGvst8nZiwLNXUQIwV8f/9A9rQTCxD22huej+va14Vr7bXWvXbdLPa93ut979mzp64+MsBv+GN+/xzyG6g+JiAF/Ie/XrdPnjypuLg47d+/X++++64kae7cuRoxYoQ2b97sft+qVasUExOjoKAgj1Gz8B0UagAfMXbsWD322GOaM2eOUlNT1aVLF6WkpOjrr79WQUGBIiIiFBERoS5duiggIEAXX3yxewjjxRdfrCNHjkiSunbtqqCgIHXr1k1ffPGFxzHy8vKUlJQkSTp06JAOHTqkkSNHavjw4WrXrp0effRR98SDVRliHRgYqLS0NN1www26+uqr1alTJ0nS008/rdtvv13t2rVTnz59avNjAvySP+b32ZDfQM0wASngP/z1ut24cWNt2LBBW7duVVpamt555x2tWrVKS5Ys8SjUZGRkaOHChVq0aFFtfFyoAxRqAB8RHh6ul19+Wfv379ddd92lMWPGqFOnTnrjjTc0adIkdzX/x9X1yiYf3LVrl8rKypSXl3fGJIFXX321Fi9erKZNm6qkpEQBAQEaNmyYkpOTlZKSoi1btrifV69q5T4pKUlJSUnKzs7Wxx9/LEmKiorS2rVrtXfvXr344ou19AkB/stf87sy5DfgPdHR0TV+BOqX3HjjjTp69KgaN26s+fPn65JLLtHhw4f1+9//XocOHVJ8fLwmTZpUJ8cGfJ0/XrettSotLVWjRo0UGhqqkJAQFRQUaN++fUpMTNQXX3yh5cuXq3v37tq7d68GDx6sr7/+WtZaRUdH6/LLL6/dDxHnhEIN4CNmzZqlJUuWqKioSA899JAiIyM1bdo05ebmKiwsTBEREVXaT+vWrTV48GAdPHhQr7/+use69PR0JSUl6dSpU2rZsqXmzJmjpKQklZWVKTQ0VF27dnW/t6qV+9TUVO3evVvt2rXTyy+/LEmaNm2aMjMz1apVK82aNavqHwJQT/lrfk+bNk1vvPGGrLXav3+/0tLSyG+gnnjhhRfUoUMHrV69Ws8//7yee+45paena+rUqXxhQ4Pnj9ftEydOKDExUcYYGWP00ksvqW3bttqyZYskacqUKYqOjlZ4eLi2b98uSZo3b55KS0vJeR9kztYCEEDtMcZYb+Rafn6+Jk+e7J6c0N8YY2StNb/8TsA31WWu+2N+k9Pwd1XJ6Y0bN2rChAlq2rSphg8frhEjRigxMVElJSW64IIL9NZbb2nfvn268847df7552vv3r16+OGHNXPmTH3//fdatWqVDh48qNGjRys0NFQFBQX6+9//rg4dOrhH1GzevFkPPfSQSkpKNGbMGI0ePVojR45Ufn6+AgICtGbNGvcjEtW1du1arVixQk8//bTi4+PVtm1b7du3T48//niV59Ih1+FP+Lu85sh172FEDQAAAFBD/jrpqCSVlZVp2rRp7tFxGzdu1CeffKKWLVvq5ptvrrPHrgAAP49CDVCP/LjVJ4D6hfwGfJO/TjoqSffdd5+Sk5Pdc2d06tRJV1xxhSTVeIQOgHJct3Eu+A0M+LApU6YoMzOzzvY/atQoRUZG6j//+Y972fjx4zV8+HCP902fPl3R0dFn3U9+fr4uvPBCuVwuj7t4YWFhcrlccrlc7tbdgwYN+tl9AQ2RN3N927Zt6tq1q9q3b+9e//3332vgwIFyuVy66aabdOLEiUr3s3//fl1zzTUKDg52t/MsKipSr169zvi9ATQUFZOOPvXUU3rkkUe0atUqderUSevWrdPNN99ca5OOLl++XNnZ2dq2bZvatGmjYcOG6bXXXtPBgwfdc1BI5SNqKq69Fa/4+Pgz4p4zZ46MMUpOTnYv69Spk7755hsdP36clr3AT3jzWv3BBx8oOjpaUVFRHpN6T5gwQX379tUf//hH97LVq1crLi5OLpdLW7durXTfhw4dUu/evRUTE6OkpCQVFxdLkh588EH16dNHffv21d69eyVJY8aMUceOHevsPFE1FGqABu71119XWFiYJKmgoED5+fke60+cOKG8vLxf3E///v2VnZ2tDz74wL2sa9euys7OVnZ2tlq2bCmpfDg4AO+ryPWOHTsqJydHl1xyiXvdypUrFRkZqezsbPXs2VMrV66sdB8tW7bUmjVr1KtXL/eyZs2a0d4TDdqsWbPUr18/DRo0yP1Fa+nSpRo0aNAZ19SfUzHp6Pjx4/Xggw96rKuYdDQ2Nla33367jh07pvj4ePXp00f79u2rdNLRH78qe+xp7Nixys3Nlcvl0iOPPOI+zrBhwxQXF6fJkyfX7AMBUGMV1+rY2Fht2LBBmzZt0saNG3Xw4EF98sknOn78uNavX6+TJ09qy5YtKi4u1qxZs7R69WplZ2ere/fule43PDxcGzZs0Lp169S9e3ctW7ZMhw8fVm5urj766CM9+eST7qYgGRkZatOmjTdPG5Xg0SfAAXfffbf+9Kc/6YorrtALL7ygiy66SM2bN9cTTzyh48ePa/z48R53uCpmZB8zZoymTJkil8ulmJgYjR07Vp999plCQkL02muvKTw8/Jziev7555Wamqr58+e7l2VkZGjkyJFKS0v72W3Xrl2rvn376je/+Y0mTJggSdqzZ4/69u2rPn366IknnvC4gwg0BL6Y682bNz9j2WWXXea+C3f06FG1atWq0m2Dg4MVHBxc42MD9dG9996re++912PZJ598csb7Kh6BqBjlIpXfQZfKR6ZecsklZzwmUTFHTM+ePc8otnz44YfnFHdlI+e6dOlS5cemgPrCF6/VjRo1klQ+j1SbNm0UGhqqTZs2KSEhQZKUkJCgnJwcHTt2TAEBAbr++ut14YUXaubMmWratOkZ+wsMDHT/XFZWpoiICDVr1kytWrVSWVnZz1774QxG1AAOGDp0qBYvXiyp/E72DTfcoH79+mndunXKycnR7Nmzf3Efy5Yt06WXXqqsrCyNGzfOY9JBSZo6deoZQ59Xr1591v0dPnxYBw8e9Gg3WFJSonXr1ikuLu5nY7nooov0+eefa+3atcrMzNSOHTskSXv37tWHH36oI0eO6J///OcvnhNQ3/hirlcmIiJCmzdv1q9//Wvl5uaqd+/e1doeAAB/5avX6tmzZ6tz585q1aqVmjRpoqNHjyo0NFRS+fQCR44cUUFBgb755hutWLFCvXv3dk8MXpmPP/5YPXr0UFZWljp06KDGjRurY8eO6ty5s/7whz/orrvu+sXzhPdQqAEcEB8fr7Vr1+rAgQNq3ry5mjZtqq1btyohIUHx8fHavXu3x/sre5Z9z549WrRokVwul6ZNm+aeA6ZCWlraGUOf+/fvf9aYZsyYoXHjxnksW7hwoe64445fPJ8mTZqoadOmCgoK0qBBg7Rr1y5J5Y9JGGM0ePBg9zKgIfHFXK/M/PnzNWDAAO3evVsDBw5k8kPAy5h0FHCOr16rU1JS9Nlnn+mrr77Stm3b1KJFCxUWFkqSCgsL1aJFC4WFhSk6OlqBgYGKi4vTnj17zrq/nj17Kjc3V0OGDNGrr76qPXv2aNeuXfr888+1ePFij7lw4DwefQIcEBQUpPbt2+uZZ57R4MGDJUlPP/20MjIy1LZtW49RLVJ51Xznzp2SpJ07dyo2NladO3dWcnKy7rvvPknlo19+bOrUqcrKyvJYNmnSpLNeFL788kv9+c9/VnFxsfbu3au33npLn332mbZv366ZM2dq9+7d+utf/6rU1FR9/fXXatu2rXvbY8eOuR+n+Oijj5Samqrjx48rODhYgYGB+uijjzyenwcaCl/M9cpYa93zSJ1//vnuCcZ/musAANQ3vnitPnHihJo0aaLAwEA1bdpUISEhioqK0qxZs3TrrbcqMzNTo0aNUocOHdwjfrZv364OHTpIOvP6ffLkSTVu3FiSFBoaqrKyMllr1aJFCwUEBHhc++EbKNQADhk6dKhuvfVWffPNN5KkIUOG6KabblK3bt3OeKY1Pj5ezzzzjLZs2aKgoPK0TUpK0vjx492PJd17773u1p1SeeX+l+aV+bEFCxZIKn9OfvLkybr11lt16623utdHR0crNTVVpaWlGjVqlMdwzfXr1+vhhx9WkyZNFB0drcjISG3fvl133nmnmjZtqv/5n/9Renp6NT8hoH7wtVzft2+fRo8erV27dikhIUEZGRm64447dNttt2nhwoVq1KiR3nzzzUpzvaSkRNdff73y8vI0YMAAPf7444qMjKzxZwPUV1OmTFF0dLR7PonaNmrUKO3Zs0cffPCB/u///k/Jyck6duyYe/LiXbt2KSUlRYGBgerYsaNeffXVSueJ279/vwYNGqRPP/1URUVFCgoK0qFDh5SUlKRGjRopLCxMb775pkJCQjRo0CAdPXrUPW8OUJ/42rV67ty5WrRokUpLSxUbG6vLL79cUvlccX379tVVV12lnj17SpJiYmLUr18/nXfeeXrjjTckSb/97W895pvavn27HnjgAQUEBKhly5ZauHChzjvvPDVv3lx9+/ZVaWmpZsyYUc1PDXXJVAzXAlB3jDHWF3Pt/vvvV05OjpYvX+7u/PRLtm7dqry8PN155501OuagQYMUEhKit99++4x1xhhZa5lxGH6rIeZ6UVGREhMTde211+r555/3WEdOw9/VNKe9UaiZPHmyOnbs6J5MdMCAAe4iSklJiXsy0tGjR2vs2LG69tprz9jPDz/8oOLiYg0ZMkSZmZkKCgpSWVmZjDEKCAhQenq6unTpoltuuUVS+U2bygo15Dr8SX26Vlfmu+++0wsvvFDjm6RjxozRZ599pvXr15+xjlz3HkbUAA3Ys88+W+1tunfvftbWf1VBe27A++oy15s1a8YddjQYvtgdprJObhVFGql8Hrlf/epXlW5bWSe3yrrDAKh7NblWV6ZVq1bnNJI9IyOjVuLAuWEyYQAAAKAKfLU7TGWWLl2qK6+8UgcOHKh2292fdocBAHgXhRoAAACgCny1O0xlkpKStGvXLrVt27bao1l/2h0GAOBdFGqAesDlcqm0tNQrx1qxYoUuv/xyRUdHe+V4QEPnzfw+duyYbrzxRvXp08c9wTiA//q57jCZmZlnzCsRFhbmnpy0oktMRXeY7OxsbdiwQY8//rjHNrUxoubEiRPun0NDQxUSEiKpvBPMLzl58mSl2wKoHq7fOBcUagBUS69evZSXl+d0GADqwN/+9jcNGzZMH374oTIyMjy+sAEoN3ToUL3yyisaNGiQpP92hxkzZkyl3WFWrlzp0f0lKSlJ+fn5iouLU1xcnFasWOGxTXVH1Ozbt08JCQnuTm75+flauXKlYmJiFBMTo4KCAl133XXuTm4/VlJSooSEBHcnt82bN2v79u2KiYlRbGysVq5c6THnDgDfxPW7HrLW8uLFq45f5alWO8rKyuxdd91l+/XrZxMTE6211sbExNiSkhK7YsUK269fP9u9e3c7f/58a621L774oo2MjLQul8tu3brVLlmyxF577bU2NjbWLl++vMZx9OnTp1bO58dOf06O//fixaumr3PNdafze+jQobagoMBaa+24cePsjh07zul8yGle/v6qzet3bbrvvvtsnz597NGjR6u8TW5urp0zZ06Njzlw4EA7dOjQSteR67z86VUXeV3frt9nQ65770XXJ8DPvPfee2rdurUyMjJ06tQpj3X9+vVTYmKiSktL5XK5lJycrPfee09r165VSEiIrLWaPn263nrrLbVv377iYuU2depUZWVleSybNGlSjZ6NB1B9Tuf30aNHFRoaKqn8kY0jR47U0ZkCOBd0bQR8C9dv1DYKNYCf+fzzz9W7d29JUkCA59OLW7duVXp6ukpKStwTGqanp+uee+5R48aN9eijj2ry5Ml67LHHVFpaqkmTJnm03UxLS1NaWpr3TgaAB6fzu0WLFiosLFRwcLAKCwvVokWL2j1BAADqIa7fqG3MUQP4mc6dOysnJ0eSzqjYVzahYbdu3TRv3jy5XC7NmzdP7dq1U0ZGhlJSUjR9+nSP7WurJSiAmnE6v6OiorRmzRqVlZVp+/bt6ty5cx2eLQAmGwXqB67fqG3mp0OrANQ+Y4ytrVw7deqU7r77bu3du1fNmjXT+++/L5fLpczMTC1YsEAzZsxQt27dtGPHDm3btk0jR47Ul19+qRMnTmju3LmaP3++cnJyVFRUpOeee04ul6tax8/NzdXEiROVm5urHj16aNmyZQoODq6VczPGyFprfvmdgG8611x3Or8LCwt1xx136PDhw0pJSTlj4tHqIqfh72rz+l2ZivwOCqr7Qe7Tp09XmzZtdNtttyk2NlaZmZlq3LhxreybXIc/qYu8rm/X77Mh172HQg3gBXX9h159wS9/+Dty3RM5DX9X2zdaUlJStHfvXp133nlasWKF+4tcZmamnnjiCR0/flzjx49XcnKyXnrpJS1cuFAhISF67rnn9K9//UtPPPGEmjVrpvvvv1833HBDtY5/yy236KWXXlLr1q2VmpqqlJQUde3atVbOjVyHP+FaXXPkuvcwRw0AAABQx5hsFABQVRRqAAAAgDrGZKMAgKpiMmEAAACgjjHZKACgqpijBvACnoWtGp57hb8j1z2R0/B39akZQF1ONkquw59wra45ct17KNQAXsAFoWr45Q9/R657Iqfh78jpqiHX4U/I65oj172HOWoALwgODi4wxlzodBy+Ljg4uMDpGIBzQa57IqcBAL6Ga3XNcV33HkbUAAAAAJXgznvVcJcdAGoXkwkDAAAAAAD4CAo1AAAAAAAAPoJCDQAAAAAAgI9gMmEAAACgEkw6WjVMMAoAtYvJhAEAAIBKGGOaSHpVUgdJSdbaQw6H5BOMMUbS/ZJSJQ2y1u5wOCQAqFd49AkAAAD4CWNMuKRVkkIkxVOk+S9b7hlJD0rKNMYkOB0TANQnFGoAAACAHzHGtJO0QdJ2SbdYa4udjcg3WWsXSRoq6XVjzEin4wGA+oJHnwAAAIDTjDHXSPqnpGestX9xOBy/YIy5QtL7Kn9M7DHLFwwAOCcUagAAAABJxpjrJS2Q9Htr7TtOx+NPjDFtJC2XtE3SPdbaEodDAgC/xaNPAAAAaPCMMWMkzZV0E0Wa6rPWfispRtJFkv5pjGnucEgA4Lco1AAAAKDBMuUelTRRUj9r7UanY/JX1toiSTdJ+pekD40xFzscEgD4JQo1AAAAaJCMMY0lzZd0naTe1trPHQ7J71lrSyX9XtLbkjYZY37tcEgA4HeCnA4AAAAA8DZjTJikJZKOSYq11n7vcEj1xunJhB83xvxb0lpjzG3W2rVOxwUA/oIRNQAAAGhQjDG/Unn77T2SbqZIUzesta9Juk3Sm8aY3zodDwD4Cwo1AAAAaDCMMVdJ2ihpnqRUa22ZsxHVb6dH0sSpfITN/2eMMU7HBAC+jvbcAAAAaBCMMddJek3lBZo3nY6nITk9sfBySR9L+sPpuWwAAJWgUAMAAIB6zxgzWtKTkoZaa9c7HU9DdLpl99uSyiTddrpLFADgJ3j0CQAAAPXW6fbbUySlSXJRpHGOtfaYpBslfStpnTGmjcMhAYBPolADAACAeskY00jSq5IGSYqy1u5xOKQGz1pbImmMpH+ovH33Fc5GBAC+h/bcAAAAqHeMMaGSFks6ISnGWnvc4ZBw2un23Y+ebt+dbYy5xVr7odNxAYCvYEQNAAAA6hVjTFtJ6yX9/5KGUKTxTdba+ZJ+K2mxMeZ2p+MBAF9BoQYAAAD1hjGmq8rbb78haSzdhXybtTZTUoKkp40xD9C+GwDo+gQAAIB6whgTJ2mRpD9aa//udDyoOmPMJZLeV/lIqPHW2jKHQwIAx1CoAQAAgN8zxoyQ9KykW62165yOB9VnjAlT+bxCxZKG8cgagIaKR58AAADgt063354k6VFJsRRp/Je19j+SBko6LGmtMeZCh0MCAEdQqAEAAIBfOt1+e7akm1XefvtTh0PCObLWnpQ0WtIKSRuNMZ0dDgkAvI723AAAAPA7xpjmkt6SZCX1s9YWORwSasnp9t2PGGP+JWmdMeZma+1HTscFAN7CiBoAAAD4FWPMRZLWSfpKUhJFmvrJWvuqpJGS3jXGDHU6HgDwFgo1AAAA8BvGmC6SNql80tkU2m/Xb9baVZKuk/QXY8yfaN8NoCGg6xMAAAD8gjHGJelNSfdZa19zNhp4kzHmUpW3714j6U+07wZQn1GoAQAAgM8zxtwh6S+SbrfWZjkcDhxgjGkhaYmko5KGW2u/dzQgAKgjPPoEAAAAn3W6/fZESU9IiqNI03BZa49KSpR0XNIaY8wFzkYEAHWDQg0AAAB8kjEmSNLLkm6X1Ntau8vhkOCw0+27k1X+CNRGY0yEwyEBQK2jPTcAAAB8jjGmmaRFkhqpvP12ocMhwUecbt892Rjzb0kfGmN+Y63d5HRcAFBbGFEDAAAAn2KMaSMpW9IBSYMo0qAy1trZku6StNQYM8TpeACgtlCoAQAAgM8wxlwuaaOkpZLustaWOBwSfJi19n1JAyS9aIwZ73Q8AFAb6PoEAAAAn2CM6StpsaQHrbXznY4H/sMY017l7btXSrrfWnvK2YgAoOYo1AAAAMBxxpjbJP1V0m+ttaudjgf+xxgTLuldSYckjbDWFjscEgDUCI8+AQAAwDGn22/fL+kZSQkUaVBT1tojKn8MqkRSpjHmfIdDAoAaoVADAAAARxhjAlU+iiZZ5e23dzgcEvyctfaEpN9KWi/pI2PMZQ6HBADVRntuAAAAeJ0x5jxJf5fUVFJfa+1/HA4J9cTp+WkmGmP+JWmDMeYma+3HTscFAFXFiBoAAAB4lTGmtaS1ko5KuoEiDeqCtfYVSXdLWmaMSXI6HgCoKgo1AAAA8BpjTCdJm1TenWeUtfakwyGhHrPWLpM0UNJMY8wfnI4HAKqCrk8AAADwCmNMH0nvSJpkrZ3jdDxoOIwxHSStkLRU0kTadwPwZRRqAAAAUOeMMTdLekXlbZNXOR0PGh5jTCtJ/5D0tcpHc/3gbEQAUDkefQIAAECdMsZMkDRD0gCKNHCKtfY7Sf0lGUkfGGNaOhwSAFSKQg0AAADqhDEm0BjzF0l3qbz99jaHQ0IDd3oUzTBJmyVtPP1IFAD4FNpzAwAAoNYZY0IkvS4pXFK0tfaosxEB5U7PT/PAT9p35zodFwBUYEQNAAAAapUx5gJJWZK+l5RIkQa+yFr7oqSxklYYYwY5HQ8AVKBQAwAAgFpjjOkoaaPKCzUjrLUnHA4JOCtr7XuSBkn6mzHmd07HAwASXZ8AAABQS4wxvSS9K+kRa+1sp+MBqsoYc5nK23dXtI+nfTcAx1CoAQAAQI0YY4w9/cekMWaIpNmSRlpr33c2MqD6jDHnS1oq6UtJd1prT/z4/3EA8BYKNQAAAKgRY8wMSRsktZE0UVKStXars1EBNXd6EuzXJLWSNETSJknXWmuPORoYgAaFQg0AAACqzRgTJilf0iJJLknXW2vzHQwJqBXGmEBJz0q6TtK/Jb1nrZ3pbFQAGhImEwYAAEBNjFJ5V6eKeWk6OBoNUHuaSCqVtEVST0kTjDHG2ZAANCQUagAAAFATj0i66PTPjSTtczAWoDadlPQvlT/+FCKpk6RERyMC0KDw6BMAAACqzRiTKCnXWnvI6ViAumKMaSRpgKS11trjTscDoGGgUAMAAAAAAOAjgpwOAAAAwB+FhIR8+8MPP1zodBy+Ijg4uKC4uLiN03EAtYH89kR+A97FiBoAAIAaMMZY/o76L2OMrLVMuIp6gfz2RH4D3sVkwgAAAAAAAD6CQg0AAAAAAICPoFADAADgA7799ltNmzbtrOtTU1NrtN+srCxFRUUpNjZWX3311RnrJ0yYoL59++qPf/xjjfYPwJMTubx//35dc801Cg4OVmlpqSTp0KFD6t27t2JiYpSUlKTi4mJJUr9+/RQTE6P4+HgdOHCgRrEAqFvMUQMAAFAD/jKHRWxsrJYuXapPP/1UCxYs0EsvveRe98knn2jmzJmaPXu27rnnHt1555269tpra3Qc5rBAfeKL+f1zufzDDz+ouLhYQ4YMUWZmpoKCglRWViZjjAICApSenq4uXbrolltuUUlJiRo1aqT58+frwIEDeuCBB37x2OQ34F2MqAEAAPCyEydOKCkpSYmJibr99ts1b9485efna/jw4ZKkqKgojRs3Tt26ddPKlSslSdHR0dU+zvfff6+QkBA1b95ckZGR+vTTTz3Wb9q0SQkJCZKkhIQE5eTknOOZAQ2Lr+RycHCwwsPDPZYFBgYqIKD8615ZWZkiIiIkSY0aNZIkFRcX69e//nW1YwFQ9yjUAAAAeNk//vEP9e7dWytXrjzjy5Ukfffdd0pLS9Py5cs1a9asSvdx8uRJuVwuj1d8fLzHe44cOaLQ0FD3v8vKyjzWHz161L0+LCxMR44cOddTAxoUX8nls/n444/Vo0cPZWVlqUOHDpKkf//734qKitKLL76orl27VvVUAXhRkNMBAAAANDRffvml/vd//1eS1K1btzPWX3DBBWrdurWk8mJKZRo3bqzs7OyfPU54eLgKCwvd/664u16hRYsW7vWFhYVq0aJF1U4AgCTfyeWz6dmzp3Jzc/Xcc8/p1Vdf1YQJE3TppZdq06ZNeuedd/Tss89qxowZVdoXAO9hRA0AAICXdejQQTt37pQk7dix44z1xvx3KoizzZNRlbvw5513noqLi1VUVKSPP/5YXbp08VgfFRWlNWvWSJIyMzPVq1evczovoKHxlVw+234rhIaGKiQkRCUlJe44KpYB8D2MqAEAAPCywYMH65ZbbtGAAQPUrFkz95wR1VGVu/CSNGnSJPXv31/BwcGaP3++JOnJJ5/UiBEj3F1i+vbtq6uuuko9e/asdhxAQ+Yrudy6dWtdf/31ysvL04ABA/T444/LGKMHHnhAAQEBatmypRYuXKhvvvlGI0aMUEBAgJo0aaJ58+ZVO14AdY+uTwAAADVwrl1hSktLFRQUpHvuuUfJycmKioqqxei8j64wqE+qk9/1LZcrQ34D3sWIGgAAAAcMHDhQRUVF6tixY738Ygc0FOQygNrGiBoAAIAaONcRNfUNd9xRn5DfnshvwLuYTBgAAAAAAMBHUKgBAADwE9HR0XW277Fjx+qCCy5QRkaGe9nvfvc79enTR9HR0ZV2tAFQe7yd35JUXFysNm3aKDMzs86ODaD6mKMGAAAAevjhh9WzZ0+Vlpa6l02cOFEdOnTQ3r17NXHiRL3zzjsORgigpirLb0maPXu2rrzySoeiAnA2jKgBAACoRRs3blRkZKTi4uL06quvqqSkRPHx8erXr59uvvlmlZWVKT8/X3Fxcbr11lt19dVXa8mSJbruuusUHR2t48ePKz8/X7GxsbrpppvUq1cvffnllx7H2Lx5s1wul/r06aO5c+dKkkaOHKmYmBjFxsbq1KlT1Y77oosuOmNZhw4dJEmNGjVSYGBgDT4NoH6pT/l98uRJbd68uU5H8gCoGUbUAAAA1KL3339fTz31lFwulyomI122bJlCQkI0efJkZWVlKSIiQkeOHFFmZqYWLVqk+fPn64MPPtDjjz+uVatW6ZprrtG3336r1atXa+vWrXrqqac0c+ZM9zHS0tK0dOlSNW/eXP3799cdd9yhr776SuvWrZO1Vsb8d87PkydP6rrrrvOIMTAwUGvWrKnyOf35z3/W+PHjz/GTAfxffcrvuXPnasSIEdq8eXMtfToAaguFGgAAgFo0duxYPfbYY5ozZ45SU1PVpUsXpaSk6Ouvv1ZBQYEiIiIUERGhLl26KCAgQBdffLH70YOLL75YR44ckSR17dpVQUFB6tatm7744guPY+Tl5SkpKUmSdOjQIR06dEgjR47U8OHD1a5dOz366KMKCCgfON24cWNlZ2fX+Hz+8pe/qEuXLtx1B1R/8ru0tFSrVq3SkiVLKNQAPohCDQAAQC0KDw/Xyy+/rP379+uuu+7SmDFj1KlTJ73xxhuaNGmS+y78j++K//jnivW7du1SWVmZ8vLydNlll3kc4+qrr9bixYvVtGlTlZSUKCAgQMOGDVNycrJSUlK0ZcsWRUZGSjq3O+4ffPCBNm7cqDfffLNmHwZQz9SX/C4oKNC+ffuUmJioL774QsuXL1f37t0VHh5e8w8HQK2hUAMAAFCLZs2apSVLlqioqEgPPfSQIiMjNW3aNOXm5iosLEwRERFV2k/r1q01ePBgHTx4UK+//rrHuvT0dCUlJenUqVNq2bKl5syZo6SkJJWVlSk0NFRdu3Z1v7eqd9ynTZumN954Q9Za7d+/X2lpaUpNTVVoaKhiY2PVuXNnzZo1q1qfBVDf1Kf83rJliyRpypQpio6OpkgD+BBTUdUFAABA1RljbF39HZWfn6/Jkyfrtddeq5P91wVjjKy15pffCfg+8tsT+Q14F12fAAAAAAAAfAQjagAAAGqgLu+4+yPuuKM+Ib89kd+AdzGiBgAAwIumTJmizMzMOtv/qFGjFBkZqf/85z8qLS3ViBEjFB0drSeffPKs22zbtk1du3ZV+/bt3cs2b96s3r17q2/fvpowYYIkqaioSL169dLw4cPrLH7An3kzvyvL2127drnzdvTo0TpbsWn//v265pprFBwcrNLSUknkN+BLKNQAAADUM6+//rrCwsK0dOlSXXHFFdqwYYM2bNigb7/9ttL3d+zYUTk5Obrkkkvcy9q1a6esrCytX79eBw4c0M6dO9WsWTMtWrTIW6cBoBIV+V1Z3nbu3FkbN27U+vXrJUm5ubmV7qNly5Zas2aNevXq5V5GfgO+g0INAABALbj77ru1Z88eSdILL7ygt99+WytXrlRMTIx69OihBQsWeLx/3rx5ysjIkFR+Fz47O1vWWt1zzz2Ki4vTwIEDdeTIkXOKadOmTUpISJAkxcbGuru8/FTz5s3VtGlTj2Vt2rRRcHCwJCkoKEiBgYHnFAvgz3wxvyvL20aNGrl/btKkiX71q19Vum1wcDBdngAfRqEGAACgFgwdOlSLFy+WJK1cuVI33HCD+vXrp3Xr1iknJ0ezZ8/+xX0sW7ZMl156qbKysjRu3DjNnDnTY/3UqVPlcrk8XqtXrz7r/o4eParQ0FBJUlhYWI2+GO7YsUOHDh1Sly5dqr0tUF/4Yn6fzdKlS3XllVfqwIEDatWqVbW3B+C8IKcDAAAAqA/i4+P11FNP6Xe/+537Tvf69euVnp6ukpIS7d692+P9xvx3Xs6KeST27NmjRYsWadWqVSotLVVUVJTHNmlpaUpLS6tyTC1atFBhYaEkqbCwUB07dqzWOR0+fFjjxo3TW2+9Va3tgPrGF/P7bJKSkpSUlKTU1FQtW7ZMQ4YMOed9AvAuCjUAAAC1ICgoSO3bt9czzzyjwYMHS5KefvppZWRkqG3btoqIiPB4f1hYmHbu3ClJ2rlzp2JjY9W5c2clJyfrvvvukySVlJR4bDN16lRlZWV5LJs0aZL69+9faUxRUVFas2aNevbsqbVr12rYsGEqLS3Vd999pwsvvPBnz6e0tFTDhw/XM888ozZt2lT5cwDqI1/M78qcOHFCTZo0kSSFhoYqJCREkvT111+rbdu2VT9hAI7i0ScAAIBaMnToUL3yyisaNGiQJGnIkCG66aabNGbMmDPmg4iPj9fKlSuVlJTkXpaUlKT8/HzFxcUpLi5OK1as8NgmLS1N2dnZHq+f+xJ34403ateuXYqOjlZUVJQuuugi5efna/LkyR7v27dvnxISErRr1y4lJCQoPz9fb7/9trZs2aKHHnpILpdLmzZtOtePB/BrvpbfleVtxbw5MTExKigo0HXXXafS0lKNGjXKY9uSkhIlJCQoLy9PAwYM0ObNm8/x0wFQm8zZWrYBAADg7Iwx1hf/jrr//vuVk5Oj5cuXKyws7Iz177zzjsLDwxUXF1ftfRcVFSkxMVHXXnutnn/+eY91xhhZa81ZNgX8ir/md2W2bt2qvLw83XnnnT/7PvIb8B0UagAAAGrAV7/IOYUvcqhPyG9P5DfgXTz6BAAAAAAA4CMo1AAAAAAAAPgICjUAAAA+wuVyqbS01CvHWrFihS6//HJFR0d75XhAQ+fN/D527JhuvPFG9enTRwsWLPDKMQHUHgo1AAAADVCvXr2Ul5fndBgA6sDf/vY3DRs2TB9++KEyMjJ08uRJp0MCUA0UagAAABxw6tQpjRkzRjExMbr++us91lW02O3Ro4f7bvhLL72kXr16KTY2Vp988oneffdd9ezZU3FxcXr//ferffzw8HA1adKkVs4FgCen83vTpk1KSEhQYGCgrrrqKn322We1cl4AvCPI6QAAAAAaovfee0+tW7dWRkaGTp065bGuX79+SkxMVGlpqVwul5KTk/Xee+9p7dq1CgkJkbVW06dP11tvvaX27dvrp91ppk6dqqysLI9lkyZNUv/+/ev8vAA4n99Hjx5VaGioJCksLExHjhypozMFUBco1AAAADjg888/V+/evSVJAQGeg5y3bt2q9PR0lZSUaPfu3ZKk9PR03XPPPWrcuLEeffRRTZ48WY899phKS0s1adIkRUREuLdPS0tTWlqa904GgAen87tFixYqLCxUcHCwCgsL1aJFi9o9QQB1ikefAAAAHNC5c2fl5ORI0hl33J9++mllZGQoMzNTYWFhkqRu3bpp3rx5crlcmjdvntq1a6eMjAylpKRo+vTpHttPnTpVLpfL47V69WrvnBgAx/M7KipKa9asUVlZmbZv367OnTvX4dkCqG3mp0PpAAAA8MuMMfZc/o46deqU7r77bu3du1fNmjXT+++/L5fLpczMTC1YsEAzZsxQt27dtGPHDm3btk0jR47Ul19+qRMnTmju3LmaP3++cnJyVFRUpOeee04ul6tax8/NzdXEiROVm5urHj16aNmyZQoODq7x+RhjZK01Nd4B4EP8Pb8LCwt1xx136PDhw0pJSdGoUaNqfC4S+Q14G4UaAACAGjjXL3L1DV/kUJ+Q357Ib8C7ePQJAAAAAADAR1CoAQAAAAAA8BEUagAAAAAAAHwEhRoAAAAAAAAfQaEGAAAAAADARwQ5HQAAAIA/Cg4OLjDGXOh0HL4iODi4wOkYgNpCfnsivwHvoj03AAAAAACAj+DRJwAAAAAAAB9BoQYAAAAAAMBHUKgBAAAAAADwERRqAAAAAAAAfASFGgAAAAAAAB9BoQYAAAAAAMBHUKgBAAAAAADwERRqAAAAAAAAfASFGgAAAAAAAB9BoQYAAAAAAMBHUKgBAAAAAADwERRqAAAAAAAAfASFGgAAAAAAAB9BoQYAAAAAAMBHUKgBAAAAAADwERRqAAAAAAAAfASFGgAAAAAAAB9BoQYAAAAAAMBHUKgBAAAAAADwERRqAAAAAAAAfASFGgAAAAAAAB9BoQYAAAAAAMBHUKgBAAAAAADwERRqAAAAAAAAfASFGgAAAAAAAB9BoQYAAAAAAMBHUKgBAAAAAADwERRqAAAAAAAAfASFGgAAAAAAAB9BoQYAAAAAAMBHUKgBAAAAAADwERRqAAAAAAAAfASFGgAAAAAAAB9BoQYAAAAAAMBHUKgBAAAAAADwEf8PiFlCf0eWALcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  [0.72, 0.78, 0.78, 0.79, 0.81, 0.81, 0.82, 0.75, 0.75, 0.64]\n",
      "f1_score :  [0.67, 0.71, 0.71, 0.71, 0.7, 0.7, 0.7, 0.53, 0.52, 0.0]\n",
      "auc score :  [0.73, 0.77, 0.77, 0.77, 0.76, 0.76, 0.77, 0.67, 0.67, 0.5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df.drop(['class', 'alive', 'embark_town', 'who', 'adult_male', 'alone'], axis=1, inplace=True)\n",
    "df1 = df.copy()\n",
    "df1.deck = df1.deck.astype('O')\n",
    "# 결측치 처리\n",
    "df1.embarked.fillna('S', inplace=True)\n",
    "df1.age.fillna(df1.age.mean(), inplace=True)\n",
    "df1.deck.fillna('N', inplace=True)\n",
    "def label_encoding(df):\n",
    "    ecs = df.columns[(df.dtypes=='O')|(df.dtypes=='category')|(df.dtypes=='bool')]\n",
    "    for i in ecs:\n",
    "        globals()[f'{df}_{i}_encoder'] = LabelEncoder()\n",
    "        globals()[f'{df}_{i}_encoder'].fit(df[i])\n",
    "        df[i] = globals()[f'{df}_{i}_encoder'].transform(df[i])\n",
    "label_encoding(df1)\n",
    "X = df1.drop('survived', axis=1)\n",
    "y = df1.survived\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "def get_clf_eval(y_test, pred):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy  = accuracy_score(y_test, pred)\n",
    "    recall    = recall_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    f1score   = f1_score(y_test, pred)\n",
    "    auc = roc_auc_score(y_test,pred)\n",
    "    return accuracy, recall, precision, f1score, auc\n",
    "dtclf = DecisionTreeClassifier(ccp_alpha=0.01)\n",
    "dtclf.fit(X_train, y_train)\n",
    "pred = dtclf.predict(X_test)\n",
    "get_clf_eval(y_test, pred)\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dtclf, class_names=['0', '1'],\n",
    "          feature_names=X_train.columns, fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "def get_eval_by_threshold(y_test, pred_proba, thresholds):\n",
    "    ts_accuracy = []\n",
    "    ts_precision = []\n",
    "    ts_recall = []\n",
    "    ts_f1score = []\n",
    "    ts_auc = []\n",
    "    ts_datas = {}\n",
    "    for custom_threshold in thresholds:\n",
    "        tt_binarizer = Binarizer(threshold=custom_threshold * 0.1)\n",
    "        custom_predict = tt_binarizer.fit_transform(pred_proba)[:, 1]\n",
    "        result = get_clf_eval(y_test, custom_predict)\n",
    "        ts_accuracy.append(round(result[0],2))\n",
    "        ts_precision.append(round(result[1],2))\n",
    "        ts_recall.append(round(result[2],2))\n",
    "        ts_f1score.append(round(result[3],2))\n",
    "        ts_auc.append(round(result[4],2))\n",
    "        \n",
    "    ts_datas['thresholds'] = [i * 0.1 for i in thresholds]\n",
    "    ts_datas['accuracy'] = ts_accuracy\n",
    "    ts_datas['precision'] = ts_precision\n",
    "    ts_datas['recall'] = ts_recall\n",
    "    ts_datas['f1score'] = ts_f1score\n",
    "    ts_datas['auc'] = ts_auc\n",
    "    print('accuracy : ',ts_accuracy)\n",
    "    print('f1_score : ',ts_f1score)\n",
    "    print('auc score : ',ts_auc)\n",
    "    pd.DataFrame(ts_datas).set_index('thresholds').plot()\n",
    "    plt.show()\n",
    "    \n",
    "dtclf = DecisionTreeClassifier(max_depth=6)\n",
    "dtclf.fit(X_train, y_train)\n",
    "pred_proba = dtclf.predict_proba(X_test)\n",
    "thresholds = range(1, 11)\n",
    "get_eval_by_threshold(y_test, pred_proba, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63020f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2ce7b32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(C=1.0, degree=1, kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "pred_svc = svc.predict(X_test)\n",
    "accuracy_score(y_test, pred_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d4bce1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8222222222222222"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(n_estimators=300)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "pred_bag = ran_clf.predict(X_test)\n",
    "accuracy_score(y_test, pred_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "cb7f0083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8222222222222222"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran_clf = RandomForestClassifier(n_estimators=300, max_depth=5, min_samples_split=5, min_samples_leaf=1, )\n",
    "ran_clf.fit(X_train, y_train)\n",
    "pred_ran = ran_clf.predict(X_test)\n",
    "accuracy_score(y_test, pred_ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "acad97ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_clf = ExtraTreesClassifier(n_estimators=300, min_samples_split=10)\n",
    "ext_clf.fit(X_train, y_train)\n",
    "pred_ext = ext_clf.predict(X_test)\n",
    "accuracy_score(y_test, pred_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "39a9d35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7555555555555555"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=9, )\n",
    "knn.fit(X_train, y_train)\n",
    "pred_knn = knn.predict(X_test)\n",
    "accuracy_score(y_test, pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "058d3142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = LogisticRegression(\n",
    "    solver='newton-cg',\n",
    "    max_iter=100,)\n",
    "log.fit(X_train, y_train)\n",
    "pred_log = log.predict(X_test)\n",
    "accuracy_score(y_test, pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c022b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'pred_svc' : accuracy_score(y_test, pred_svc), \n",
    "              'pred_bag':accuracy_score(y_test, pred_bag),\n",
    "              'pred_ran':accuracy_score(y_test, pred_ran),\n",
    "              'pred_ext':accuracy_score(y_test, pred_ext),\n",
    "              'pred_knn':accuracy_score(y_test, pred_knn),\n",
    "              'pred_log':accuracy_score(y_test, pred_log)},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "cdb807e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame([accuracy_score(y_test, pred_svc), \n",
    "              accuracy_score(y_test, pred_bag),\n",
    "              accuracy_score(y_test, pred_ran),\n",
    "              accuracy_score(y_test, pred_ext),\n",
    "              accuracy_score(y_test, pred_knn),\n",
    "              accuracy_score(y_test, pred_log)],index=('pred_svc','pred_bag','pred_ran','pred_ext','pred_knn','pred_log') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b9cbfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df1= pred_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "a53231d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.033518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.755556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "count  6.000000\n",
       "mean   0.805556\n",
       "std    0.033518\n",
       "min    0.755556\n",
       "25%    0.783333\n",
       "50%    0.816667\n",
       "75%    0.833333\n",
       "max    0.833333"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e4c40757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_svc</th>\n",
       "      <th>pred_bag</th>\n",
       "      <th>pred_ran</th>\n",
       "      <th>pred_ext</th>\n",
       "      <th>pred_knn</th>\n",
       "      <th>pred_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred_svc  pred_bag  pred_ran  pred_ext  pred_knn  pred_log\n",
       "count  1.000000  1.000000  1.000000  1.000000  1.000000       1.0\n",
       "mean   0.777778  0.833333  0.833333  0.833333  0.755556       0.8\n",
       "std         NaN       NaN       NaN       NaN       NaN       NaN\n",
       "min    0.777778  0.833333  0.833333  0.833333  0.755556       0.8\n",
       "25%    0.777778  0.833333  0.833333  0.833333  0.755556       0.8\n",
       "50%    0.777778  0.833333  0.833333  0.833333  0.755556       0.8\n",
       "75%    0.777778  0.833333  0.833333  0.833333  0.755556       0.8\n",
       "max    0.777778  0.833333  0.833333  0.833333  0.755556       0.8"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "e144d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_clf.fit(X, y)\n",
    "pred_ext = ext_clf.predict(test)\n",
    "tit = pd.read_csv('test.csv')\n",
    "tit.drop(list(tit.columns)[1:], axis = 1, inplace=True) \n",
    "tit['Survived'] = pred_ext\n",
    "tit.set_index('PassengerId', inplace=True)\n",
    "tit.to_csv('tit_test(8).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c2da9c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8222222222222222"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(loss='deviance')\n",
    "gb_clf.fit(X_train, y_train)\n",
    "pred_gb = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test,pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "41596b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(loss='exponential')\n",
    "gb_clf.fit(X_train, y_train)\n",
    "pred_gb = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test,pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "4650feb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8222222222222222"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "pred_gb = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test,pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "4b2c6af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8222222222222222"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.4)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "pred_gb = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test,pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "fe11000e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8222222222222222"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.01)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "pred_gb = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test,pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "6c9d3e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "pred_gb = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test,pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "4320ba8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8444444444444444"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200, subsample=0.9)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "pred_gb = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test,pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "93d98273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200, subsample=1.0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "pred_gb = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test,pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "383a3393",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators : 100\n",
      "sub_sample :  0.1\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.6889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.3333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.5667\n",
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.4778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.4000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.15 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.2\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.15 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.4556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.3000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.30000000000000004\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.5778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.4\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.67 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.67 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.5\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.6000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.15 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.7000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.8\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.9\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "n_estimators : 200\n",
      "sub_sample :  0.1\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.5667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.4444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.5444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.2889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 1.67 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.2\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.4556\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.5667\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.30000000000000004\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.4\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.5\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.6000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.15 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.7000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.8\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.15 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.9\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "n_estimators : 300\n",
      "sub_sample :  0.1\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.5778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.5444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.5889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 1.67 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.4000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.4222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.2\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.4556\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.4444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.30000000000000004\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.4\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.15 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.5\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.6000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.7000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.8\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.9\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "n_estimators : 400\n",
      "sub_sample :  0.1\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.4889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.4778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.2\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.6111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.30000000000000004\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.4444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4444\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.4\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.5\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.6000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.7000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.15 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.8\n",
      "learning_rate : 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.9\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "n_estimators : 500\n",
      "sub_sample :  0.1\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.3333\n",
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.5556\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.4556\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.2\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.3333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.5778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.3889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.3667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.30000000000000004\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.3889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.4\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.5\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.77 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.5889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.6000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.7000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.8\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.9\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "n_estimators : 600\n",
      "sub_sample :  0.1\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.4111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.4000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.4222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.5444\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.2\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.5556\n",
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.4667\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.4000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.30000000000000004\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.6111\n",
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.3556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.4\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.5\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.4667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.6000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.7000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.8\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.9\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "n_estimators : 700\n",
      "sub_sample :  0.1\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.6889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.4556\n",
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 7.87 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.3444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.5222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.4778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.3333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4444\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.2\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.3889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.6111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.4667\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.30000000000000004\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.4111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.3889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4444\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.4\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.6889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.5111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.3667\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.4000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.5\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.3333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.6000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.7000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.8\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.9\n",
      "learning_rate : 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "n_estimators : 800\n",
      "sub_sample :  0.1\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.5222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.3889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.4889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.4889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.4333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.2\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.6111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.6111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.3333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.3889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.5444\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.30000000000000004\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.5111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.4\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.5556\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5444\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.5\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.6000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.7000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.8\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.9\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "n_estimators : 900\n",
      "sub_sample :  0.1\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.3667\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.6111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.3889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.2\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.5222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.5556\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.30000000000000004\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 1.91 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.6000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.5556\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.4\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.4778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.5\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4444\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.6000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8222\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.7000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.8\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.9\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "n_estimators : 1000\n",
      "sub_sample :  0.1\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.3889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.4000\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.5778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6444\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.4667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.2889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.2\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.6333\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.4889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.5556\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.5444\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.30000000000000004\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7333\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.5000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.4667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.5556\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.6667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.4889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.5111\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.4\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기의 정확도:  0.6556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.5778\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.4111\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.5\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.6222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.5333\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.6000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.6889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.7000000000000001\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.7111\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.4889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.6889\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.8\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7889\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7222\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7444\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7667\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "sub_sample :  0.9\n",
      "learning_rate : 0.1\n",
      "분류기의 정확도:  0.8000\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.2\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.30000000000000004\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.81 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.4\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.5\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.6000000000000001\n",
      "분류기의 정확도:  0.8111\n",
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.05 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.7000000000000001\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.8\n",
      "분류기의 정확도:  0.7778\n",
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.62 µs\n",
      "--------------------------------------------------\n",
      "learning_rate : 0.9\n",
      "분류기의 정확도:  0.7556\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#loss = 'deviance'\n",
    "for i in range(100,1001,100):\n",
    "    print('n_estimators :',i)\n",
    "    for j in range(1,10):\n",
    "        print('sub_sample : ',j*0.1)\n",
    "        for k in range(1,10):\n",
    "            print('learning_rate :', k*0.1)\n",
    "            gb_clf = GradientBoostingClassifier(n_estimators=i,subsample=(j*0.1),learning_rate=(k*0.1))\n",
    "            gb_clf.fit(X_train, y_train)\n",
    "            pred = gb_clf.predict(X_test)\n",
    "            print('분류기의 정확도: {0: .4f}'.format(accuracy_score(y_test, pred)))\n",
    "            %time\n",
    "            print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "65ffd3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼 파라미터 : \n",
      "  {'learning_rate': 0.1, 'n_estimators': 100, 'subsample': 0.5}\n",
      "최고 예측 정확도 :0.8340\n",
      "정확도 : .4f\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'n_estimators':range(100,1000,100),\n",
    "          'subsample':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "          'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}\n",
    "gsv=GridSearchCV(gb_clf, param_grid=params)\n",
    "gsv.fit(X_train,y_train)\n",
    "pred_gsv = gsv.predict(X_test)\n",
    "print('최적 하이퍼 파라미터 : \\n ',gsv.best_params_)\n",
    "print('최고 예측 정확도 :{0:.4f}'.format(gsv.best_score_))\n",
    "print('정확도 : .4f'.format(accuracy_score(y_test, pred_gsv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "ac16d1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200, subsample=1.0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_clf.bes\n",
    "pred_gb = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test,pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "76e85aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8555555555555555"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(loss='exponential', \n",
    "                                    learning_rate=0.1, \n",
    "                                    n_estimators=300, \n",
    "                                    subsample=1.0, \n",
    "                                    ccp_alpha=0.011001332269966666,\n",
    "                                    max_depth=8,\n",
    "                                    max_features='log2',\n",
    "                                    min_impurity_decrease=0.8671820004771132,\n",
    "                                    min_samples_leaf=28,\n",
    "                                    min_samples_split=0.32582763423323347)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "pred_gb = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test,pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "ada8f433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(ccp_alpha=0.012604155917628049, \n",
    "                                    loss='exponential',\n",
    "                           max_depth=7, max_features='log2',\n",
    "                           min_impurity_decrease=0.6770446055558269,\n",
    "                           min_samples_leaf=15,\n",
    "                           min_samples_split=0.4281965740832672,\n",
    "                           n_estimators=200)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "pred_gb = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test,pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb08f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-22.9.0               |   py39h6e9494a_1         969 KB  conda-forge\n",
      "    python_abi-3.9             |           2_cp39           4 KB  conda-forge\n",
      "    xgboost-1.5.0              |   py39hecd8cb5_1          25 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         998 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  python_abi         conda-forge/osx-64::python_abi-3.9-2_cp39\n",
      "  xgboost            pkgs/main/osx-64::xgboost-1.5.0-py39hecd8cb5_1\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda               anaconda::conda-4.13.0-py39hecd8cb5_0 --> conda-forge::conda-22.9.0-py39h6e9494a_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "python_abi-3.9       | 4 KB      | ##################################### | 100% \n",
      "xgboost-1.5.0        | 25 KB     | ##################################### | 100% \n",
      "conda-22.9.0         | 969 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcd0c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('./train.csv')\n",
    "df_kg = pd.read_csv('./test.csv')\n",
    "df1 = df.copy()\n",
    "df1.set_index('PassengerId', inplace=True)\n",
    "df_kg.set_index('PassengerId', inplace=True)\n",
    "df1.drop(['Name'], axis=1, inplace=True)\n",
    "df_kg.drop(['Name'], axis=1, inplace=True)\n",
    "\n",
    "df_kg.Cabin.astype(str)\n",
    "\n",
    "# 결측치 처리\n",
    "df1.Cabin.fillna('N', inplace=True)\n",
    "df1.Embarked.fillna('S', inplace=True)\n",
    "df1.Age.fillna(df1.Age.median(), inplace=True)\n",
    "df_kg.Cabin.fillna('N', inplace=True)\n",
    "df_kg.Fare.fillna(df1.Fare.median(), inplace=True)\n",
    "df_kg.Age.fillna(df1.Age.median(), inplace=True)\n",
    "\n",
    "df1.Cabin = df1.Cabin.apply(lambda x:x[0])\n",
    "df_kg.Cabin = df_kg.Cabin.apply(lambda x:x[0])\n",
    "\n",
    "# 인코딩\n",
    "og_columns = df1.columns[(df1.dtypes=='O')|(df1.dtypes=='category')|(df1.dtypes=='bool')]\n",
    "og_columns\n",
    "\n",
    "for i in og_columns:\n",
    "    globals()[f'df1_{i}_encoder'] = LabelEncoder()\n",
    "    globals()[f'df1_{i}_encoder'].fit(df1[i])\n",
    "    globals()[f'df_kg_{i}_encoder'] = LabelEncoder()\n",
    "    globals()[f'df_kg_{i}_encoder'].fit(df_kg[i])\n",
    "    df1[i] = globals()[f'df1_{i}_encoder'].transform(df1[i])\n",
    "    df_kg[i] = globals()[f'df_kg_{i}_encoder'].transform(df_kg[i])\n",
    "\n",
    "# X, y 분리\n",
    "X = df1.drop('Survived', axis=1)\n",
    "y = df1.Survived\n",
    "\n",
    "# train, test 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64186cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7045653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2e3fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6ecc0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:37:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc.fit(X_train, y_train)\n",
    "pred_xgb = xgbc.predict(X_val)\n",
    "accuracy_score(y_val, pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36c5a6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:45] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:38:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:38:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:38:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:06] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:06] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:08] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:08] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:08] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:08] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:09] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:09] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:09] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:09] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:09] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:10] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:10] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:10] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:10] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:10] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:10] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:45] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:45] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:45] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:39:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:39:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:40:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:06] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:40:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:40:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|█████████████████████| 500/500 [01:54<00:00,  4.36trial/s, best loss: -0.9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
    "\n",
    "def xgb_objective(search_space):\n",
    "    model = XGBClassifier(**search_space)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# new search space\n",
    "search_space={'loss':hp.choice('loss', ['deviance', 'exponential']),\n",
    "              'max_depth':hp.choice('max_depth', range(2, 10)),\n",
    "              'min_samples_split':hp.uniform('min_samples_split', 0.1, 1),\n",
    "              'min_samples_leaf':hp.choice('min_samples_leaf', range(1, 30)),\n",
    "              'max_features':hp.choice('max_features', [None, 'sqrt', 'log2']),\n",
    "              'min_impurity_decrease':hp.uniform('min_impurity_decrease', 0.1, 1),\n",
    "              'ccp_alpha':hp.uniform('ccp_alpha', 0.01, 1),\n",
    "              'learning_rate':hp.choice('learning_rate', [0.1, 0.01]),\n",
    "              'n_estimators':hp.choice('n_estimators', [100, 200, 300, 400, 500,1000])}\n",
    "\n",
    "# set the hyperparam tuning algorithm\n",
    "algorithm=tpe.suggest\n",
    "# implement Hyperopt\n",
    "best_params = fmin(fn=xgb_objective,\n",
    "                   space=search_space,\n",
    "                   algo=algorithm,\n",
    "                   max_evals=500)\n",
    "\n",
    "params1 = space_eval(search_space, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96f76329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.4504416497821457,\n",
       " 'learning_rate': 0.01,\n",
       " 'loss': 'exponential',\n",
       " 'max_depth': 7,\n",
       " 'max_features': None,\n",
       " 'min_impurity_decrease': 0.8012054943169539,\n",
       " 'min_samples_leaf': 6,\n",
       " 'min_samples_split': 0.6272831796542432,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45bb6f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:45:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb_clf = XGBClassifier(**params1)\n",
    "best_xgb_clf.fit(X_train, y_train)\n",
    "best_xgb_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "541fc298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:45:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', ccp_alpha=0.4504416497821457,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.01, loss='exponential', max_delta_step=0,\n",
       "              max_depth=7, max_features=None, min_child_weight=1,\n",
       "              min_impurity_decrease=0.8012054943169539, min_samples_leaf=6,\n",
       "              min_samples_split=0.6272831796542432, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, ...)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "    poly = PolynomialFeatures(5)\n",
    "    X_poly = poly.fit_transform(X_pca)\n",
    "    X_trn, X_val, y_trn, y_val = train_test_split(X_poly, y, test_size=0.2, random_state=19)\n",
    "    bt_lr_model = Ridge(alpha=alpha, normalize=True, random_state=19)\n",
    "    bt_lr_model.fit(X_trn, y_trn)\n",
    "    bt_lr_pred = bt_lr_model.predict(X_val)\n",
    "    val_mse = round(mean_squared_error(y_val, bt_lr_pred), 3)\n",
    "    trn_r2 = round(bt_lr_model.score(X_trn, y_trn), 3)\n",
    "    val_r2 = round(bt_lr_model.score(X_val, y_val), 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
