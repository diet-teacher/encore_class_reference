{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6a544df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff80e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d9bd23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53cddaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "892      nan\n",
       "893      nan\n",
       "894      nan\n",
       "895      nan\n",
       "896      nan\n",
       "        ... \n",
       "1305     nan\n",
       "1306    C105\n",
       "1307     nan\n",
       "1308     nan\n",
       "1309     nan\n",
       "Name: Cabin, Length: 418, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./train.csv')\n",
    "df_kg = pd.read_csv('./test.csv')\n",
    "df1 = df.copy()\n",
    "df1.set_index('PassengerId', inplace=True)\n",
    "df_kg.set_index('PassengerId', inplace=True)\n",
    "df1.drop(['Name'], axis=1, inplace=True)\n",
    "df_kg.drop(['Name'], axis=1, inplace=True)\n",
    "\n",
    "df_kg.Cabin.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f6db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 처리\n",
    "df1.Cabin.fillna('N', inplace=True)\n",
    "df1.Embarked.fillna('S', inplace=True)\n",
    "df1.Age.fillna(df1.Age.median(), inplace=True)\n",
    "df_kg.Cabin.fillna('N', inplace=True)\n",
    "df_kg.Fare.fillna(df1.Fare.median(), inplace=True)\n",
    "df_kg.Age.fillna(df1.Age.median(), inplace=True)\n",
    "\n",
    "df1.Cabin = df1.Cabin.apply(lambda x:x[0])\n",
    "df_kg.Cabin = df_kg.Cabin.apply(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9170e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩\n",
    "og_columns = df1.columns[(df1.dtypes=='O')|(df1.dtypes=='category')|(df1.dtypes=='bool')]\n",
    "for i in og_columns:\n",
    "    globals()[f'df1_{i}_encoder'] = LabelEncoder()\n",
    "    globals()[f'df1_{i}_encoder'].fit(df1[i])\n",
    "    globals()[f'df_kg_{i}_encoder'] = LabelEncoder()\n",
    "    globals()[f'df_kg_{i}_encoder'].fit(df_kg[i])\n",
    "    df1[i] = globals()[f'df1_{i}_encoder'].transform(df1[i])\n",
    "    df_kg[i] = globals()[f'df_kg_{i}_encoder'].transform(df_kg[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d9574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y 분리\n",
    "X = df1.drop('Survived', axis=1)\n",
    "y = df1.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f44e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b8cef6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:31:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(X_train, y_train)\n",
    "xgbc.predict(X_val)\n",
    "accuracy_score(y_val, xgbc.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea86fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8dea35ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:45] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:45] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:42:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:42:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:01] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:02] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:04] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:05] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:06] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:06] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:06] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:06] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:06] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:07] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:08] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:08] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:08] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:09] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:09] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:09] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:09] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:10] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:10] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:10] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:10] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:11] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:13] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:14] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:15] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:16] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:17] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:18] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:19] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:23] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:25] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:26] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:27] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:29] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:31] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:32] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:34] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:35] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:37] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:38] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:39] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:40] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:41] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:42] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:43] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:44] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:45] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:45] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:45] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:46] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:47] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:48] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:49] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:50] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:51] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:52] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:53] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:54] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:57] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"max_features\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:43:59] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:43:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:44:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:44:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:44:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:44:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:44:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:44:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:44:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:44:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:44:00] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[10:44:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|█████████████████████| 500/500 [01:49<00:00,  4.58trial/s, best loss: -0.9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def xgb_objective(search_space):\n",
    "    model = XGBClassifier(**search_space)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# new search space\n",
    "search_space={'loss':hp.choice('loss', ['deviance', 'exponential']),\n",
    "              'max_depth':hp.choice('max_depth', range(2, 10)),\n",
    "              'min_samples_split':hp.uniform('min_samples_split', 0.1, 1),\n",
    "              'min_samples_leaf':hp.choice('min_samples_leaf', range(1, 30)),\n",
    "              'max_features':hp.choice('max_features', [None, 'sqrt', 'log2']),\n",
    "              'min_impurity_decrease':hp.uniform('min_impurity_decrease', 0.1, 1),\n",
    "              'ccp_alpha':hp.uniform('ccp_alpha', 0.01, 1),\n",
    "              'learning_rate':hp.choice('learning_rate', [0.1, 0.01]),\n",
    "              'n_estimators':hp.choice('n_estimators', [100, 200, 300, 400, 500,1000])}\n",
    "\n",
    "# set the hyperparam tuning algorithm\n",
    "algorithm=tpe.suggest\n",
    "# implement Hyperopt\n",
    "best_params = fmin(fn=xgb_objective,\n",
    "                   space=search_space,\n",
    "                   algo=algorithm,\n",
    "                   max_evals=500)\n",
    "\n",
    "params1 = space_eval(search_space, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021cfcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22e3d385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.7638002771865584,\n",
       " 'learning_rate': 0.01,\n",
       " 'loss': 'exponential',\n",
       " 'max_depth': 8,\n",
       " 'max_features': None,\n",
       " 'min_impurity_decrease': 0.969767042752594,\n",
       " 'min_samples_leaf': 20,\n",
       " 'min_samples_split': 0.8119215852058577,\n",
       " 'n_estimators': 300}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5820af87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:30:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:30:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb_clf = XGBClassifier(**params1)\n",
    "best_xgb_clf.fit(X_train, y_train)\n",
    "best_xgb_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9203b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:30:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:30:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', ccp_alpha=0.7638002771865584,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.01, loss='exponential', max_delta_step=0,\n",
       "              max_depth=8, max_features=None, min_child_weight=1,\n",
       "              min_impurity_decrease=0.969767042752594, min_samples_leaf=20,\n",
       "              min_samples_split=0.8119215852058577, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=300, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, ...)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6db8071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcn = XGBClassifier(base_score=0.5, booster='gbtree', ccp_alpha=0.7638002771865584,\n",
    "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
    "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
    "              importance_type=None, interaction_constraints='',\n",
    "              learning_rate=0.01, loss='exponential', max_delta_step=0,\n",
    "              max_depth=8, max_features=None, min_child_weight=1,\n",
    "              min_impurity_decrease=0.969767042752594, min_samples_leaf=20,\n",
    "              min_samples_split=0.8119215852058577,\n",
    "              monotone_constraints='()', n_estimators=300, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d6a21b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:33:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:33:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbcn.fit(X_train, y_train)\n",
    "xgbcn.predict(X_val)\n",
    "accuracy_score(y_val, xgbcn.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "816103b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:35:26] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:35:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbcn.fit(X,y)\n",
    "xgbcn.predict(df_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03af598",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02698d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0520ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:38:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:38:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgbcn.fit(X, y)\n",
    "pred_xgbc = xgbcn.predict(df_kg)\n",
    "tit = pd.read_csv('test.csv')\n",
    "tit.drop(list(tit.columns)[1:], axis = 1, inplace=True) \n",
    "tit['Survived'] = pred_xgbc\n",
    "tit.set_index('PassengerId', inplace=True)\n",
    "tit.to_csv('tit_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45278fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HistGradientBoostingClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a29cf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████| 500/500 [04:46<00:00,  1.74trial/s, best loss: -0.9111111111111111]\n"
     ]
    }
   ],
   "source": [
    "hgbc = HistGradientBoostingClassifier()\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def hgbc_objective(search_space):\n",
    "    model = HistGradientBoostingClassifier(**search_space)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# new search space\n",
    "search_space={'loss':hp.choice('loss', ['auto', 'binary_crossentropy']),\n",
    "              'max_iter':hp.choice('max_iter', range(100, 1000, 300)),\n",
    "              'max_depth':hp.choice('max_depth', range(2, 10)),\n",
    "              'min_samples_leaf':hp.choice('min_samples_leaf', range(1, 30)),\n",
    "              'learning_rate':hp.choice('learning_rate', [0.01,0.1,0.3,0.5,0.7,0.9]),\n",
    "              'l2_regularization':hp.choice('l2_regularization', [0.0,0.2,0.4,0.6,0.8,1]),\n",
    "              'max_bins':hp.choice('max_bins', [2,50, 100, 150, 200, 255]),\n",
    "              'warm_start':hp.choice('warm_start', ['True', 'False']),\n",
    "              'n_iter_no_change':hp.choice('n_iter_no_change', [1, 5, 10, 15, 20])}\n",
    "\n",
    "# set the hyperparam tuning algorithm\n",
    "algorithm=tpe.suggest\n",
    "# implement Hyperopt\n",
    "best_params = fmin(fn=hgbc_objective,\n",
    "                   space=search_space,\n",
    "                   algo=algorithm,\n",
    "                   max_evals=500)\n",
    "\n",
    "params1 = space_eval(search_space, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87efe1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l2_regularization': 0.2,\n",
       " 'learning_rate': 0.01,\n",
       " 'loss': 'auto',\n",
       " 'max_bins': 255,\n",
       " 'max_depth': 4,\n",
       " 'max_iter': 400,\n",
       " 'min_samples_leaf': 12,\n",
       " 'n_iter_no_change': 20,\n",
       " 'warm_start': 'False'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d525b042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hgbc_clf = HistGradientBoostingClassifier(**params1)\n",
    "best_hgbc_clf.fit(X_train, y_train)\n",
    "best_hgbc_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fb40195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HistGradientBoostingClassifier(l2_regularization=0.2, learning_rate=0.01,\n",
       "                               loss=&#x27;auto&#x27;, max_depth=4, max_iter=400,\n",
       "                               min_samples_leaf=12, n_iter_no_change=20,\n",
       "                               warm_start=&#x27;False&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier(l2_regularization=0.2, learning_rate=0.01,\n",
       "                               loss=&#x27;auto&#x27;, max_depth=4, max_iter=400,\n",
       "                               min_samples_leaf=12, n_iter_no_change=20,\n",
       "                               warm_start=&#x27;False&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "HistGradientBoostingClassifier(l2_regularization=0.2, learning_rate=0.01,\n",
       "                               loss='auto', max_depth=4, max_iter=400,\n",
       "                               min_samples_leaf=12, n_iter_no_change=20,\n",
       "                               warm_start='False')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hgbc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1037872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████| 500/500 [01:52<00:00,  4.45trial/s, best loss: -0.9111111111111111]\n"
     ]
    }
   ],
   "source": [
    "hgbc = HistGradientBoostingClassifier()\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def rf_objective(search_space):\n",
    "    model = RandomForestClassifier(**search_space)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# new search space\n",
    "search_space={'criterion':hp.choice('criterion', ['gini', 'entropy','log_loss']),\n",
    "              'min_samples_split':hp.choice('min_samples_split', range(2, 10)),\n",
    "              'min_samples_leaf':hp.choice('min_samples_leaf', range(1, 20,2)),\n",
    "              'max_features':hp.choice('max_features', [\"sqrt\",\"log2\",None]),\n",
    "              'min_impurity_decrease':hp.choice('min_impurity_decrease', [0.0,0.2,0.4,0.6,0.8,1]),\n",
    "              'ccp_alpha':hp.choice('ccp_alpha', [0.0,0.2,0.4,0.6,0.8,1]),\n",
    "              'warm_start':hp.choice('warm_start', ['True', 'False']),\n",
    "              'n_estimators':hp.choice('n_estimators', range(100,500,100))}\n",
    "\n",
    "# set the hyperparam tuning algorithm\n",
    "algorithm=tpe.suggest\n",
    "# implement Hyperopt\n",
    "best_params = fmin(fn=rf_objective,\n",
    "                   space=search_space,\n",
    "                   algo=algorithm,\n",
    "                   max_evals=500)\n",
    "\n",
    "params1 = space_eval(search_space, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60705a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'criterion': 'entropy',\n",
       " 'max_features': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 7,\n",
       " 'n_estimators': 100,\n",
       " 'warm_start': 'True'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d69b84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8777777777777778"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_clf = RandomForestClassifier(**params1)\n",
    "best_rf_clf.fit(X_train, y_train)\n",
    "best_rf_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a4f9fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_features=None,\n",
       "                       min_samples_split=7, warm_start=&#x27;True&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_features=None,\n",
       "                       min_samples_split=7, warm_start=&#x27;True&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_features=None,\n",
       "                       min_samples_split=7, warm_start='True')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9398cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#의정이 코드\n",
    "\n",
    "# def hist_objective(search_space):\n",
    "#     model = HistGradientBoostingClassifier(**search_space)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# # new search space\n",
    "# search_space={'loss':hp.choice('loss', ['auto', 'binary_crossentropy']),\n",
    "#               'learning_rate':hp.uniform('learning_rate',0.1,1),\n",
    "#               'max_iter':hp.choice('max_irer',range(100,1000)),\n",
    "              \n",
    "#               'min_samples_leaf':hp.choice('min_sanmples_leak',range(1,30)), \n",
    "            \n",
    "#               'n_iter_no_change':hp.choice('n_iter_no_change',range(10,20)),\n",
    "#               'random_state':hp.choice('random_choice',range(1,30))}\n",
    "\n",
    "# # set the hyperparam tuning algorithm\n",
    "# algorithm=tpe.suggest\n",
    "# # implement Hyperopt\n",
    "# best_params = fmin(fn=hist_objective,\n",
    "#                    space=search_space,\n",
    "#                    algo=algorithm,\n",
    "#                    max_evals=500)\n",
    "\n",
    "# params1 = space_eval(search_space, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c640e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l2_regularization': 0.2,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'auto',\n",
       " 'max_bins': 150,\n",
       " 'max_depth': 4,\n",
       " 'max_iter': 100,\n",
       " 'min_samples_leaf': 12,\n",
       " 'n_iter_no_change': 5,\n",
       " 'warm_start': 'False'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e12d358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9222222222222223"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hgbc_clf = HistGradientBoostingClassifier(**params1)\n",
    "best_hgbc_clf.fit(X_train, y_train)\n",
    "best_hgbc_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9049a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(l2_regularization=0.6, learning_rate=0.01,\n",
       "                               max_bins=50, max_depth=7, max_iter=600,\n",
       "                               min_samples_leaf=29, n_iter_no_change=5,\n",
       "                               warm_start='True')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hgbc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8efc2461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9222222222222223"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgbcn = HistGradientBoostingClassifier(l2_regularization=0.6, learning_rate=0.01,\n",
    "                               max_bins=50, max_depth=7, max_iter=600,\n",
    "                               min_samples_leaf=29, n_iter_no_change=5,\n",
    "                               warm_start='True')\n",
    "hgbcn.fit(X_train, y_train)\n",
    "hgbcn.predict(X_val)\n",
    "accuracy_score(y_val, hgbcn.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbcn.fit(X_train, y_train)\n",
    "pred_hgbc = hgbcn.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "369a38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbcn.fit(X, y)\n",
    "pred_hgbc = hgbcn.predict(df_kg)\n",
    "tit = pd.read_csv('test.csv')\n",
    "tit.drop(list(tit.columns)[1:], axis = 1, inplace=True) \n",
    "tit['Survived'] = pred_hgbc\n",
    "tit.set_index('PassengerId', inplace=True)\n",
    "tit.to_csv('tit_test(2).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1ceb23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf1 = DecisionTreeClassifier()\n",
    "dt_clf1.fit(X, y)\n",
    "dt_hat1 = dt_clf1.predict(df_kg)\n",
    "result_df = pd.DataFrame({'dt1':dt_hat1})\n",
    "result_df.index = list(range(892, 892 + result_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "665c3807",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf2 = DecisionTreeClassifier(ccp_alpha=0.01)\n",
    "dt_clf2.fit(X, y)\n",
    "dt_hat2 = dt_clf2.predict(df_kg)\n",
    "result_df['dt2'] = dt_hat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd197bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf3 = DecisionTreeClassifier(max_depth=5)\n",
    "dt_clf3.fit(X, y)\n",
    "dt_hat3 = dt_clf3.predict(df_kg)\n",
    "result_df['dt3'] = dt_hat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "90928561",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf4 = DecisionTreeClassifier(min_samples_split=5)\n",
    "dt_clf4.fit(X, y)\n",
    "dt_hat4 = dt_clf4.predict(df_kg)\n",
    "result_df['dt4'] = dt_hat4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8955985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8372615039281706\n"
     ]
    }
   ],
   "source": [
    "bg_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                           n_estimators=500, bootstrap=True, n_jobs=4, oob_score=True)\n",
    "bg_clf.fit(X, y)\n",
    "print(bg_clf.oob_score_)\n",
    "bg_hat = bg_clf.predict(df_kg)\n",
    "result_df['bg1'] = bg_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36e82a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_clf1 = BaggingClassifier(base_estimator=RandomForestClassifier(),\n",
    "                           n_estimators=1000, bootstrap=False, n_jobs=4)\n",
    "bg_clf1.fit(X, y)\n",
    "bg_hat1 = bg_clf1.predict(df_kg)\n",
    "result_df['bg2'] = bg_hat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66bb9fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8282828282828283\n"
     ]
    }
   ],
   "source": [
    "rf_clf  = RandomForestClassifier(n_estimators=2000, bootstrap=True, max_samples=0.5,\n",
    "                                 max_depth=7, n_jobs=4, oob_score=True)\n",
    "rf_clf.fit(X, y)\n",
    "print(rf_clf.oob_score_)\n",
    "rf_hat = rf_clf.predict(df_kg)\n",
    "result_df['rf'] = rf_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c0a6936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7755331088664422\n"
     ]
    }
   ],
   "source": [
    "rf_clf1  = RandomForestClassifier(n_estimators=2000, bootstrap=True, max_samples=0.2,\n",
    "                                 ccp_alpha=0.05, n_jobs=4, oob_score=True)\n",
    "rf_clf1.fit(X, y)\n",
    "print(rf_clf1.oob_score_)\n",
    "rf_hat1 = rf_clf1.predict(df_kg)\n",
    "result_df['rf1'] = rf_hat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03e46ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8249158249158249\n"
     ]
    }
   ],
   "source": [
    "et_clf  = ExtraTreesClassifier(n_estimators=500, bootstrap=True, n_jobs=4, oob_score=True)\n",
    "et_clf.fit(X, y)\n",
    "print(et_clf.oob_score_)\n",
    "et_hat = et_clf.predict(df_kg)\n",
    "result_df['et'] = et_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b9998ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clf1  = ExtraTreesClassifier(n_estimators=100, min_samples_split=10, bootstrap=False, n_jobs=4)\n",
    "et_clf1.fit(X, y)\n",
    "et_hat1 = et_clf1.predict(df_kg)\n",
    "result_df['et1'] = et_hat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca6bb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf_10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_clf_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "knn_clf_30 = KNeighborsClassifier(n_neighbors=30)\n",
    "knn_clf_40 = KNeighborsClassifier(n_neighbors=40)\n",
    "knn_clf_5.fit(X, y)\n",
    "knn_clf_10.fit(X, y)\n",
    "knn_clf_20.fit(X, y)\n",
    "knn_clf_30.fit(X, y)\n",
    "knn_clf_40.fit(X, y)\n",
    "result_df['knn5'] = knn_clf_5.predict(df_kg)\n",
    "result_df['knn10'] = knn_clf_10.predict(df_kg)\n",
    "result_df['knn20'] = knn_clf_20.predict(df_kg)\n",
    "result_df['knn30'] = knn_clf_30.predict(df_kg)\n",
    "result_df['knn40'] = knn_clf_40.predict(df_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21215655",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf  = LogisticRegression(C=0.05)\n",
    "lr_clf.fit(X, y)\n",
    "result_df['lr'] = lr_clf.predict(df_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "99d63cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(probability=True, kernel='rbf')\n",
    "svm_clf1 = SVC(probability=True, kernel='poly')\n",
    "svm_clf2 = SVC(probability=True, kernel='sigmoid')\n",
    "svm_clf.fit(X, y)\n",
    "svm_clf1.fit(X, y)\n",
    "svm_clf2.fit(X, y)\n",
    "result_df['svm'] = svm_clf.predict(df_kg)\n",
    "result_df['svm1'] = svm_clf1.predict(df_kg)\n",
    "result_df['svm2'] = svm_clf2.predict(df_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cbb9eb53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:26:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:26:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgbcn = XGBClassifier(base_score=0.5, booster='gbtree', ccp_alpha=0.7638002771865584,\n",
    "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
    "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
    "              importance_type=None, interaction_constraints='',\n",
    "              learning_rate=0.01, loss='exponential', max_delta_step=0,\n",
    "              max_depth=8, max_features=None, min_child_weight=1,\n",
    "              min_impurity_decrease=0.969767042752594, min_samples_leaf=20,\n",
    "              min_samples_split=0.8119215852058577,\n",
    "              monotone_constraints='()', n_estimators=300, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1)\n",
    "xgbcn.fit(X,y)\n",
    "result_df['xgbc'] = xgbcn.predict(df_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b9c0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbcn = HistGradientBoostingClassifier(l2_regularization=0.6, learning_rate=0.01,\n",
    "                               max_bins=50, max_depth=7, max_iter=600,\n",
    "                               min_samples_leaf=29, n_iter_no_change=5,\n",
    "                               warm_start='True')\n",
    "hgbcn.fit(X, y)\n",
    "result_df['hgbc'] = hgbcn.predict(df_kg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea412952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt1</th>\n",
       "      <th>dt2</th>\n",
       "      <th>dt3</th>\n",
       "      <th>dt4</th>\n",
       "      <th>bg1</th>\n",
       "      <th>bg2</th>\n",
       "      <th>rf</th>\n",
       "      <th>rf1</th>\n",
       "      <th>et</th>\n",
       "      <th>et1</th>\n",
       "      <th>...</th>\n",
       "      <th>knn10</th>\n",
       "      <th>knn20</th>\n",
       "      <th>knn30</th>\n",
       "      <th>knn40</th>\n",
       "      <th>lr</th>\n",
       "      <th>svm</th>\n",
       "      <th>svm1</th>\n",
       "      <th>svm2</th>\n",
       "      <th>xgbc</th>\n",
       "      <th>hgbc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.509019</td>\n",
       "      <td>0.600770</td>\n",
       "      <td>0.888561</td>\n",
       "      <td>0.662115</td>\n",
       "      <td>0.626134</td>\n",
       "      <td>0.573769</td>\n",
       "      <td>0.471583</td>\n",
       "      <td>0.561496</td>\n",
       "      <td>0.567571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132186</td>\n",
       "      <td>0.166604</td>\n",
       "      <td>0.200724</td>\n",
       "      <td>0.194815</td>\n",
       "      <td>0.497330</td>\n",
       "      <td>0.182609</td>\n",
       "      <td>0.094228</td>\n",
       "      <td>-0.086705</td>\n",
       "      <td>0.610685</td>\n",
       "      <td>0.570318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt2</th>\n",
       "      <td>0.509019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.790363</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.767341</td>\n",
       "      <td>0.796129</td>\n",
       "      <td>0.869158</td>\n",
       "      <td>0.873051</td>\n",
       "      <td>0.731172</td>\n",
       "      <td>0.813614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044659</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>0.050243</td>\n",
       "      <td>0.044897</td>\n",
       "      <td>0.819278</td>\n",
       "      <td>0.077376</td>\n",
       "      <td>0.098996</td>\n",
       "      <td>0.027164</td>\n",
       "      <td>0.794286</td>\n",
       "      <td>0.788858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt3</th>\n",
       "      <td>0.600770</td>\n",
       "      <td>0.790363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.671223</td>\n",
       "      <td>0.762742</td>\n",
       "      <td>0.767269</td>\n",
       "      <td>0.794407</td>\n",
       "      <td>0.702949</td>\n",
       "      <td>0.722393</td>\n",
       "      <td>0.767932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188490</td>\n",
       "      <td>0.189819</td>\n",
       "      <td>0.173459</td>\n",
       "      <td>0.175766</td>\n",
       "      <td>0.807017</td>\n",
       "      <td>0.265822</td>\n",
       "      <td>0.104608</td>\n",
       "      <td>-0.138890</td>\n",
       "      <td>0.792107</td>\n",
       "      <td>0.740055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt4</th>\n",
       "      <td>0.888561</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.671223</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681076</td>\n",
       "      <td>0.655620</td>\n",
       "      <td>0.601554</td>\n",
       "      <td>0.486600</td>\n",
       "      <td>0.601017</td>\n",
       "      <td>0.585960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174363</td>\n",
       "      <td>0.175329</td>\n",
       "      <td>0.201006</td>\n",
       "      <td>0.193767</td>\n",
       "      <td>0.506962</td>\n",
       "      <td>0.185959</td>\n",
       "      <td>0.100220</td>\n",
       "      <td>-0.069921</td>\n",
       "      <td>0.640781</td>\n",
       "      <td>0.597434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bg1</th>\n",
       "      <td>0.662115</td>\n",
       "      <td>0.767341</td>\n",
       "      <td>0.762742</td>\n",
       "      <td>0.681076</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883143</td>\n",
       "      <td>0.835025</td>\n",
       "      <td>0.735569</td>\n",
       "      <td>0.797730</td>\n",
       "      <td>0.810062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194772</td>\n",
       "      <td>0.163484</td>\n",
       "      <td>0.143636</td>\n",
       "      <td>0.130647</td>\n",
       "      <td>0.661229</td>\n",
       "      <td>0.226165</td>\n",
       "      <td>0.121323</td>\n",
       "      <td>-0.067881</td>\n",
       "      <td>0.839157</td>\n",
       "      <td>0.850514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bg2</th>\n",
       "      <td>0.626134</td>\n",
       "      <td>0.796129</td>\n",
       "      <td>0.767269</td>\n",
       "      <td>0.655620</td>\n",
       "      <td>0.883143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883418</td>\n",
       "      <td>0.787885</td>\n",
       "      <td>0.863883</td>\n",
       "      <td>0.857621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145082</td>\n",
       "      <td>0.159938</td>\n",
       "      <td>0.160461</td>\n",
       "      <td>0.137634</td>\n",
       "      <td>0.707878</td>\n",
       "      <td>0.197299</td>\n",
       "      <td>0.116518</td>\n",
       "      <td>-0.018745</td>\n",
       "      <td>0.854310</td>\n",
       "      <td>0.837709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>0.573769</td>\n",
       "      <td>0.869158</td>\n",
       "      <td>0.794407</td>\n",
       "      <td>0.601554</td>\n",
       "      <td>0.835025</td>\n",
       "      <td>0.883418</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.826621</td>\n",
       "      <td>0.828469</td>\n",
       "      <td>0.912319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142271</td>\n",
       "      <td>0.170270</td>\n",
       "      <td>0.163431</td>\n",
       "      <td>0.138180</td>\n",
       "      <td>0.813687</td>\n",
       "      <td>0.184456</td>\n",
       "      <td>0.127280</td>\n",
       "      <td>-0.004794</td>\n",
       "      <td>0.870490</td>\n",
       "      <td>0.879467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf1</th>\n",
       "      <td>0.471583</td>\n",
       "      <td>0.873051</td>\n",
       "      <td>0.702949</td>\n",
       "      <td>0.486600</td>\n",
       "      <td>0.735569</td>\n",
       "      <td>0.787885</td>\n",
       "      <td>0.826621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690992</td>\n",
       "      <td>0.782468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079662</td>\n",
       "      <td>0.099077</td>\n",
       "      <td>0.100896</td>\n",
       "      <td>0.094074</td>\n",
       "      <td>0.749862</td>\n",
       "      <td>0.132279</td>\n",
       "      <td>0.111240</td>\n",
       "      <td>-0.004790</td>\n",
       "      <td>0.745700</td>\n",
       "      <td>0.731774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>0.561496</td>\n",
       "      <td>0.731172</td>\n",
       "      <td>0.722393</td>\n",
       "      <td>0.601017</td>\n",
       "      <td>0.797730</td>\n",
       "      <td>0.863883</td>\n",
       "      <td>0.828469</td>\n",
       "      <td>0.690992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182751</td>\n",
       "      <td>0.173869</td>\n",
       "      <td>0.183653</td>\n",
       "      <td>0.173108</td>\n",
       "      <td>0.713715</td>\n",
       "      <td>0.281104</td>\n",
       "      <td>0.083056</td>\n",
       "      <td>-0.112407</td>\n",
       "      <td>0.789319</td>\n",
       "      <td>0.824164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et1</th>\n",
       "      <td>0.567571</td>\n",
       "      <td>0.813614</td>\n",
       "      <td>0.767932</td>\n",
       "      <td>0.585960</td>\n",
       "      <td>0.810062</td>\n",
       "      <td>0.857621</td>\n",
       "      <td>0.912319</td>\n",
       "      <td>0.782468</td>\n",
       "      <td>0.873942</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174365</td>\n",
       "      <td>0.201161</td>\n",
       "      <td>0.202282</td>\n",
       "      <td>0.190247</td>\n",
       "      <td>0.757515</td>\n",
       "      <td>0.240501</td>\n",
       "      <td>0.091318</td>\n",
       "      <td>-0.060946</td>\n",
       "      <td>0.813848</td>\n",
       "      <td>0.855249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn5</th>\n",
       "      <td>0.130343</td>\n",
       "      <td>-0.001136</td>\n",
       "      <td>0.139695</td>\n",
       "      <td>0.122711</td>\n",
       "      <td>0.128423</td>\n",
       "      <td>0.110901</td>\n",
       "      <td>0.085400</td>\n",
       "      <td>0.051657</td>\n",
       "      <td>0.121628</td>\n",
       "      <td>0.110626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571613</td>\n",
       "      <td>0.476794</td>\n",
       "      <td>0.418021</td>\n",
       "      <td>0.417068</td>\n",
       "      <td>0.128707</td>\n",
       "      <td>0.481747</td>\n",
       "      <td>0.241989</td>\n",
       "      <td>-0.230562</td>\n",
       "      <td>0.146303</td>\n",
       "      <td>0.103175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn10</th>\n",
       "      <td>0.132186</td>\n",
       "      <td>0.044659</td>\n",
       "      <td>0.188490</td>\n",
       "      <td>0.174363</td>\n",
       "      <td>0.194772</td>\n",
       "      <td>0.145082</td>\n",
       "      <td>0.142271</td>\n",
       "      <td>0.079662</td>\n",
       "      <td>0.182751</td>\n",
       "      <td>0.174365</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.579624</td>\n",
       "      <td>0.501973</td>\n",
       "      <td>0.468742</td>\n",
       "      <td>0.214535</td>\n",
       "      <td>0.551552</td>\n",
       "      <td>0.297072</td>\n",
       "      <td>-0.304585</td>\n",
       "      <td>0.165387</td>\n",
       "      <td>0.169207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn20</th>\n",
       "      <td>0.166604</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>0.189819</td>\n",
       "      <td>0.175329</td>\n",
       "      <td>0.163484</td>\n",
       "      <td>0.159938</td>\n",
       "      <td>0.170270</td>\n",
       "      <td>0.099077</td>\n",
       "      <td>0.173869</td>\n",
       "      <td>0.201161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.779874</td>\n",
       "      <td>0.702854</td>\n",
       "      <td>0.205118</td>\n",
       "      <td>0.636151</td>\n",
       "      <td>0.150176</td>\n",
       "      <td>-0.402939</td>\n",
       "      <td>0.190723</td>\n",
       "      <td>0.162472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn30</th>\n",
       "      <td>0.200724</td>\n",
       "      <td>0.050243</td>\n",
       "      <td>0.173459</td>\n",
       "      <td>0.201006</td>\n",
       "      <td>0.143636</td>\n",
       "      <td>0.160461</td>\n",
       "      <td>0.163431</td>\n",
       "      <td>0.100896</td>\n",
       "      <td>0.183653</td>\n",
       "      <td>0.202282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501973</td>\n",
       "      <td>0.779874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.885302</td>\n",
       "      <td>0.158531</td>\n",
       "      <td>0.604611</td>\n",
       "      <td>0.031621</td>\n",
       "      <td>-0.515154</td>\n",
       "      <td>0.177190</td>\n",
       "      <td>0.157834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn40</th>\n",
       "      <td>0.194815</td>\n",
       "      <td>0.044897</td>\n",
       "      <td>0.175766</td>\n",
       "      <td>0.193767</td>\n",
       "      <td>0.130647</td>\n",
       "      <td>0.137634</td>\n",
       "      <td>0.138180</td>\n",
       "      <td>0.094074</td>\n",
       "      <td>0.173108</td>\n",
       "      <td>0.190247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468742</td>\n",
       "      <td>0.702854</td>\n",
       "      <td>0.885302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.147690</td>\n",
       "      <td>0.584441</td>\n",
       "      <td>-0.063601</td>\n",
       "      <td>-0.540741</td>\n",
       "      <td>0.155773</td>\n",
       "      <td>0.142754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.497330</td>\n",
       "      <td>0.819278</td>\n",
       "      <td>0.807017</td>\n",
       "      <td>0.506962</td>\n",
       "      <td>0.661229</td>\n",
       "      <td>0.707878</td>\n",
       "      <td>0.813687</td>\n",
       "      <td>0.749862</td>\n",
       "      <td>0.713715</td>\n",
       "      <td>0.757515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214535</td>\n",
       "      <td>0.205118</td>\n",
       "      <td>0.158531</td>\n",
       "      <td>0.147690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245935</td>\n",
       "      <td>0.204595</td>\n",
       "      <td>-0.056826</td>\n",
       "      <td>0.704309</td>\n",
       "      <td>0.706296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>0.182609</td>\n",
       "      <td>0.077376</td>\n",
       "      <td>0.265822</td>\n",
       "      <td>0.185959</td>\n",
       "      <td>0.226165</td>\n",
       "      <td>0.197299</td>\n",
       "      <td>0.184456</td>\n",
       "      <td>0.132279</td>\n",
       "      <td>0.281104</td>\n",
       "      <td>0.240501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551552</td>\n",
       "      <td>0.636151</td>\n",
       "      <td>0.604611</td>\n",
       "      <td>0.584441</td>\n",
       "      <td>0.245935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.239335</td>\n",
       "      <td>-0.679390</td>\n",
       "      <td>0.263286</td>\n",
       "      <td>0.235563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm1</th>\n",
       "      <td>0.094228</td>\n",
       "      <td>0.098996</td>\n",
       "      <td>0.104608</td>\n",
       "      <td>0.100220</td>\n",
       "      <td>0.121323</td>\n",
       "      <td>0.116518</td>\n",
       "      <td>0.127280</td>\n",
       "      <td>0.111240</td>\n",
       "      <td>0.083056</td>\n",
       "      <td>0.091318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297072</td>\n",
       "      <td>0.150176</td>\n",
       "      <td>0.031621</td>\n",
       "      <td>-0.063601</td>\n",
       "      <td>0.204595</td>\n",
       "      <td>0.239335</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116039</td>\n",
       "      <td>0.111844</td>\n",
       "      <td>0.100972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm2</th>\n",
       "      <td>-0.086705</td>\n",
       "      <td>0.027164</td>\n",
       "      <td>-0.138890</td>\n",
       "      <td>-0.069921</td>\n",
       "      <td>-0.067881</td>\n",
       "      <td>-0.018745</td>\n",
       "      <td>-0.004794</td>\n",
       "      <td>-0.004790</td>\n",
       "      <td>-0.112407</td>\n",
       "      <td>-0.060946</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304585</td>\n",
       "      <td>-0.402939</td>\n",
       "      <td>-0.515154</td>\n",
       "      <td>-0.540741</td>\n",
       "      <td>-0.056826</td>\n",
       "      <td>-0.679390</td>\n",
       "      <td>0.116039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.105255</td>\n",
       "      <td>-0.076028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgbc</th>\n",
       "      <td>0.610685</td>\n",
       "      <td>0.794286</td>\n",
       "      <td>0.792107</td>\n",
       "      <td>0.640781</td>\n",
       "      <td>0.839157</td>\n",
       "      <td>0.854310</td>\n",
       "      <td>0.870490</td>\n",
       "      <td>0.745700</td>\n",
       "      <td>0.789319</td>\n",
       "      <td>0.813848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165387</td>\n",
       "      <td>0.190723</td>\n",
       "      <td>0.177190</td>\n",
       "      <td>0.155773</td>\n",
       "      <td>0.704309</td>\n",
       "      <td>0.263286</td>\n",
       "      <td>0.111844</td>\n",
       "      <td>-0.105255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hgbc</th>\n",
       "      <td>0.570318</td>\n",
       "      <td>0.788858</td>\n",
       "      <td>0.740055</td>\n",
       "      <td>0.597434</td>\n",
       "      <td>0.850514</td>\n",
       "      <td>0.837709</td>\n",
       "      <td>0.879467</td>\n",
       "      <td>0.731774</td>\n",
       "      <td>0.824164</td>\n",
       "      <td>0.855249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169207</td>\n",
       "      <td>0.162472</td>\n",
       "      <td>0.157834</td>\n",
       "      <td>0.142754</td>\n",
       "      <td>0.706296</td>\n",
       "      <td>0.235563</td>\n",
       "      <td>0.100972</td>\n",
       "      <td>-0.076028</td>\n",
       "      <td>0.866787</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dt1       dt2       dt3       dt4       bg1       bg2        rf  \\\n",
       "dt1    1.000000  0.509019  0.600770  0.888561  0.662115  0.626134  0.573769   \n",
       "dt2    0.509019  1.000000  0.790363  0.525971  0.767341  0.796129  0.869158   \n",
       "dt3    0.600770  0.790363  1.000000  0.671223  0.762742  0.767269  0.794407   \n",
       "dt4    0.888561  0.525971  0.671223  1.000000  0.681076  0.655620  0.601554   \n",
       "bg1    0.662115  0.767341  0.762742  0.681076  1.000000  0.883143  0.835025   \n",
       "bg2    0.626134  0.796129  0.767269  0.655620  0.883143  1.000000  0.883418   \n",
       "rf     0.573769  0.869158  0.794407  0.601554  0.835025  0.883418  1.000000   \n",
       "rf1    0.471583  0.873051  0.702949  0.486600  0.735569  0.787885  0.826621   \n",
       "et     0.561496  0.731172  0.722393  0.601017  0.797730  0.863883  0.828469   \n",
       "et1    0.567571  0.813614  0.767932  0.585960  0.810062  0.857621  0.912319   \n",
       "knn5   0.130343 -0.001136  0.139695  0.122711  0.128423  0.110901  0.085400   \n",
       "knn10  0.132186  0.044659  0.188490  0.174363  0.194772  0.145082  0.142271   \n",
       "knn20  0.166604  0.051094  0.189819  0.175329  0.163484  0.159938  0.170270   \n",
       "knn30  0.200724  0.050243  0.173459  0.201006  0.143636  0.160461  0.163431   \n",
       "knn40  0.194815  0.044897  0.175766  0.193767  0.130647  0.137634  0.138180   \n",
       "lr     0.497330  0.819278  0.807017  0.506962  0.661229  0.707878  0.813687   \n",
       "svm    0.182609  0.077376  0.265822  0.185959  0.226165  0.197299  0.184456   \n",
       "svm1   0.094228  0.098996  0.104608  0.100220  0.121323  0.116518  0.127280   \n",
       "svm2  -0.086705  0.027164 -0.138890 -0.069921 -0.067881 -0.018745 -0.004794   \n",
       "xgbc   0.610685  0.794286  0.792107  0.640781  0.839157  0.854310  0.870490   \n",
       "hgbc   0.570318  0.788858  0.740055  0.597434  0.850514  0.837709  0.879467   \n",
       "\n",
       "            rf1        et       et1  ...     knn10     knn20     knn30  \\\n",
       "dt1    0.471583  0.561496  0.567571  ...  0.132186  0.166604  0.200724   \n",
       "dt2    0.873051  0.731172  0.813614  ...  0.044659  0.051094  0.050243   \n",
       "dt3    0.702949  0.722393  0.767932  ...  0.188490  0.189819  0.173459   \n",
       "dt4    0.486600  0.601017  0.585960  ...  0.174363  0.175329  0.201006   \n",
       "bg1    0.735569  0.797730  0.810062  ...  0.194772  0.163484  0.143636   \n",
       "bg2    0.787885  0.863883  0.857621  ...  0.145082  0.159938  0.160461   \n",
       "rf     0.826621  0.828469  0.912319  ...  0.142271  0.170270  0.163431   \n",
       "rf1    1.000000  0.690992  0.782468  ...  0.079662  0.099077  0.100896   \n",
       "et     0.690992  1.000000  0.873942  ...  0.182751  0.173869  0.183653   \n",
       "et1    0.782468  0.873942  1.000000  ...  0.174365  0.201161  0.202282   \n",
       "knn5   0.051657  0.121628  0.110626  ...  0.571613  0.476794  0.418021   \n",
       "knn10  0.079662  0.182751  0.174365  ...  1.000000  0.579624  0.501973   \n",
       "knn20  0.099077  0.173869  0.201161  ...  0.579624  1.000000  0.779874   \n",
       "knn30  0.100896  0.183653  0.202282  ...  0.501973  0.779874  1.000000   \n",
       "knn40  0.094074  0.173108  0.190247  ...  0.468742  0.702854  0.885302   \n",
       "lr     0.749862  0.713715  0.757515  ...  0.214535  0.205118  0.158531   \n",
       "svm    0.132279  0.281104  0.240501  ...  0.551552  0.636151  0.604611   \n",
       "svm1   0.111240  0.083056  0.091318  ...  0.297072  0.150176  0.031621   \n",
       "svm2  -0.004790 -0.112407 -0.060946  ... -0.304585 -0.402939 -0.515154   \n",
       "xgbc   0.745700  0.789319  0.813848  ...  0.165387  0.190723  0.177190   \n",
       "hgbc   0.731774  0.824164  0.855249  ...  0.169207  0.162472  0.157834   \n",
       "\n",
       "          knn40        lr       svm      svm1      svm2      xgbc      hgbc  \n",
       "dt1    0.194815  0.497330  0.182609  0.094228 -0.086705  0.610685  0.570318  \n",
       "dt2    0.044897  0.819278  0.077376  0.098996  0.027164  0.794286  0.788858  \n",
       "dt3    0.175766  0.807017  0.265822  0.104608 -0.138890  0.792107  0.740055  \n",
       "dt4    0.193767  0.506962  0.185959  0.100220 -0.069921  0.640781  0.597434  \n",
       "bg1    0.130647  0.661229  0.226165  0.121323 -0.067881  0.839157  0.850514  \n",
       "bg2    0.137634  0.707878  0.197299  0.116518 -0.018745  0.854310  0.837709  \n",
       "rf     0.138180  0.813687  0.184456  0.127280 -0.004794  0.870490  0.879467  \n",
       "rf1    0.094074  0.749862  0.132279  0.111240 -0.004790  0.745700  0.731774  \n",
       "et     0.173108  0.713715  0.281104  0.083056 -0.112407  0.789319  0.824164  \n",
       "et1    0.190247  0.757515  0.240501  0.091318 -0.060946  0.813848  0.855249  \n",
       "knn5   0.417068  0.128707  0.481747  0.241989 -0.230562  0.146303  0.103175  \n",
       "knn10  0.468742  0.214535  0.551552  0.297072 -0.304585  0.165387  0.169207  \n",
       "knn20  0.702854  0.205118  0.636151  0.150176 -0.402939  0.190723  0.162472  \n",
       "knn30  0.885302  0.158531  0.604611  0.031621 -0.515154  0.177190  0.157834  \n",
       "knn40  1.000000  0.147690  0.584441 -0.063601 -0.540741  0.155773  0.142754  \n",
       "lr     0.147690  1.000000  0.245935  0.204595 -0.056826  0.704309  0.706296  \n",
       "svm    0.584441  0.245935  1.000000  0.239335 -0.679390  0.263286  0.235563  \n",
       "svm1  -0.063601  0.204595  0.239335  1.000000  0.116039  0.111844  0.100972  \n",
       "svm2  -0.540741 -0.056826 -0.679390  0.116039  1.000000 -0.105255 -0.076028  \n",
       "xgbc   0.155773  0.704309  0.263286  0.111844 -0.105255  1.000000  0.866787  \n",
       "hgbc   0.142754  0.706296  0.235563  0.100972 -0.076028  0.866787  1.000000  \n",
       "\n",
       "[21 rows x 21 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d1d1975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(result_df.corr(),annot=True, cbar=False, cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0263f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [('dt1', dt_clf1), ('dt2', dt_clf2), ('dt3', dt_clf3), ('dt4', dt_clf4),\n",
    "        ('bg', bg_clf), ('bg1', bg_clf1), \n",
    "        ('rf', rf_clf), ('rf1', rf_clf1), \n",
    "        ('et', et_clf), ('et1', et_clf1),\n",
    "        ('knn5', knn_clf_5), ('knn10', knn_clf_10), ('knn20', knn_clf_20),\n",
    "        ('knn30', knn_clf_30), ('knn40', knn_clf_40),\n",
    "        ('lr', lr_clf),\n",
    "        ('svm', svm_clf), ('svm1', svm_clf1), ('svm2', svm_clf2),\n",
    "       ('xgbc',xgbcn),('hgbc',hgbcn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03aff42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:30:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:30:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vo_clf  = VotingClassifier(estimators=clfs, voting='soft')\n",
    "vo_clf.fit(X_train, y_train)\n",
    "vo_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7b227bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:31:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:31:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "vo_clf.fit(X, y)\n",
    "pred_vo = vo_clf.predict(df_kg)\n",
    "tit = pd.read_csv('test.csv')\n",
    "tit.drop(list(tit.columns)[1:], axis = 1, inplace=True) \n",
    "tit['Survived'] = pred_vo\n",
    "tit.set_index('PassengerId', inplace=True)\n",
    "tit.to_csv('tit_test(vo).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "91a76944",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [('bg', bg_clf), ('bg1', bg_clf1), \n",
    "        ('rf', rf_clf), ('rf1', rf_clf1), \n",
    "        ('et', et_clf), ('et1', et_clf1),\n",
    "        ('hgbc',hgbcn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "26412a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8777777777777778"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vo_clf  = VotingClassifier(estimators=clfs, voting='soft')\n",
    "vo_clf.fit(X_train, y_train)\n",
    "vo_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9593cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "vo_clf.fit(X, y)\n",
    "pred_vo = vo_clf.predict(df_kg)\n",
    "tit = pd.read_csv('test.csv')\n",
    "tit.drop(list(tit.columns)[1:], axis = 1, inplace=True) \n",
    "tit['Survived'] = pred_vo\n",
    "tit.set_index('PassengerId', inplace=True)\n",
    "tit.to_csv('tit_test(vb).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f071d295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.9.1-py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Collecting scikit-learn>=1.1.0\n",
      "  Downloading scikit_learn-1.1.2-cp39-cp39-macosx_10_9_x86_64.whl (8.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.7 MB 69.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
      "Installing collected packages: scikit-learn, imbalanced-learn, imblearn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "Successfully installed imbalanced-learn-0.9.1 imblearn-0.0 scikit-learn-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b6398b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.9/site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn --user --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b978233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Oversampling, the shape of tain_X : (980, 9)\n",
      "After Oversampling, the shape of tain_y : (980,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(sampling_strategy='auto',\n",
    "          random_state=19,\n",
    "          k_neighbors=5,\n",
    "          n_jobs=4)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "print('After Oversampling, the shape of tain_X : {}'.format(X_resampled.shape))\n",
    "print('After Oversampling, the shape of tain_y : {}'.format(y_resampled.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "534f8f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8111111111111111"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_resampled, y_resampled)\n",
    "pred_dt = dt.predict(X_val)\n",
    "accuracy_score(y_val, pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb007b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8111111111111111"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt2 = DecisionTreeClassifier()\n",
    "dt2.fit(X_train, y_train)\n",
    "pred_dt2 = dt.predict(X_val)\n",
    "accuracy_score(y_val, pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6be6354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8555555555555555"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_resampled, y_resampled)\n",
    "pred_rf = rf.predict(X_val)\n",
    "accuracy_score(y_val, pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f45dfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8555555555555555"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et = ExtraTreesClassifier()\n",
    "et.fit(X_resampled, y_resampled)\n",
    "pred_et = et.predict(X_val)\n",
    "accuracy_score(y_val, pred_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50be487a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg = BaggingClassifier()\n",
    "bg.fit(X_resampled, y_resampled)\n",
    "pred_bg = bg.predict(X_val)\n",
    "accuracy_score(y_val, pred_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d59c8660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87        59\n",
      "           1       0.77      0.74      0.75        31\n",
      "\n",
      "    accuracy                           0.83        90\n",
      "   macro avg       0.82      0.81      0.81        90\n",
      "weighted avg       0.83      0.83      0.83        90\n",
      "\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86        59\n",
      "           1       0.73      0.71      0.72        31\n",
      "\n",
      "    accuracy                           0.81        90\n",
      "   macro avg       0.79      0.79      0.79        90\n",
      "weighted avg       0.81      0.81      0.81        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,dt2.predict(X_val)))\n",
    "print('-'*60)\n",
    "print(metrics.classification_report(y_val,dt.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2101967b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Using backend LokyBackend with 19 concurrent workers.\n",
      "[Parallel(n_jobs=19)]: Done   3 out of  19 | elapsed:    2.8s remaining:   15.1s\n",
      "[Parallel(n_jobs=19)]: Done  10 out of  19 | elapsed:    2.9s remaining:    2.6s\n",
      "[Parallel(n_jobs=19)]: Done  17 out of  19 | elapsed:    3.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=19)]: Done  19 out of  19 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=19)]: Using backend LokyBackend with 19 concurrent workers.\n",
      "[Parallel(n_jobs=19)]: Done   3 out of  19 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=19)]: Done  10 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=19)]: Done  17 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=19)]: Done  19 out of  19 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:10:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:10:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "et1 = ExtraTreesClassifier(ccp_alpha=0.2, criterion='log_loss', max_depth=7,\n",
    "                     max_features=None, min_samples_leaf=3, min_samples_split=8,\n",
    "                     n_estimators=300, warm_start='False')\n",
    "et1.fit(X_train, y_train)\n",
    "pred_et = et1.predict(X_val)\n",
    "\n",
    "bg1 = BaggingClassifier(max_samples=0.4, n_estimators=220, n_jobs=19, random_state=0,\n",
    "                  verbose=3, warm_start='False')\n",
    "bg1.fit(X_train, y_train)\n",
    "pred_bg = bg1.predict(X_val)\n",
    "\n",
    "rf1 = RandomForestClassifier(criterion='entropy', max_features=None,\n",
    "                       min_samples_split=7, warm_start='True')\n",
    "rf1.fit(X_train, y_train)\n",
    "pred_rf = rf1.predict(X_val)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(base_score=0.5, booster='gbtree', ccp_alpha=0.7638002771865584,\n",
    "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
    "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
    "              importance_type=None, interaction_constraints='',\n",
    "              learning_rate=0.01, loss='exponential', max_delta_step=0,\n",
    "              max_depth=8, max_features=None, min_child_weight=1,\n",
    "              min_impurity_decrease=0.969767042752594, min_samples_leaf=20,\n",
    "              min_samples_split=0.8119215852058577,\n",
    "              monotone_constraints='()', n_estimators=300, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1)\n",
    "xgb.fit(X_train, y_train)\n",
    "pred_xgb = xgb.predict(X_val)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_resampled, y_resampled)\n",
    "pred_dt = dt.predict(X_val)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "hgb = HistGradientBoostingClassifier(l2_regularization=0.2, learning_rate=0.01,\n",
    "                               loss='auto', max_depth=4, max_iter=400,\n",
    "                               min_samples_leaf=12, n_iter_no_change=20,\n",
    "                               warm_start='False')\n",
    "hgb.fit(X_train, y_train)\n",
    "pred_hgb = hgb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "164b396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = pd.DataFrame([pred_bg, pred_et, pred_hgb, pred_rf, pred_xgb, y_val]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c589407",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.rename({0:'bag', 1:'ext', 2:'hgb', 3:'rf', 4:'xgb', 5:'y_val'},axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ece5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6fb53512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bag</th>\n",
       "      <th>ext</th>\n",
       "      <th>hgb</th>\n",
       "      <th>rf</th>\n",
       "      <th>xgb</th>\n",
       "      <th>y_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bag  ext  hgb  rf  xgb  y_val\n",
       "0     0    0    0   0    0      0\n",
       "1     0    0    0   0    0      0\n",
       "2     0    0    0   0    0      0\n",
       "3     0    0    0   0    0      1\n",
       "4     0    0    0   0    0      0\n",
       "5     0    0    0   0    0      0\n",
       "6     1    1    1   1    1      1\n",
       "7     0    1    0   0    0      0\n",
       "8     0    0    0   0    0      0\n",
       "9     0    0    0   0    0      0\n",
       "10    1    1    1   1    1      1\n",
       "11    0    0    0   0    0      0\n",
       "12    0    0    0   0    0      0\n",
       "13    1    1    1   1    1      1\n",
       "14    1    0    0   1    1      1\n",
       "15    0    0    0   0    0      0\n",
       "16    0    0    0   0    0      0\n",
       "17    0    0    0   0    0      0\n",
       "18    0    0    0   0    0      0\n",
       "19    0    0    0   0    0      0\n",
       "20    1    1    1   1    1      1\n",
       "21    0    0    0   0    0      0\n",
       "22    0    0    0   0    0      0\n",
       "23    0    0    1   0    0      1\n",
       "24    0    0    0   0    0      0\n",
       "25    1    1    1   1    1      1\n",
       "26    1    0    1   1    0      1\n",
       "27    0    0    0   0    1      1\n",
       "28    0    1    0   0    0      0\n",
       "29    0    0    0   0    0      0\n",
       "30    1    0    0   1    1      0\n",
       "31    0    0    0   0    0      0\n",
       "32    1    1    1   1    1      1\n",
       "33    0    0    0   0    0      0\n",
       "34    1    1    1   1    1      1\n",
       "35    1    1    1   1    1      1\n",
       "36    1    0    0   1    1      1\n",
       "37    0    0    0   0    0      0\n",
       "38    0    0    0   0    0      0\n",
       "39    1    1    1   1    1      1\n",
       "40    0    1    0   0    0      1\n",
       "41    0    0    0   0    0      1\n",
       "42    0    1    0   0    0      0\n",
       "43    0    0    0   0    0      0\n",
       "44    1    1    1   1    1      1\n",
       "45    1    1    1   1    1      1\n",
       "46    0    0    0   0    0      0\n",
       "47    1    1    1   1    1      1\n",
       "48    0    0    0   0    0      0\n",
       "49    0    0    0   0    0      0\n",
       "50    0    0    0   0    0      0\n",
       "51    0    0    0   0    0      0\n",
       "52    0    0    1   1    1      1\n",
       "53    0    0    0   0    0      0\n",
       "54    0    0    0   0    0      0\n",
       "55    0    0    0   0    0      0\n",
       "56    1    1    1   1    1      1\n",
       "57    0    0    0   0    0      0\n",
       "58    0    0    0   0    0      0\n",
       "59    0    0    0   0    0      0\n",
       "60    1    1    1   1    1      1\n",
       "61    0    0    0   0    0      0\n",
       "62    0    0    0   0    0      0\n",
       "63    0    0    0   0    0      0\n",
       "64    0    0    0   0    0      0\n",
       "65    0    0    0   0    0      0\n",
       "66    1    1    1   1    1      1\n",
       "67    0    0    0   0    0      0\n",
       "68    0    1    0   0    0      0\n",
       "69    1    1    1   1    1      1\n",
       "70    0    0    0   0    0      0\n",
       "71    0    0    0   0    0      0\n",
       "72    0    0    0   0    0      0\n",
       "73    0    0    0   0    0      0\n",
       "74    1    1    1   1    1      1\n",
       "75    0    0    0   0    0      0\n",
       "76    1    1    1   1    1      1\n",
       "77    0    0    0   0    0      0\n",
       "78    0    1    1   0    0      1\n",
       "79    0    0    0   0    0      0\n",
       "80    0    0    0   0    0      0\n",
       "81    0    0    0   0    0      0\n",
       "82    0    0    0   0    0      0\n",
       "83    0    0    0   0    0      0\n",
       "84    0    0    0   0    0      0\n",
       "85    0    0    0   0    0      1\n",
       "86    0    0    0   0    0      0\n",
       "87    1    0    1   1    1      1\n",
       "88    0    0    0   0    0      0\n",
       "89    0    0    0   0    0      1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc9a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00135418",
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0c23d9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████| 500/500 [00:04<00:00, 110.45trial/s, best loss: -0.7555555555555555]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def kn_objective(search_space):\n",
    "    model = KNeighborsClassifier(**search_space)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# new search space\n",
    "search_space={'n_neighbors':hp.choice('n_neighbors', range(2,10)),\n",
    "              'leaf_size':hp.choice('leaf_size', range(10,100,10)),\n",
    "              'p':hp.choice('p', range(1, 10)),\n",
    "              'n_jobs':hp.choice('n_jobs', range(1,10))}\n",
    "\n",
    "# set the hyperparam tuning algorithm\n",
    "algorithm=tpe.suggest\n",
    "# implement Hyperopt\n",
    "best_params = fmin(fn=kn_objective,\n",
    "                   space=search_space,\n",
    "                   algo=algorithm,\n",
    "                   max_evals=500)\n",
    "\n",
    "params1 = space_eval(search_space, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "aea78f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'leaf_size': 1, 'n_jobs': 3, 'n_neighbors': 3, 'p': 1}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d46f9fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7555555555555555"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_kn_clf = KNeighborsClassifier(**params1)\n",
    "best_kn_clf.fit(X_train, y_train)\n",
    "best_kn_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3c5e5453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(leaf_size=20, n_jobs=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(leaf_size=20, n_jobs=4)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(leaf_size=20, n_jobs=4)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_kn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "07a514f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████| 500/500 [01:17<00:00,  6.44trial/s, best loss: -0.8333333333333334]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def ex_objective(search_space):\n",
    "    model = ExtraTreesClassifier(**search_space)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# new search space\n",
    "search_space={'criterion':hp.choice('criterion', ['gini', 'entropy','log_loss']),\n",
    "              'max_depth':hp.choice('max_depth', range(4,14)),\n",
    "              'min_samples_split':hp.choice('min_samples_split', range(2, 10)),\n",
    "              'min_samples_leaf':hp.choice('min_samples_leaf', range(1, 10)),\n",
    "              'max_features':hp.choice('max_features', [\"sqrt\",\"log2\",None]),\n",
    "              'min_impurity_decrease':hp.choice('min_impurity_decrease', [0.0,0.2,0.4,0.6,0.8,1]),\n",
    "              'ccp_alpha':hp.choice('ccp_alpha', [0.0,0.2,0.4,0.6,0.8,1]),\n",
    "              'warm_start':hp.choice('warm_start', ['True', 'False']),\n",
    "              'n_estimators':hp.choice('n_estimators', range(100,500,100))}\n",
    "\n",
    "\n",
    "\n",
    "# set the hyperparam tuning algorithm\n",
    "algorithm=tpe.suggest\n",
    "# implement Hyperopt\n",
    "best_params = fmin(fn=ex_objective,\n",
    "                   space=search_space,\n",
    "                   algo=algorithm,\n",
    "                   max_evals=500)\n",
    "\n",
    "params1 = space_eval(search_space, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "189c17ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 1,\n",
       " 'criterion': 2,\n",
       " 'max_depth': 3,\n",
       " 'max_features': 2,\n",
       " 'min_impurity_decrease': 0,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 6,\n",
       " 'n_estimators': 2,\n",
       " 'warm_start': 1}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f8e455c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ex_clf = ExtraTreesClassifier(**params1)\n",
    "best_ex_clf.fit(X_train, y_train)\n",
    "best_ex_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3c59833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesClassifier(ccp_alpha=0.2, criterion=&#x27;log_loss&#x27;, max_depth=7,\n",
       "                     max_features=None, min_samples_leaf=3, min_samples_split=8,\n",
       "                     n_estimators=300, warm_start=&#x27;False&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesClassifier</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesClassifier(ccp_alpha=0.2, criterion=&#x27;log_loss&#x27;, max_depth=7,\n",
       "                     max_features=None, min_samples_leaf=3, min_samples_split=8,\n",
       "                     n_estimators=300, warm_start=&#x27;False&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesClassifier(ccp_alpha=0.2, criterion='log_loss', max_depth=7,\n",
       "                     max_features=None, min_samples_leaf=3, min_samples_split=8,\n",
       "                     n_estimators=300, warm_start='False')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ex_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "be80ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stack = stack.drop('y_val', axis=1)\n",
    "y_stack = stack.y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b52ebae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct = DecisionTreeClassifier()\n",
    "dct.fit(X_stack, y_stack)\n",
    "pred_stack = dct.predict(X_stack)\n",
    "accuracy_score(y_val, pred_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "046a6f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier()\n",
    "rf1.fit(X_stack, y_stack)\n",
    "pred_stack_rf1 = rf1.predict(X_stack)\n",
    "accuracy_score(y_val, pred_stack_rf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0dbb0f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Using backend LokyBackend with 19 concurrent workers.\n",
      "[Parallel(n_jobs=19)]: Done   3 out of  19 | elapsed:    2.8s remaining:   14.7s\n",
      "[Parallel(n_jobs=19)]: Done  10 out of  19 | elapsed:    2.9s remaining:    2.6s\n",
      "[Parallel(n_jobs=19)]: Done  17 out of  19 | elapsed:    3.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=19)]: Done  19 out of  19 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=19)]: Using backend LokyBackend with 19 concurrent workers.\n",
      "[Parallel(n_jobs=19)]: Done   3 out of  19 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=19)]: Done  10 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=19)]: Done  17 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=19)]: Done  19 out of  19 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:38:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:38:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "#테스트 예측\n",
    "\n",
    "et1 = ExtraTreesClassifier(ccp_alpha=0.2, criterion='log_loss', max_depth=7,\n",
    "                     max_features=None, min_samples_leaf=3, min_samples_split=8,\n",
    "                     n_estimators=300, warm_start='False')\n",
    "et1.fit(X, y)\n",
    "pred_et1 = et1.predict(df_kg)\n",
    "\n",
    "bg1 = BaggingClassifier(max_samples=0.4, n_estimators=220, n_jobs=19, random_state=0,\n",
    "                  verbose=3, warm_start='False')\n",
    "bg1.fit(X, y)\n",
    "pred_bg1 = bg1.predict(df_kg)\n",
    "\n",
    "rf1 = RandomForestClassifier(criterion='entropy', max_features=None,\n",
    "                       min_samples_split=7, warm_start='True')\n",
    "rf1.fit(X, y)\n",
    "pred_rf1 = rf1.predict(df_kg)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(base_score=0.5, booster='gbtree', ccp_alpha=0.7638002771865584,\n",
    "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
    "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
    "              importance_type=None, interaction_constraints='',\n",
    "              learning_rate=0.01, loss='exponential', max_delta_step=0,\n",
    "              max_depth=8, max_features=None, min_child_weight=1,\n",
    "              min_impurity_decrease=0.969767042752594, min_samples_leaf=20,\n",
    "              min_samples_split=0.8119215852058577,\n",
    "              monotone_constraints='()', n_estimators=300, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1)\n",
    "xgb.fit(X, y)\n",
    "pred_xgb1 = xgb.predict(df_kg)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "hgb = HistGradientBoostingClassifier(l2_regularization=0.2, learning_rate=0.01,\n",
    "                               loss='auto', max_depth=4, max_iter=400,\n",
    "                               min_samples_leaf=12, n_iter_no_change=20,\n",
    "                               warm_start='False')\n",
    "hgb.fit(X, y)\n",
    "pred_hgb1 = hgb.predict(df_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b8371a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "hgb = HistGradientBoostingClassifier(l2_regularization=0.2, learning_rate=0.01,\n",
    "                               loss='auto', max_depth=4, max_iter=400,\n",
    "                               min_samples_leaf=12, n_iter_no_change=20,\n",
    "                               warm_start='False')\n",
    "hgb.fit(X_train, y_train)\n",
    "pred_hgb1 = hgb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b71a1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier(criterion='entropy', max_features=None,\n",
    "                       min_samples_split=7, warm_start='True')\n",
    "rf1.fit(X_train, y_train)\n",
    "pred_rf1 = rf1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b30788c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "et1 = ExtraTreesClassifier(ccp_alpha=0.2, criterion='log_loss', max_depth=7,\n",
    "                     max_features=None, min_samples_leaf=3, min_samples_split=8,\n",
    "                     n_estimators=300, warm_start='False')\n",
    "et1.fit(X_train, y_train)\n",
    "pred_et1 = et1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "55fcce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:55:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:55:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(base_score=0.5, booster='gbtree', ccp_alpha=0.7638002771865584,\n",
    "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
    "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
    "              importance_type=None, interaction_constraints='',\n",
    "              learning_rate=0.01, loss='exponential', max_delta_step=0,\n",
    "              max_depth=8, max_features=None, min_child_weight=1,\n",
    "              min_impurity_decrease=0.969767042752594, min_samples_leaf=20,\n",
    "              min_samples_split=0.8119215852058577,\n",
    "              monotone_constraints='()', n_estimators=300, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1)\n",
    "xgb.fit(X_train, y_train)\n",
    "pred_xgb1 = xgb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "42079c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Using backend LokyBackend with 19 concurrent workers.\n",
      "[Parallel(n_jobs=19)]: Done   3 out of  19 | elapsed:    2.8s remaining:   15.0s\n",
      "[Parallel(n_jobs=19)]: Done  10 out of  19 | elapsed:    2.9s remaining:    2.6s\n",
      "[Parallel(n_jobs=19)]: Done  17 out of  19 | elapsed:    3.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=19)]: Done  19 out of  19 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=19)]: Using backend LokyBackend with 19 concurrent workers.\n",
      "[Parallel(n_jobs=19)]: Done   3 out of  19 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=19)]: Done  10 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=19)]: Done  17 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=19)]: Done  19 out of  19 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "bg1 = BaggingClassifier(max_samples=0.4, n_estimators=220, n_jobs=19, random_state=0,\n",
    "                  verbose=3, warm_start='False')\n",
    "bg1.fit(X_train, y_train)\n",
    "pred_bg1 = bg1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0721967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "et1 = ExtraTreesClassifier(ccp_alpha=0.2, criterion='log_loss', max_depth=7,\n",
    "                     max_features=None, min_samples_leaf=3, min_samples_split=8,\n",
    "                     n_estimators=300, warm_start='False')\n",
    "et1.fit(X, y)\n",
    "pred_et2 = et1.predict(df_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3ed256ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Using backend LokyBackend with 19 concurrent workers.\n",
      "[Parallel(n_jobs=19)]: Done   3 out of  19 | elapsed:    2.6s remaining:   13.9s\n",
      "[Parallel(n_jobs=19)]: Done  10 out of  19 | elapsed:    2.9s remaining:    2.6s\n",
      "[Parallel(n_jobs=19)]: Done  17 out of  19 | elapsed:    2.9s remaining:    0.3s\n",
      "[Parallel(n_jobs=19)]: Done  19 out of  19 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=19)]: Using backend LokyBackend with 19 concurrent workers.\n",
      "[Parallel(n_jobs=19)]: Done   3 out of  19 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=19)]: Done  10 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=19)]: Done  17 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=19)]: Done  19 out of  19 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "bg1 = BaggingClassifier(max_samples=0.4, n_estimators=220, n_jobs=19, random_state=0,\n",
    "                  verbose=3, warm_start='False')\n",
    "bg1.fit(X, y)\n",
    "pred_bg2 = bg1.predict(df_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e8cebe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier(criterion='entropy', max_features=None,\n",
    "                       min_samples_split=7, warm_start='True')\n",
    "rf1.fit(X, y)\n",
    "pred_rf2 = rf1.predict(df_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3d332c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:08:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"ccp_alpha\", \"loss\", \"min_impurity_decrease\", \"min_samples_leaf\", \"min_samples_split\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:08:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(base_score=0.5, booster='gbtree', ccp_alpha=0.7638002771865584,\n",
    "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
    "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
    "              importance_type=None, interaction_constraints='',\n",
    "              learning_rate=0.01, loss='exponential', max_delta_step=0,\n",
    "              max_depth=8, max_features=None, min_child_weight=1,\n",
    "              min_impurity_decrease=0.969767042752594, min_samples_leaf=20,\n",
    "              min_samples_split=0.8119215852058577,\n",
    "              monotone_constraints='()', n_estimators=300, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1)\n",
    "xgb.fit(X, y)\n",
    "pred_xgb2 = xgb.predict(df_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e4971e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "hgb = HistGradientBoostingClassifier(l2_regularization=0.2, learning_rate=0.01,\n",
    "                               loss='auto', max_depth=4, max_iter=400,\n",
    "                               min_samples_leaf=12, n_iter_no_change=20,\n",
    "                               warm_start='False')\n",
    "hgb.fit(X, y)\n",
    "pred_hgb2 = hgb.predict(df_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "31a4e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack1 = pd.DataFrame([np.array(pred_hgb1, int), \n",
    "                       np.array(pred_rf1, int), \n",
    "                       np.array(pred_et1, int), \n",
    "                       np.array(pred_xgb1, int), \n",
    "                       np.array(pred_bg1, int)]).T\n",
    "stack1.rename({0:'hgb', 1:'rf', 2:'ex', 3:'xgb', 4:'bg'},axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f9cf03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_test = pd.DataFrame([np.array(pred_hgb2, int), \n",
    "                       np.array(pred_rf2, int), \n",
    "                       np.array(pred_et2, int), \n",
    "                       np.array(pred_xgb2, int), \n",
    "                       np.array(pred_bg2, int)]).T\n",
    "stack1.rename({0:'hgb', 1:'rf', 2:'ex', 3:'xgb', 4:'bg'},axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "477f36a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hgb</th>\n",
       "      <th>rf</th>\n",
       "      <th>ex</th>\n",
       "      <th>xgb</th>\n",
       "      <th>bg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hgb  rf  ex  xgb  bg\n",
       "0      0   0   0    0   0\n",
       "1      1   1   1    1   1\n",
       "2      1   1   1    1   1\n",
       "3      1   1   1    1   1\n",
       "4      0   0   0    0   0\n",
       "5      0   0   0    0   0\n",
       "6      0   0   0    0   0\n",
       "7      0   0   0    0   0\n",
       "8      0   1   1    1   0\n",
       "9      1   1   1    1   1\n",
       "10     1   1   1    1   1\n",
       "11     1   1   1    1   1\n",
       "12     0   0   0    0   0\n",
       "13     0   0   0    0   0\n",
       "14     0   0   1    0   0\n",
       "15     1   1   1    1   1\n",
       "16     0   0   0    0   0\n",
       "17     0   0   0    0   0\n",
       "18     0   0   1    0   0\n",
       "19     1   1   1    1   1\n",
       "20     0   0   0    0   0\n",
       "21     0   1   0    1   1\n",
       "22     1   1   1    1   1\n",
       "23     1   0   0    1   0\n",
       "24     0   0   1    0   0\n",
       "25     0   0   1    1   0\n",
       "26     0   0   0    0   0\n",
       "27     1   0   0    0   0\n",
       "28     1   1   1    1   1\n",
       "29     0   0   0    0   0\n",
       "30     0   0   0    0   0\n",
       "31     1   1   1    1   1\n",
       "32     1   1   1    1   1\n",
       "33     0   0   0    0   0\n",
       "34     0   0   0    0   0\n",
       "35     0   0   0    0   0\n",
       "36     0   0   0    0   0\n",
       "37     0   0   0    0   0\n",
       "38     0   0   1    0   0\n",
       "39     1   1   1    1   1\n",
       "40     0   0   1    0   0\n",
       "41     1   0   1    1   1\n",
       "42     0   0   0    0   0\n",
       "43     1   1   1    1   1\n",
       "44     1   1   1    1   1\n",
       "45     0   0   0    0   0\n",
       "46     0   0   0    0   0\n",
       "47     1   1   1    1   1\n",
       "48     0   0   0    0   0\n",
       "49     0   0   1    0   0\n",
       "50     0   0   0    0   0\n",
       "51     0   0   0    0   0\n",
       "52     1   1   1    1   1\n",
       "53     1   1   1    1   1\n",
       "54     0   0   0    0   0\n",
       "55     1   1   0    1   1\n",
       "56     1   1   1    1   1\n",
       "57     0   0   0    0   0\n",
       "58     1   1   1    1   1\n",
       "59     0   0   0    0   0\n",
       "60     0   0   0    0   0\n",
       "61     1   1   1    1   1\n",
       "62     0   0   0    0   0\n",
       "63     0   0   0    0   0\n",
       "64     0   0   0    0   0\n",
       "65     0   1   0    1   1\n",
       "66     1   1   1    1   1\n",
       "67     0   0   0    0   0\n",
       "68     1   1   1    1   1\n",
       "69     0   0   0    0   0\n",
       "70     0   0   0    0   0\n",
       "71     0   0   1    0   0\n",
       "72     0   0   0    0   0\n",
       "73     0   0   0    0   0\n",
       "74     1   1   0    1   1\n",
       "75     0   0   0    0   0\n",
       "76     0   0   0    0   0\n",
       "77     0   0   0    0   0\n",
       "78     1   1   0    1   1\n",
       "79     0   1   1    1   1\n",
       "80     0   0   0    0   0\n",
       "81     0   1   0    0   0\n",
       "82     1   1   1    1   1\n",
       "83     0   0   0    0   0\n",
       "84     1   1   1    1   1\n",
       "85     1   1   1    1   1\n",
       "86     0   0   0    0   0\n",
       "87     0   0   0    0   0\n",
       "88     1   1   1    1   1\n",
       "89     0   0   0    0   0\n",
       "90     0   0   0    0   0\n",
       "91     0   0   0    0   0\n",
       "92     0   0   0    0   0\n",
       "93     0   0   0    0   0\n",
       "94     0   0   0    0   0\n",
       "95     0   0   0    0   0\n",
       "96     0   0   0    0   0\n",
       "97     0   1   0    0   0\n",
       "98     1   1   1    1   1\n",
       "99     0   0   0    0   0\n",
       "100    1   0   1    0   0\n",
       "101    0   0   0    0   0\n",
       "102    0   0   0    0   0\n",
       "103    0   0   0    0   0\n",
       "104    0   0   0    0   0\n",
       "105    0   0   0    0   0\n",
       "106    1   1   1    1   1\n",
       "107    0   0   0    0   0\n",
       "108    0   0   0    0   0\n",
       "109    0   1   1    1   1\n",
       "110    0   0   0    0   0\n",
       "111    1   0   1    0   0\n",
       "112    0   0   0    0   0\n",
       "113    0   0   1    0   0\n",
       "114    1   0   1    0   0\n",
       "115    0   0   0    0   0\n",
       "116    0   0   0    0   0\n",
       "117    0   0   0    0   0\n",
       "118    0   0   0    0   0\n",
       "119    0   0   1    0   0\n",
       "120    0   0   0    0   0\n",
       "121    0   0   0    0   0\n",
       "122    0   0   0    0   0\n",
       "123    1   1   1    1   1\n",
       "124    0   0   0    0   0\n",
       "125    1   1   0    1   1\n",
       "126    0   0   0    0   0\n",
       "127    0   0   0    0   0\n",
       "128    1   1   1    1   1\n",
       "129    0   0   0    0   0\n",
       "130    0   0   0    0   0\n",
       "131    0   0   0    0   0\n",
       "132    0   0   1    0   0\n",
       "133    1   1   1    1   1\n",
       "134    0   0   0    0   0\n",
       "135    0   0   0    0   0\n",
       "136    1   1   1    1   1\n",
       "137    0   0   0    0   0\n",
       "138    0   0   0    0   0\n",
       "139    0   0   0    0   0\n",
       "140    0   0   1    0   0\n",
       "141    1   1   1    1   1\n",
       "142    1   1   1    1   1\n",
       "143    0   0   0    0   0\n",
       "144    0   0   0    0   0\n",
       "145    0   0   0    0   0\n",
       "146    0   1   0    0   0\n",
       "147    0   0   1    0   0\n",
       "148    0   0   0    0   0\n",
       "149    0   0   0    0   0\n",
       "150    0   0   0    0   0\n",
       "151    1   1   1    1   1\n",
       "152    0   0   0    0   0\n",
       "153    0   0   0    0   0\n",
       "154    0   0   0    0   0\n",
       "155    0   0   0    0   0\n",
       "156    1   1   1    1   1\n",
       "157    0   0   0    0   0\n",
       "158    0   0   0    0   0\n",
       "159    0   0   0    0   0\n",
       "160    0   0   0    0   0\n",
       "161    1   1   1    1   1\n",
       "162    0   0   0    0   0\n",
       "163    0   0   0    0   0\n",
       "164    0   0   0    0   0\n",
       "165    1   1   0    1   0\n",
       "166    1   1   1    1   1\n",
       "167    0   0   1    0   0\n",
       "168    0   0   0    0   0\n",
       "169    0   1   0    1   1\n",
       "170    0   0   0    0   0\n",
       "171    0   0   0    0   0\n",
       "172    1   1   1    1   1\n",
       "173    0   0   0    0   0\n",
       "174    0   0   0    0   0\n",
       "175    0   0   0    0   0\n",
       "176    0   0   0    0   0\n",
       "177    1   1   1    1   1\n",
       "178    0   0   0    0   0\n",
       "179    0   0   0    0   0\n",
       "180    0   0   1    0   0\n",
       "181    0   0   0    0   0\n",
       "182    0   0   0    0   0\n",
       "183    1   1   0    1   1\n",
       "184    1   1   1    1   1\n",
       "185    0   0   0    0   0\n",
       "186    1   1   1    1   1\n",
       "187    0   1   0    1   1\n",
       "188    0   0   0    0   0\n",
       "189    0   0   0    0   0\n",
       "190    1   1   1    1   1\n",
       "191    0   0   0    0   0\n",
       "192    1   0   1    0   0\n",
       "193    1   1   0    1   1\n",
       "194    1   1   1    1   1\n",
       "195    1   1   1    1   1\n",
       "196    0   0   0    0   0\n",
       "197    0   0   0    0   0\n",
       "198    1   1   1    1   1\n",
       "199    1   1   1    1   1\n",
       "200    0   0   0    0   0\n",
       "201    0   0   0    0   0\n",
       "202    0   0   0    0   0\n",
       "203    0   0   0    0   0\n",
       "204    0   0   0    0   0\n",
       "205    1   0   1    0   0\n",
       "206    0   0   0    0   0\n",
       "207    0   0   0    0   0\n",
       "208    1   1   1    1   1\n",
       "209    1   1   0    1   1\n",
       "210    0   0   0    0   0\n",
       "211    1   1   1    1   1\n",
       "212    0   0   0    0   0\n",
       "213    0   0   0    0   0\n",
       "214    0   0   0    0   0\n",
       "215    1   1   1    1   1\n",
       "216    1   1   1    1   1\n",
       "217    0   0   0    0   0\n",
       "218    1   1   1    1   1\n",
       "219    0   0   0    0   0\n",
       "220    0   1   0    0   0\n",
       "221    0   0   0    0   0\n",
       "222    0   0   0    0   0\n",
       "223    0   0   0    0   0\n",
       "224    1   1   0    0   1\n",
       "225    0   0   0    0   0\n",
       "226    0   0   0    0   0\n",
       "227    0   0   0    0   0\n",
       "228    0   0   0    0   0\n",
       "229    0   0   1    0   0\n",
       "230    1   1   1    1   1\n",
       "231    0   0   0    0   0\n",
       "232    0   0   0    0   0\n",
       "233    0   0   1    1   0\n",
       "234    0   0   0    0   0\n",
       "235    1   0   1    0   0\n",
       "236    0   0   0    0   0\n",
       "237    1   1   1    1   1\n",
       "238    0   0   0    0   0\n",
       "239    0   0   0    0   0\n",
       "240    1   0   1    0   0\n",
       "241    1   1   1    1   1\n",
       "242    0   0   0    0   0\n",
       "243    0   0   0    0   0\n",
       "244    0   0   0    0   0\n",
       "245    0   0   0    0   0\n",
       "246    1   0   1    0   1\n",
       "247    1   1   1    1   1\n",
       "248    1   1   0    1   1\n",
       "249    0   0   0    0   0\n",
       "250    0   0   0    0   0\n",
       "251    0   0   1    0   0\n",
       "252    0   0   0    0   0\n",
       "253    0   0   0    0   0\n",
       "254    0   0   1    0   0\n",
       "255    1   1   1    1   1\n",
       "256    1   1   1    1   1\n",
       "257    1   1   1    1   1\n",
       "258    1   1   1    1   1\n",
       "259    1   1   1    1   1\n",
       "260    0   0   0    0   0\n",
       "261    0   0   0    0   0\n",
       "262    0   0   0    0   0\n",
       "263    0   0   0    0   0\n",
       "264    1   1   1    1   1\n",
       "265    0   0   0    0   0\n",
       "266    0   0   0    0   0\n",
       "267    0   0   0    0   0\n",
       "268    1   1   1    1   1\n",
       "269    1   1   1    1   1\n",
       "270    0   0   0    0   0\n",
       "271    0   0   0    0   0\n",
       "272    1   1   1    1   1\n",
       "273    0   0   0    0   0\n",
       "274    1   1   1    1   1\n",
       "275    1   1   1    1   1\n",
       "276    0   0   1    0   0\n",
       "277    0   0   0    0   0\n",
       "278    0   0   0    0   0\n",
       "279    0   1   1    1   1\n",
       "280    0   0   0    0   0\n",
       "281    0   0   0    0   0\n",
       "282    0   0   0    0   0\n",
       "283    0   0   0    0   0\n",
       "284    0   0   0    0   0\n",
       "285    0   0   0    0   0\n",
       "286    0   0   0    0   0\n",
       "287    0   0   0    0   0\n",
       "288    0   0   0    0   0\n",
       "289    1   1   1    1   1\n",
       "290    1   1   1    1   1\n",
       "291    1   1   1    1   1\n",
       "292    0   0   0    0   0\n",
       "293    0   0   1    0   0\n",
       "294    0   0   0    0   0\n",
       "295    0   0   0    0   0\n",
       "296    0   0   0    0   0\n",
       "297    1   0   1    1   0\n",
       "298    1   1   0    1   1\n",
       "299    1   1   1    1   1\n",
       "300    1   1   1    1   1\n",
       "301    0   0   0    0   0\n",
       "302    0   0   0    0   0\n",
       "303    1   1   1    1   1\n",
       "304    0   0   0    0   0\n",
       "305    1   1   0    1   1\n",
       "306    1   1   1    1   1\n",
       "307    1   1   1    1   1\n",
       "308    0   0   0    0   0\n",
       "309    1   1   1    1   1\n",
       "310    1   1   1    1   1\n",
       "311    1   1   1    1   1\n",
       "312    1   1   1    1   1\n",
       "313    0   0   0    0   0\n",
       "314    0   0   0    0   0\n",
       "315    1   1   1    1   1\n",
       "316    1   1   1    1   1\n",
       "317    0   0   0    0   0\n",
       "318    1   1   1    1   1\n",
       "319    1   1   1    1   1\n",
       "320    0   0   0    0   0\n",
       "321    0   0   0    0   0\n",
       "322    1   1   1    1   1\n",
       "323    1   1   1    1   1\n",
       "324    0   0   0    0   0\n",
       "325    1   1   1    1   1\n",
       "326    0   0   0    0   0\n",
       "327    1   1   1    1   1\n",
       "328    0   1   1    1   1\n",
       "329    1   1   1    1   1\n",
       "330    1   1   1    1   1\n",
       "331    0   0   0    0   0\n",
       "332    0   0   0    0   0\n",
       "333    0   0   0    0   0\n",
       "334    1   1   1    1   1\n",
       "335    0   0   0    0   0\n",
       "336    0   0   0    0   0\n",
       "337    1   1   1    1   1\n",
       "338    0   0   0    0   0\n",
       "339    0   0   0    0   0\n",
       "340    1   1   0    1   1\n",
       "341    1   1   1    1   1\n",
       "342    0   0   0    0   0\n",
       "343    0   0   0    0   0\n",
       "344    0   0   0    0   0\n",
       "345    1   1   1    1   1\n",
       "346    1   1   1    1   1\n",
       "347    1   1   1    1   1\n",
       "348    1   1   0    1   1\n",
       "349    0   0   0    0   0\n",
       "350    0   0   0    0   0\n",
       "351    0   0   0    0   0\n",
       "352    0   0   0    0   0\n",
       "353    0   0   0    0   0\n",
       "354    0   0   0    0   0\n",
       "355    0   0   0    0   0\n",
       "356    1   1   1    1   1\n",
       "357    1   1   1    1   1\n",
       "358    1   1   1    1   1\n",
       "359    1   1   1    1   1\n",
       "360    0   0   0    0   0\n",
       "361    0   0   0    0   0\n",
       "362    0   0   1    0   0\n",
       "363    0   0   0    0   0\n",
       "364    0   0   0    0   0\n",
       "365    0   0   0    0   0\n",
       "366    1   1   1    1   1\n",
       "367    1   1   1    1   1\n",
       "368    1   1   1    1   1\n",
       "369    1   1   1    1   1\n",
       "370    1   1   0    1   1\n",
       "371    0   0   0    0   0\n",
       "372    0   0   0    0   0\n",
       "373    0   0   0    0   0\n",
       "374    0   0   1    0   0\n",
       "375    1   1   1    1   1\n",
       "376    1   1   1    1   1\n",
       "377    0   0   0    0   0\n",
       "378    0   0   0    0   0\n",
       "379    0   0   0    0   0\n",
       "380    1   1   1    1   1\n",
       "381    1   1   1    1   1\n",
       "382    0   0   0    0   0\n",
       "383    1   1   1    1   1\n",
       "384    0   0   0    0   0\n",
       "385    0   0   0    0   0\n",
       "386    0   0   0    0   0\n",
       "387    1   1   1    1   1\n",
       "388    0   0   0    0   0\n",
       "389    1   1   1    1   1\n",
       "390    0   1   0    0   1\n",
       "391    0   0   0    0   0\n",
       "392    0   0   0    0   0\n",
       "393    1   1   1    1   1\n",
       "394    0   1   1    1   1\n",
       "395    0   0   0    0   0\n",
       "396    0   0   1    0   0\n",
       "397    0   0   0    0   0\n",
       "398    0   0   0    0   0\n",
       "399    1   1   1    1   1\n",
       "400    0   1   0    1   1\n",
       "401    0   0   0    0   0\n",
       "402    0   0   1    0   0\n",
       "403    0   0   0    0   0\n",
       "404    0   0   1    0   0\n",
       "405    0   0   0    0   0\n",
       "406    0   0   0    0   0\n",
       "407    1   1   0    1   1\n",
       "408    0   0   0    0   0\n",
       "409    0   0   1    0   0\n",
       "410    0   0   0    0   0\n",
       "411    0   0   0    0   0\n",
       "412    1   1   1    1   1\n",
       "413    0   0   0    0   0\n",
       "414    0   1   0    1   1\n",
       "415    0   0   1    0   0\n",
       "416    1   1   1    1   1\n",
       "417    1   1   1    1   1\n",
       "418    0   0   0    0   0\n",
       "419    0   0   1    0   0\n",
       "420    0   0   0    0   0\n",
       "421    0   0   0    0   0\n",
       "422    0   0   0    0   0\n",
       "423    0   0   1    0   0\n",
       "424    0   0   0    0   0\n",
       "425    0   0   0    0   0\n",
       "426    1   1   1    1   1\n",
       "427    1   1   1    1   1\n",
       "428    0   0   0    0   0\n",
       "429    0   0   0    1   0\n",
       "430    0   0   0    0   0\n",
       "431    1   1   1    1   1\n",
       "432    1   1   1    1   1\n",
       "433    0   0   0    0   0\n",
       "434    1   0   0    0   0\n",
       "435    1   1   1    1   1\n",
       "436    0   0   1    0   0\n",
       "437    1   1   1    1   1\n",
       "438    0   0   0    0   0\n",
       "439    0   0   0    0   0\n",
       "440    1   1   1    1   1\n",
       "441    0   0   0    0   0\n",
       "442    0   0   0    0   0\n",
       "443    1   1   1    1   1\n",
       "444    0   1   0    0   0\n",
       "445    1   1   0    1   1\n",
       "446    1   1   1    1   1\n",
       "447    0   1   0    1   1\n",
       "448    1   1   1    1   1\n",
       "449    0   1   0    1   0\n",
       "450    0   0   0    0   0\n",
       "451    0   0   0    0   0\n",
       "452    0   0   0    0   0\n",
       "453    1   0   0    0   0\n",
       "454    0   0   0    0   0\n",
       "455    0   0   0    0   0\n",
       "456    0   0   0    0   0\n",
       "457    1   1   1    1   1\n",
       "458    1   1   1    1   1\n",
       "459    0   0   0    0   0\n",
       "460    1   1   0    1   1\n",
       "461    0   0   0    0   0\n",
       "462    0   0   0    0   0\n",
       "463    0   0   0    0   0\n",
       "464    0   0   0    0   0\n",
       "465    0   0   0    0   0\n",
       "466    0   0   0    0   0\n",
       "467    0   0   0    0   0\n",
       "468    0   0   0    0   0\n",
       "469    1   1   1    1   1\n",
       "470    0   0   0    0   0\n",
       "471    0   0   0    0   0\n",
       "472    1   1   1    1   1\n",
       "473    1   1   1    1   1\n",
       "474    0   0   1    0   0\n",
       "475    0   0   0    0   0\n",
       "476    0   0   0    0   0\n",
       "477    0   0   0    0   0\n",
       "478    0   0   0    0   0\n",
       "479    1   1   1    1   1\n",
       "480    0   0   0    0   0\n",
       "481    0   0   0    0   0\n",
       "482    0   0   0    0   0\n",
       "483    0   0   1    0   0\n",
       "484    1   1   0    1   1\n",
       "485    0   0   1    0   0\n",
       "486    1   1   1    1   1\n",
       "487    0   0   0    0   0\n",
       "488    0   0   0    0   0\n",
       "489    1   1   0    1   0\n",
       "490    0   0   0    0   0\n",
       "491    0   0   0    0   0\n",
       "492    0   0   0    0   0\n",
       "493    0   0   0    0   0\n",
       "494    0   0   0    0   0\n",
       "495    0   0   0    0   0\n",
       "496    1   1   1    1   1\n",
       "497    0   0   0    0   0\n",
       "498    1   1   1    1   1\n",
       "499    0   0   0    0   0\n",
       "500    0   0   0    0   0\n",
       "501    1   0   1    0   1\n",
       "502    1   1   1    1   1\n",
       "503    0   0   1    0   0\n",
       "504    1   1   1    1   1\n",
       "505    0   0   0    0   0\n",
       "506    1   1   1    1   1\n",
       "507    0   1   0    1   1\n",
       "508    0   0   0    0   0\n",
       "509    1   1   0    1   1\n",
       "510    0   0   0    0   0\n",
       "511    0   0   0    0   0\n",
       "512    0   1   0    1   1\n",
       "513    1   1   1    1   1\n",
       "514    0   0   0    0   0\n",
       "515    0   0   0    0   0\n",
       "516    1   1   1    1   1\n",
       "517    0   0   0    0   0\n",
       "518    1   1   1    1   1\n",
       "519    0   0   0    0   0\n",
       "520    1   1   1    1   1\n",
       "521    0   0   0    0   0\n",
       "522    0   0   0    0   0\n",
       "523    1   1   1    1   1\n",
       "524    0   0   0    0   0\n",
       "525    0   0   0    0   0\n",
       "526    1   1   1    1   1\n",
       "527    0   0   0    0   0\n",
       "528    0   0   0    0   0\n",
       "529    0   0   0    0   0\n",
       "530    1   1   1    1   1\n",
       "531    0   0   0    0   0\n",
       "532    0   0   0    0   0\n",
       "533    1   1   1    1   1\n",
       "534    0   0   1    0   0\n",
       "535    1   1   1    1   1\n",
       "536    0   0   0    0   0\n",
       "537    1   1   1    1   1\n",
       "538    0   0   0    0   0\n",
       "539    1   1   1    1   1\n",
       "540    1   1   1    1   1\n",
       "541    0   0   1    0   0\n",
       "542    0   0   1    0   0\n",
       "543    0   0   0    0   0\n",
       "544    0   0   0    0   0\n",
       "545    0   0   0    0   0\n",
       "546    1   1   1    1   1\n",
       "547    0   0   0    0   0\n",
       "548    0   0   0    0   0\n",
       "549    1   1   0    1   0\n",
       "550    1   0   0    1   1\n",
       "551    0   0   0    0   0\n",
       "552    0   0   0    0   0\n",
       "553    0   1   0    1   0\n",
       "554    1   1   1    1   1\n",
       "555    0   0   0    0   0\n",
       "556    1   1   1    1   1\n",
       "557    0   0   0    0   0\n",
       "558    1   1   1    1   1\n",
       "559    1   1   1    1   1\n",
       "560    0   0   0    0   0\n",
       "561    0   0   0    0   0\n",
       "562    0   0   0    0   0\n",
       "563    0   0   0    0   0\n",
       "564    0   0   1    0   0\n",
       "565    0   0   0    0   0\n",
       "566    0   0   0    0   0\n",
       "567    0   0   1    0   0\n",
       "568    0   0   0    0   0\n",
       "569    0   0   0    0   0\n",
       "570    0   0   0    0   0\n",
       "571    1   1   1    1   1\n",
       "572    0   1   0    1   1\n",
       "573    1   1   1    1   1\n",
       "574    0   0   0    0   0\n",
       "575    0   0   0    0   0\n",
       "576    1   1   1    1   1\n",
       "577    1   1   1    1   1\n",
       "578    0   0   1    0   0\n",
       "579    0   1   0    1   1\n",
       "580    1   1   1    1   1\n",
       "581    1   1   1    1   1\n",
       "582    0   0   0    0   0\n",
       "583    1   0   0    1   0\n",
       "584    0   0   0    0   0\n",
       "585    1   1   1    1   1\n",
       "586    0   0   0    0   0\n",
       "587    0   1   0    0   1\n",
       "588    0   0   0    0   0\n",
       "589    0   0   0    0   0\n",
       "590    0   0   0    0   0\n",
       "591    1   1   1    1   1\n",
       "592    0   0   0    0   0\n",
       "593    1   0   1    0   0\n",
       "594    0   0   0    0   0\n",
       "595    0   0   0    0   0\n",
       "596    1   1   1    1   1\n",
       "597    0   0   0    0   0\n",
       "598    0   0   0    0   0\n",
       "599    0   1   0    1   1\n",
       "600    1   1   1    1   1\n",
       "601    0   0   0    0   0\n",
       "602    0   0   0    0   0\n",
       "603    0   0   0    0   0\n",
       "604    0   1   0    1   1\n",
       "605    0   0   0    0   0\n",
       "606    0   0   0    0   0\n",
       "607    0   1   0    1   1\n",
       "608    1   1   1    1   1\n",
       "609    1   1   1    1   1\n",
       "610    0   0   1    0   0\n",
       "611    0   0   0    0   0\n",
       "612    1   1   1    1   1\n",
       "613    0   0   0    0   0\n",
       "614    0   0   0    0   0\n",
       "615    1   1   1    1   1\n",
       "616    0   0   0    0   0\n",
       "617    1   1   1    1   0\n",
       "618    1   1   1    1   1\n",
       "619    0   0   0    0   0\n",
       "620    0   0   0    0   0\n",
       "621    1   1   0    1   1\n",
       "622    0   1   0    1   1\n",
       "623    0   0   0    0   0\n",
       "624    0   0   0    0   0\n",
       "625    0   0   0    0   0\n",
       "626    0   0   0    0   0\n",
       "627    1   1   1    1   1\n",
       "628    0   0   0    0   0\n",
       "629    0   0   0    0   0\n",
       "630    0   0   0    0   0\n",
       "631    0   0   0    0   0\n",
       "632    1   1   0    1   1\n",
       "633    0   0   0    0   0\n",
       "634    0   0   1    0   0\n",
       "635    1   1   1    1   1\n",
       "636    0   0   0    0   0\n",
       "637    0   0   0    0   0\n",
       "638    0   0   1    0   0\n",
       "639    0   0   0    0   0\n",
       "640    0   0   0    0   0\n",
       "641    1   1   1    1   1\n",
       "642    0   0   1    0   0\n",
       "643    0   1   0    1   1\n",
       "644    1   1   1    1   1\n",
       "645    0   1   0    1   1\n",
       "646    0   0   0    0   0\n",
       "647    0   0   0    0   0\n",
       "648    0   0   0    0   0\n",
       "649    1   1   1    1   1\n",
       "650    0   0   0    0   0\n",
       "651    1   1   1    1   1\n",
       "652    0   0   0    0   0\n",
       "653    1   1   1    1   1\n",
       "654    1   0   1    0   0\n",
       "655    0   0   0    0   0\n",
       "656    0   0   0    0   0\n",
       "657    1   0   1    1   0\n",
       "658    0   0   0    0   0\n",
       "659    0   0   0    0   0\n",
       "660    0   0   0    0   0\n",
       "661    0   0   0    0   0\n",
       "662    0   0   0    0   0\n",
       "663    0   0   0    0   0\n",
       "664    0   1   0    1   1\n",
       "665    0   0   0    0   0\n",
       "666    0   0   0    0   0\n",
       "667    0   0   0    0   0\n",
       "668    0   0   0    0   0\n",
       "669    1   1   1    1   1\n",
       "670    1   1   1    1   1\n",
       "671    0   0   0    0   0\n",
       "672    0   0   0    0   0\n",
       "673    0   1   0    0   0\n",
       "674    0   0   0    0   0\n",
       "675    0   0   0    0   0\n",
       "676    0   0   0    0   0\n",
       "677    0   1   1    0   0\n",
       "678    0   0   1    0   0\n",
       "679    0   1   0    1   1\n",
       "680    0   0   1    0   0\n",
       "681    0   1   0    1   1\n",
       "682    0   0   0    0   0\n",
       "683    0   0   0    0   0\n",
       "684    0   0   0    0   0\n",
       "685    0   0   0    0   0\n",
       "686    0   0   0    0   0\n",
       "687    0   0   0    0   0\n",
       "688    0   0   0    0   0\n",
       "689    1   1   1    1   1\n",
       "690    1   1   0    1   1\n",
       "691    1   1   1    1   1\n",
       "692    0   1   0    1   1\n",
       "693    0   0   0    0   0\n",
       "694    0   0   0    0   0\n",
       "695    0   0   0    0   0\n",
       "696    0   0   0    0   0\n",
       "697    1   1   1    1   1\n",
       "698    1   0   0    0   0\n",
       "699    0   0   0    0   0\n",
       "700    1   1   1    1   1\n",
       "701    0   1   0    1   1\n",
       "702    0   0   1    0   0\n",
       "703    0   0   0    0   0\n",
       "704    0   0   0    0   0\n",
       "705    0   0   0    0   0\n",
       "706    1   1   1    1   1\n",
       "707    0   1   0    1   1\n",
       "708    1   1   1    1   1\n",
       "709    0   1   0    1   1\n",
       "710    1   1   1    1   1\n",
       "711    0   0   0    0   0\n",
       "712    0   1   0    1   1\n",
       "713    0   0   0    0   0\n",
       "714    0   0   0    0   0\n",
       "715    0   0   0    0   0\n",
       "716    1   1   1    1   1\n",
       "717    1   1   1    1   1\n",
       "718    0   0   0    0   0\n",
       "719    0   0   0    0   0\n",
       "720    1   1   1    1   1\n",
       "721    0   0   0    0   0\n",
       "722    0   0   0    0   0\n",
       "723    0   0   0    0   0\n",
       "724    1   1   0    1   1\n",
       "725    0   0   0    0   0\n",
       "726    1   1   1    1   1\n",
       "727    1   1   1    1   1\n",
       "728    0   0   0    0   0\n",
       "729    1   0   1    0   0\n",
       "730    1   1   1    1   1\n",
       "731    0   0   0    0   0\n",
       "732    0   0   0    0   0\n",
       "733    0   0   0    0   0\n",
       "734    0   0   0    0   0\n",
       "735    0   0   0    0   0\n",
       "736    0   0   1    0   0\n",
       "737    0   1   0    1   1\n",
       "738    0   0   0    0   0\n",
       "739    0   0   0    0   0\n",
       "740    1   1   0    1   1\n",
       "741    1   0   0    0   0\n",
       "742    1   1   1    1   1\n",
       "743    0   0   0    0   0\n",
       "744    0   1   0    1   1\n",
       "745    0   0   0    0   0\n",
       "746    0   0   0    0   0\n",
       "747    1   1   1    1   1\n",
       "748    0   0   0    0   0\n",
       "749    0   0   0    0   0\n",
       "750    1   1   1    1   1\n",
       "751    1   1   0    1   1\n",
       "752    0   0   0    0   0\n",
       "753    0   0   0    0   0\n",
       "754    1   1   1    1   1\n",
       "755    1   1   0    1   1\n",
       "756    0   0   0    0   0\n",
       "757    0   0   0    0   0\n",
       "758    0   0   0    0   0\n",
       "759    1   1   1    1   1\n",
       "760    0   0   0    0   0\n",
       "761    0   0   0    0   0\n",
       "762    0   1   0    1   1\n",
       "763    1   1   1    1   1\n",
       "764    0   0   0    0   0\n",
       "765    1   1   1    1   1\n",
       "766    0   0   0    0   0\n",
       "767    1   0   1    0   0\n",
       "768    0   0   0    0   0\n",
       "769    0   0   0    0   0\n",
       "770    0   0   0    0   0\n",
       "771    0   0   0    0   0\n",
       "772    1   0   1    1   0\n",
       "773    0   0   0    0   0\n",
       "774    1   1   1    1   1\n",
       "775    0   0   0    0   0\n",
       "776    0   0   0    0   0\n",
       "777    1   1   1    1   1\n",
       "778    0   0   0    0   0\n",
       "779    1   1   1    1   1\n",
       "780    1   1   1    1   1\n",
       "781    1   1   1    1   1\n",
       "782    0   0   0    0   0\n",
       "783    0   0   0    0   0\n",
       "784    0   0   0    0   0\n",
       "785    0   0   0    0   0\n",
       "786    1   1   1    1   1\n",
       "787    0   0   0    0   0\n",
       "788    1   1   0    1   1\n",
       "789    0   0   0    0   0\n",
       "790    0   0   0    0   0\n",
       "791    0   0   0    0   0\n",
       "792    0   0   1    0   0\n",
       "793    0   0   0    0   0\n",
       "794    0   0   0    0   0\n",
       "795    0   0   0    0   0\n",
       "796    1   1   1    1   1\n",
       "797    0   0   1    1   0\n",
       "798    0   0   0    0   0\n",
       "799    0   0   1    0   0\n",
       "800    0   0   0    0   0\n",
       "801    1   1   1    1   1\n",
       "802    1   0   0    0   1\n",
       "803    1   1   0    1   1\n",
       "804    0   1   0    0   0\n",
       "805    0   0   0    0   0\n",
       "806    0   0   0    0   0\n",
       "807    1   0   1    0   0\n",
       "808    0   0   0    0   0\n",
       "809    1   1   1    1   1\n",
       "810    0   0   0    0   0\n",
       "811    0   0   0    0   0\n",
       "812    0   0   0    0   0\n",
       "813    0   0   1    0   0\n",
       "814    0   0   0    0   0\n",
       "815    0   0   0    0   0\n",
       "816    0   0   1    0   0\n",
       "817    0   0   0    0   0\n",
       "818    0   0   0    0   0\n",
       "819    0   0   0    0   0\n",
       "820    1   1   1    1   1\n",
       "821    0   0   0    0   0\n",
       "822    0   0   0    0   0\n",
       "823    1   1   1    1   1\n",
       "824    0   0   0    0   0\n",
       "825    0   0   0    0   0\n",
       "826    0   1   0    1   1\n",
       "827    1   1   0    1   1\n",
       "828    0   0   0    0   0\n",
       "829    1   1   1    1   1\n",
       "830    1   1   1    1   1\n",
       "831    1   1   0    1   1\n",
       "832    0   0   0    0   0\n",
       "833    0   0   0    0   0\n",
       "834    0   0   0    0   0\n",
       "835    1   1   1    1   1\n",
       "836    0   0   0    0   0\n",
       "837    0   0   0    0   0\n",
       "838    1   1   0    1   1\n",
       "839    1   1   0    1   1\n",
       "840    0   0   0    0   0\n",
       "841    0   0   0    0   0\n",
       "842    1   1   1    1   1\n",
       "843    0   0   0    0   0\n",
       "844    0   0   0    0   0\n",
       "845    0   0   0    0   0\n",
       "846    0   0   0    0   0\n",
       "847    0   0   0    0   0\n",
       "848    0   0   0    0   0\n",
       "849    1   1   1    1   1\n",
       "850    0   0   0    0   0\n",
       "851    0   0   0    0   0\n",
       "852    0   0   1    0   0\n",
       "853    1   1   1    1   1\n",
       "854    1   1   1    1   1\n",
       "855    0   1   1    0   1\n",
       "856    1   1   1    1   1\n",
       "857    0   1   0    0   0\n",
       "858    1   1   1    1   1\n",
       "859    0   0   0    0   0\n",
       "860    0   0   0    0   0\n",
       "861    0   0   0    0   0\n",
       "862    1   1   1    1   1\n",
       "863    0   0   1    0   0\n",
       "864    0   0   0    0   0\n",
       "865    1   1   1    1   1\n",
       "866    1   1   1    1   1\n",
       "867    0   0   0    0   0\n",
       "868    0   0   0    0   0\n",
       "869    1   1   0    1   1\n",
       "870    0   0   0    0   0\n",
       "871    1   1   1    1   1\n",
       "872    0   0   0    0   0\n",
       "873    0   0   0    0   0\n",
       "874    1   1   1    1   1\n",
       "875    1   1   1    1   1\n",
       "876    0   0   0    0   0\n",
       "877    0   0   0    0   0\n",
       "878    0   0   0    0   0\n",
       "879    1   1   1    1   1\n",
       "880    1   1   1    1   1\n",
       "881    0   0   0    0   0\n",
       "882    0   0   1    0   0\n",
       "883    0   0   0    0   0\n",
       "884    0   0   0    0   0\n",
       "885    0   0   1    0   0\n",
       "886    0   0   0    0   0\n",
       "887    1   1   1    1   1\n",
       "888    0   0   1    0   0\n",
       "889    1   1   0    1   1\n",
       "890    0   0   0    0   0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dd4f3c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4\n",
       "0    0  0  0  0  0\n",
       "1    0  0  1  0  0\n",
       "2    0  0  0  0  0\n",
       "3    0  0  0  0  0\n",
       "4    1  1  1  1  1\n",
       "5    0  0  0  0  0\n",
       "6    1  1  1  1  1\n",
       "7    0  1  0  1  0\n",
       "8    1  1  1  1  1\n",
       "9    0  0  0  0  0\n",
       "10   0  0  0  0  0\n",
       "11   0  0  0  0  0\n",
       "12   1  1  1  1  1\n",
       "13   0  0  0  0  0\n",
       "14   1  1  1  1  1\n",
       "15   1  1  1  1  1\n",
       "16   0  0  0  0  0\n",
       "17   0  0  0  0  0\n",
       "18   0  0  1  1  1\n",
       "19   1  1  1  1  1\n",
       "20   0  0  0  0  0\n",
       "21   1  1  0  1  0\n",
       "22   1  1  1  1  1\n",
       "23   0  0  0  0  0\n",
       "24   1  1  1  1  1\n",
       "25   0  0  0  0  0\n",
       "26   1  1  1  1  1\n",
       "27   0  0  0  0  0\n",
       "28   0  0  0  1  0\n",
       "29   0  0  0  0  0\n",
       "30   0  0  0  0  0\n",
       "31   0  0  0  0  0\n",
       "32   0  1  1  1  0\n",
       "33   0  0  1  0  0\n",
       "34   0  0  0  0  0\n",
       "35   0  0  0  0  0\n",
       "36   1  1  1  1  1\n",
       "37   1  1  1  1  1\n",
       "38   0  0  0  0  0\n",
       "39   0  0  0  0  0\n",
       "40   0  0  0  0  0\n",
       "41   0  0  0  1  0\n",
       "42   0  0  0  0  0\n",
       "43   1  1  1  1  1\n",
       "44   1  1  1  1  1\n",
       "45   0  0  0  0  0\n",
       "46   0  0  0  0  0\n",
       "47   0  0  0  0  0\n",
       "48   1  1  1  1  1\n",
       "49   1  1  1  1  1\n",
       "50   1  0  0  0  0\n",
       "51   0  0  0  0  0\n",
       "52   1  1  1  1  1\n",
       "53   1  1  1  1  1\n",
       "54   0  0  0  0  0\n",
       "55   0  0  0  0  0\n",
       "56   0  0  0  0  0\n",
       "57   0  0  0  1  0\n",
       "58   0  0  0  0  0\n",
       "59   1  1  1  1  1\n",
       "60   0  0  0  0  0\n",
       "61   0  0  0  0  0\n",
       "62   0  0  0  0  0\n",
       "63   1  1  1  1  1\n",
       "64   0  1  0  1  0\n",
       "65   1  1  1  1  1\n",
       "66   1  1  1  1  1\n",
       "67   0  0  0  0  0\n",
       "68   1  1  0  1  1\n",
       "69   1  1  1  1  1\n",
       "70   1  1  1  1  1\n",
       "71   0  0  0  0  0\n",
       "72   1  1  1  1  1\n",
       "73   0  1  0  0  1\n",
       "74   1  1  1  1  1\n",
       "75   0  0  0  0  0\n",
       "76   0  0  0  0  0\n",
       "77   1  1  1  1  1\n",
       "78   0  0  0  0  0\n",
       "79   1  1  1  1  1\n",
       "80   1  1  0  1  1\n",
       "81   0  0  0  0  0\n",
       "82   0  0  0  0  0\n",
       "83   0  0  0  0  0\n",
       "84   0  0  0  0  0\n",
       "85   0  0  0  0  0\n",
       "86   1  1  1  1  1\n",
       "87   0  0  1  0  0\n",
       "88   1  1  1  1  1\n",
       "89   1  1  0  1  1\n",
       "90   0  0  1  1  0\n",
       "91   0  0  0  0  0\n",
       "92   1  1  1  1  1\n",
       "93   0  0  0  0  0\n",
       "94   0  0  0  0  0\n",
       "95   0  0  0  0  0\n",
       "96   1  1  1  1  1\n",
       "97   0  0  0  0  0\n",
       "98   1  1  1  1  1\n",
       "99   0  0  0  0  0\n",
       "100  1  1  1  1  1\n",
       "101  0  0  0  0  0\n",
       "102  0  0  0  0  0\n",
       "103  0  0  0  0  0\n",
       "104  1  1  1  1  1\n",
       "105  0  0  0  0  0\n",
       "106  0  0  0  0  0\n",
       "107  0  0  0  0  0\n",
       "108  0  0  0  0  0\n",
       "109  0  0  0  1  0\n",
       "110  0  0  0  0  0\n",
       "111  1  1  1  1  1\n",
       "112  1  1  1  1  1\n",
       "113  1  1  1  1  1\n",
       "114  1  1  1  1  1\n",
       "115  0  0  0  0  0\n",
       "116  0  0  0  0  0\n",
       "117  1  1  1  0  1\n",
       "118  0  0  0  0  0\n",
       "119  1  1  1  1  1\n",
       "120  1  1  1  1  1\n",
       "121  0  0  0  0  0\n",
       "122  1  1  1  1  1\n",
       "123  0  0  0  0  0\n",
       "124  0  0  0  0  0\n",
       "125  1  1  1  1  1\n",
       "126  0  0  0  0  0\n",
       "127  1  1  1  1  1\n",
       "128  0  0  0  0  0\n",
       "129  0  0  0  0  0\n",
       "130  0  0  0  0  0\n",
       "131  0  0  0  0  0\n",
       "132  0  0  1  0  0\n",
       "133  0  0  0  0  0\n",
       "134  0  0  0  0  0\n",
       "135  0  0  0  0  0\n",
       "136  0  0  0  0  0\n",
       "137  0  0  0  0  0\n",
       "138  0  0  1  0  0\n",
       "139  0  0  0  0  0\n",
       "140  0  0  1  0  0\n",
       "141  1  1  1  1  1\n",
       "142  0  0  0  0  0\n",
       "143  0  0  0  0  0\n",
       "144  0  0  0  0  0\n",
       "145  0  0  0  0  0\n",
       "146  1  0  0  1  0\n",
       "147  0  0  0  0  0\n",
       "148  0  0  0  1  0\n",
       "149  0  0  0  1  0\n",
       "150  1  1  1  1  1\n",
       "151  0  0  0  0  0\n",
       "152  0  0  0  0  0\n",
       "153  0  0  1  0  0\n",
       "154  0  0  0  0  0\n",
       "155  0  0  0  0  0\n",
       "156  1  1  1  1  1\n",
       "157  1  1  1  1  1\n",
       "158  0  0  0  1  0\n",
       "159  0  1  1  1  1\n",
       "160  1  1  1  1  1\n",
       "161  1  1  0  1  1\n",
       "162  1  1  1  1  1\n",
       "163  0  0  0  0  0\n",
       "164  0  0  0  0  0\n",
       "165  1  1  1  1  1\n",
       "166  0  0  0  0  0\n",
       "167  0  0  0  0  0\n",
       "168  1  1  1  1  1\n",
       "169  0  0  1  0  0\n",
       "170  0  0  0  0  0\n",
       "171  0  0  0  0  0\n",
       "172  0  0  0  0  0\n",
       "173  0  0  0  0  0\n",
       "174  0  0  0  0  0\n",
       "175  1  1  1  1  1\n",
       "176  1  1  1  1  1\n",
       "177  0  0  0  0  0\n",
       "178  1  1  1  1  1\n",
       "179  1  1  1  1  1\n",
       "180  0  0  0  0  0\n",
       "181  0  0  0  0  0\n",
       "182  1  1  1  1  1\n",
       "183  0  0  0  0  0\n",
       "184  1  1  1  1  1\n",
       "185  0  0  0  0  0\n",
       "186  1  1  1  1  1\n",
       "187  0  0  0  0  0\n",
       "188  0  0  1  0  0\n",
       "189  0  0  0  0  0\n",
       "190  0  0  0  0  0\n",
       "191  0  1  0  1  1\n",
       "192  0  0  0  0  0\n",
       "193  0  0  0  0  0\n",
       "194  1  1  0  1  1\n",
       "195  0  0  0  0  0\n",
       "196  1  1  0  1  1\n",
       "197  1  1  1  1  1\n",
       "198  0  0  0  0  0\n",
       "199  0  0  1  0  0\n",
       "200  1  0  1  1  1\n",
       "201  1  1  0  1  1\n",
       "202  0  0  0  0  0\n",
       "203  1  1  1  1  1\n",
       "204  0  0  0  0  0\n",
       "205  0  0  0  0  0\n",
       "206  1  1  1  1  1\n",
       "207  0  0  0  0  0\n",
       "208  1  1  1  1  1\n",
       "209  0  0  0  0  0\n",
       "210  0  0  0  0  0\n",
       "211  0  0  0  0  0\n",
       "212  0  0  0  0  0\n",
       "213  1  1  1  1  1\n",
       "214  1  1  1  1  1\n",
       "215  0  0  0  0  0\n",
       "216  1  1  1  1  1\n",
       "217  0  0  0  0  0\n",
       "218  1  1  1  1  1\n",
       "219  0  0  0  0  0\n",
       "220  1  1  1  1  1\n",
       "221  0  0  0  0  0\n",
       "222  1  1  1  1  1\n",
       "223  0  0  0  0  0\n",
       "224  1  1  1  1  1\n",
       "225  1  1  1  1  1\n",
       "226  0  0  0  0  0\n",
       "227  1  1  1  1  1\n",
       "228  0  0  0  0  0\n",
       "229  0  0  0  0  0\n",
       "230  0  1  0  0  1\n",
       "231  1  1  1  1  1\n",
       "232  0  0  0  0  0\n",
       "233  0  0  0  0  0\n",
       "234  0  0  0  0  0\n",
       "235  0  0  0  0  0\n",
       "236  0  0  0  0  0\n",
       "237  0  0  0  0  0\n",
       "238  1  1  1  1  1\n",
       "239  1  1  1  1  1\n",
       "240  1  1  1  1  1\n",
       "241  1  1  1  1  1\n",
       "242  1  0  0  0  0\n",
       "243  0  0  0  0  0\n",
       "244  0  0  0  0  0\n",
       "245  1  0  0  1  0\n",
       "246  1  1  1  1  1\n",
       "247  0  0  0  0  0\n",
       "248  1  1  1  1  1\n",
       "249  1  0  1  0  1\n",
       "250  1  1  1  1  1\n",
       "251  0  0  0  0  0\n",
       "252  0  0  0  0  0\n",
       "253  0  0  0  0  0\n",
       "254  0  0  0  0  0\n",
       "255  0  0  0  0  0\n",
       "256  0  0  0  0  0\n",
       "257  0  0  0  0  0\n",
       "258  1  1  1  1  1\n",
       "259  0  0  0  0  0\n",
       "260  0  0  0  0  0\n",
       "261  0  0  0  0  0\n",
       "262  1  1  1  1  1\n",
       "263  1  1  1  1  1\n",
       "264  0  0  0  0  0\n",
       "265  0  0  0  0  0\n",
       "266  0  0  0  0  1\n",
       "267  0  0  0  0  0\n",
       "268  0  0  1  0  0\n",
       "269  0  0  0  0  0\n",
       "270  0  0  0  0  0\n",
       "271  0  0  0  0  0\n",
       "272  1  1  1  1  1\n",
       "273  1  0  1  1  1\n",
       "274  0  0  0  0  0\n",
       "275  1  1  1  1  1\n",
       "276  0  0  0  0  0\n",
       "277  0  0  0  0  0\n",
       "278  0  0  0  0  0\n",
       "279  0  0  0  0  0\n",
       "280  1  1  1  1  1\n",
       "281  1  1  0  1  1\n",
       "282  1  1  1  1  1\n",
       "283  1  1  1  1  1\n",
       "284  1  1  1  1  1\n",
       "285  0  0  0  0  0\n",
       "286  0  0  0  0  0\n",
       "287  1  1  0  1  1\n",
       "288  0  0  0  0  0\n",
       "289  0  0  0  0  0\n",
       "290  0  0  0  0  0\n",
       "291  0  1  1  1  1\n",
       "292  0  0  0  0  0\n",
       "293  0  0  0  1  0\n",
       "294  0  0  0  0  0\n",
       "295  0  0  0  0  0\n",
       "296  1  1  1  1  1\n",
       "297  0  0  0  0  0\n",
       "298  0  0  0  0  0\n",
       "299  0  0  0  0  0\n",
       "300  0  0  0  0  0\n",
       "301  1  1  0  1  1\n",
       "302  0  0  0  0  0\n",
       "303  0  0  0  0  0\n",
       "304  1  1  1  1  1\n",
       "305  1  1  1  1  1\n",
       "306  0  0  0  0  0\n",
       "307  1  1  0  1  1\n",
       "308  0  0  0  0  0\n",
       "309  0  0  1  0  0\n",
       "310  0  0  0  0  0\n",
       "311  0  0  0  0  0\n",
       "312  0  0  0  0  0\n",
       "313  1  0  1  1  1\n",
       "314  1  1  1  1  1\n",
       "315  1  1  1  1  1\n",
       "316  0  0  0  0  0\n",
       "317  0  0  0  0  0\n",
       "318  0  0  0  0  0\n",
       "319  0  0  0  0  0\n",
       "320  0  0  0  0  0\n",
       "321  1  0  0  1  0\n",
       "322  0  0  0  1  0\n",
       "323  0  0  0  1  0\n",
       "324  1  1  1  1  1\n",
       "325  0  0  0  0  0\n",
       "326  1  1  1  1  1\n",
       "327  0  0  0  0  0\n",
       "328  0  0  0  0  0\n",
       "329  0  0  0  0  0\n",
       "330  1  1  1  1  1\n",
       "331  0  0  0  0  0\n",
       "332  0  0  0  0  0\n",
       "333  1  1  1  1  1\n",
       "334  0  0  0  0  0\n",
       "335  0  0  0  0  0\n",
       "336  0  0  0  0  0\n",
       "337  0  0  0  0  0\n",
       "338  0  0  0  0  0\n",
       "339  0  0  0  0  0\n",
       "340  0  0  0  0  0\n",
       "341  0  0  0  0  0\n",
       "342  0  0  0  0  0\n",
       "343  1  1  1  1  1\n",
       "344  0  0  0  0  0\n",
       "345  1  1  1  1  1\n",
       "346  0  0  0  0  0\n",
       "347  1  1  1  1  1\n",
       "348  0  0  0  0  0\n",
       "349  1  1  1  1  1\n",
       "350  1  1  1  1  1\n",
       "351  0  0  0  0  0\n",
       "352  0  0  0  0  0\n",
       "353  0  0  0  0  0\n",
       "354  1  1  1  1  1\n",
       "355  0  0  0  0  0\n",
       "356  1  1  1  1  1\n",
       "357  0  0  0  0  0\n",
       "358  0  0  0  0  0\n",
       "359  0  0  1  1  0\n",
       "360  0  0  0  0  0\n",
       "361  1  1  1  1  1\n",
       "362  1  1  1  1  1\n",
       "363  0  0  0  0  0\n",
       "364  1  1  1  1  1\n",
       "365  0  0  1  0  0\n",
       "366  0  0  0  0  0\n",
       "367  0  0  1  1  0\n",
       "368  1  1  1  1  1\n",
       "369  0  0  0  0  0\n",
       "370  0  0  0  0  0\n",
       "371  1  1  1  1  1\n",
       "372  0  0  0  0  0\n",
       "373  0  0  0  0  0\n",
       "374  1  1  1  1  1\n",
       "375  1  1  1  1  1\n",
       "376  1  0  1  1  1\n",
       "377  0  0  0  0  0\n",
       "378  0  0  0  0  0\n",
       "379  0  0  0  0  0\n",
       "380  0  0  0  0  0\n",
       "381  0  0  0  0  0\n",
       "382  0  0  1  0  0\n",
       "383  1  0  1  1  0\n",
       "384  0  0  0  0  0\n",
       "385  1  1  1  1  1\n",
       "386  0  0  0  0  0\n",
       "387  0  0  0  0  0\n",
       "388  0  0  0  0  0\n",
       "389  0  0  0  0  0\n",
       "390  0  0  0  0  0\n",
       "391  1  1  1  1  1\n",
       "392  0  0  0  0  0\n",
       "393  0  0  0  0  0\n",
       "394  0  0  0  0  0\n",
       "395  1  1  1  1  1\n",
       "396  0  0  0  0  0\n",
       "397  1  1  1  1  1\n",
       "398  0  0  0  0  0\n",
       "399  0  0  0  0  0\n",
       "400  1  1  1  1  1\n",
       "401  0  0  0  0  0\n",
       "402  1  1  1  1  1\n",
       "403  0  1  0  0  1\n",
       "404  0  1  0  1  0\n",
       "405  0  0  0  0  0\n",
       "406  0  0  0  0  0\n",
       "407  0  0  0  0  0\n",
       "408  1  1  1  1  1\n",
       "409  1  1  1  1  1\n",
       "410  1  1  1  1  1\n",
       "411  1  1  1  1  1\n",
       "412  1  1  1  1  1\n",
       "413  0  0  0  0  0\n",
       "414  1  1  1  1  1\n",
       "415  0  0  0  0  0\n",
       "416  0  0  0  0  0\n",
       "417  0  0  0  0  0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e6c9f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rft = RandomForestClassifier()\n",
    "rft.fit(stack1, y)\n",
    "pred_stack_rft = rft.predict(stack_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "85689b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418,)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0b7c66c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 12 for this parallel run (total 220)...\n",
      "Building estimator 2 of 12 for this parallel run (total 220)...\n",
      "Building estimator 3 of 12 for this parallel run (total 220)...\n",
      "Building estimator 4 of 12 for this parallel run (total 220)...\n",
      "Building estimator 5 of 12 for this parallel run (total 220)...\n",
      "Building estimator 6 of 12 for this parallel run (total 220)...\n",
      "Building estimator 7 of 12 for this parallel run (total 220)...\n",
      "Building estimator 8 of 12 for this parallel run (total 220)...\n",
      "Building estimator 9 of 12 for this parallel run (total 220)...\n",
      "Building estimator 10 of 12 for this parallel run (total 220)...\n",
      "Building estimator 11 of 12 for this parallel run (total 220)...\n",
      "Building estimator 12 of 12 for this parallel run (total 220)...\n",
      "Building estimator 1 of 12 for this parallel run (total 220)...\n",
      "Building estimator 2 of 12 for this parallel run (total 220)...\n",
      "Building estimator 3 of 12 for this parallel run (total 220)...\n",
      "Building estimator 4 of 12 for this parallel run (total 220)...\n",
      "Building estimator 5 of 12 for this parallel run (total 220)...\n",
      "Building estimator 6 of 12 for this parallel run (total 220)...\n",
      "Building estimator 7 of 12 for this parallel run (total 220)...\n",
      "Building estimator 8 of 12 for this parallel run (total 220)...\n",
      "Building estimator 9 of 12 for this parallel run (total 220)...\n",
      "Building estimator 10 of 12 for this parallel run (total 220)...\n",
      "Building estimator 11 of 12 for this parallel run (total 220)...\n",
      "Building estimator 12 of 12 for this parallel run (total 220)...\n",
      "Building estimator 1 of 12 for this parallel run (total 220)...\n",
      "Building estimator 2 of 12 for this parallel run (total 220)...\n",
      "Building estimator 3 of 12 for this parallel run (total 220)...\n",
      "Building estimator 4 of 12 for this parallel run (total 220)...\n",
      "Building estimator 5 of 12 for this parallel run (total 220)...\n",
      "Building estimator 6 of 12 for this parallel run (total 220)...\n",
      "Building estimator 7 of 12 for this parallel run (total 220)...\n",
      "Building estimator 8 of 12 for this parallel run (total 220)...\n",
      "Building estimator 9 of 12 for this parallel run (total 220)...\n",
      "Building estimator 10 of 12 for this parallel run (total 220)...\n",
      "Building estimator 11 of 12 for this parallel run (total 220)...\n",
      "Building estimator 12 of 12 for this parallel run (total 220)...\n",
      "Building estimator 1 of 12 for this parallel run (total 220)...\n",
      "Building estimator 2 of 12 for this parallel run (total 220)...\n",
      "Building estimator 3 of 12 for this parallel run (total 220)...\n",
      "Building estimator 4 of 12 for this parallel run (total 220)...\n",
      "Building estimator 5 of 12 for this parallel run (total 220)...\n",
      "Building estimator 6 of 12 for this parallel run (total 220)...\n",
      "Building estimator 7 of 12 for this parallel run (total 220)...\n",
      "Building estimator 8 of 12 for this parallel run (total 220)...\n",
      "Building estimator 9 of 12 for this parallel run (total 220)...\n",
      "Building estimator 10 of 12 for this parallel run (total 220)...\n",
      "Building estimator 11 of 12 for this parallel run (total 220)...\n",
      "Building estimator 12 of 12 for this parallel run (total 220)...\n",
      "Building estimator 1 of 12 for this parallel run (total 220)...\n",
      "Building estimator 2 of 12 for this parallel run (total 220)...\n",
      "Building estimator 3 of 12 for this parallel run (total 220)...\n",
      "Building estimator 4 of 12 for this parallel run (total 220)...\n",
      "Building estimator 5 of 12 for this parallel run (total 220)...\n",
      "Building estimator 6 of 12 for this parallel run (total 220)...\n",
      "Building estimator 7 of 12 for this parallel run (total 220)...\n",
      "Building estimator 8 of 12 for this parallel run (total 220)...\n",
      "Building estimator 9 of 12 for this parallel run (total 220)...\n",
      "Building estimator 10 of 12 for this parallel run (total 220)...\n",
      "Building estimator 11 of 12 for this parallel run (total 220)...\n",
      "Building estimator 12 of 12 for this parallel run (total 220)...\n",
      "Building estimator 1 of 12 for this parallel run (total 220)...\n",
      "Building estimator 2 of 12 for this parallel run (total 220)...\n",
      "Building estimator 3 of 12 for this parallel run (total 220)...\n",
      "Building estimator 4 of 12 for this parallel run (total 220)...\n",
      "Building estimator 5 of 12 for this parallel run (total 220)...\n",
      "Building estimator 6 of 12 for this parallel run (total 220)...\n",
      "Building estimator 7 of 12 for this parallel run (total 220)...\n",
      "Building estimator 8 of 12 for this parallel run (total 220)...\n",
      "Building estimator 9 of 12 for this parallel run (total 220)...\n",
      "Building estimator 10 of 12 for this parallel run (total 220)...\n",
      "Building estimator 11 of 12 for this parallel run (total 220)...\n",
      "Building estimator 12 of 12 for this parallel run (total 220)...\n",
      "Building estimator 1 of 12 for this parallel run (total 220)...\n",
      "Building estimator 2 of 12 for this parallel run (total 220)...\n",
      "Building estimator 3 of 12 for this parallel run (total 220)...\n",
      "Building estimator 4 of 12 for this parallel run (total 220)...\n",
      "Building estimator 5 of 12 for this parallel run (total 220)...\n",
      "Building estimator 6 of 12 for this parallel run (total 220)...\n",
      "Building estimator 7 of 12 for this parallel run (total 220)...\n",
      "Building estimator 8 of 12 for this parallel run (total 220)...\n",
      "Building estimator 9 of 12 for this parallel run (total 220)...\n",
      "Building estimator 10 of 12 for this parallel run (total 220)...\n",
      "Building estimator 11 of 12 for this parallel run (total 220)...\n",
      "Building estimator 12 of 12 for this parallel run (total 220)...\n",
      "Building estimator 1 of 12 for this parallel run (total 220)...\n",
      "Building estimator 2 of 12 for this parallel run (total 220)...\n",
      "Building estimator 3 of 12 for this parallel run (total 220)...\n",
      "Building estimator 4 of 12 for this parallel run (total 220)...\n",
      "Building estimator 5 of 12 for this parallel run (total 220)...\n",
      "Building estimator 6 of 12 for this parallel run (total 220)...\n",
      "Building estimator 7 of 12 for this parallel run (total 220)...\n",
      "Building estimator 8 of 12 for this parallel run (total 220)...\n",
      "Building estimator 9 of 12 for this parallel run (total 220)...\n",
      "Building estimator 10 of 12 for this parallel run (total 220)...\n",
      "Building estimator 11 of 12 for this parallel run (total 220)...\n",
      "Building estimator 12 of 12 for this parallel run (total 220)...\n",
      "Building estimator 1 of 12 for this parallel run (total 220)...\n",
      "Building estimator 2 of 12 for this parallel run (total 220)...\n",
      "Building estimator 3 of 12 for this parallel run (total 220)...\n",
      "Building estimator 4 of 12 for this parallel run (total 220)...\n",
      "Building estimator 5 of 12 for this parallel run (total 220)...\n",
      "Building estimator 6 of 12 for this parallel run (total 220)...\n",
      "Building estimator 7 of 12 for this parallel run (total 220)...\n",
      "Building estimator 8 of 12 for this parallel run (total 220)...\n",
      "Building estimator 9 of 12 for this parallel run (total 220)...\n",
      "Building estimator 10 of 12 for this parallel run (total 220)...\n",
      "Building estimator 11 of 12 for this parallel run (total 220)...\n",
      "Building estimator 12 of 12 for this parallel run (total 220)...\n",
      "Building estimator 1 of 11 for this parallel run (total 220)...\n",
      "Building estimator 2 of 11 for this parallel run (total 220)...\n",
      "Building estimator 3 of 11 for this parallel run (total 220)...\n",
      "Building estimator 4 of 11 for this parallel run (total 220)...\n",
      "Building estimator 5 of 11 for this parallel run (total 220)...\n",
      "Building estimator 6 of 11 for this parallel run (total 220)...\n",
      "Building estimator 7 of 11 for this parallel run (total 220)...\n",
      "Building estimator 8 of 11 for this parallel run (total 220)...\n",
      "Building estimator 9 of 11 for this parallel run (total 220)...\n",
      "Building estimator 10 of 11 for this parallel run (total 220)...\n",
      "Building estimator 11 of 11 for this parallel run (total 220)...\n",
      "Building estimator 1 of 12 for this parallel run (total 220)...\n",
      "Building estimator 2 of 12 for this parallel run (total 220)...\n",
      "Building estimator 3 of 12 for this parallel run (total 220)...\n",
      "Building estimator 4 of 12 for this parallel run (total 220)...\n",
      "Building estimator 5 of 12 for this parallel run (total 220)...\n",
      "Building estimator 6 of 12 for this parallel run (total 220)...\n",
      "Building estimator 7 of 12 for this parallel run (total 220)...\n",
      "Building estimator 8 of 12 for this parallel run (total 220)...\n",
      "Building estimator 9 of 12 for this parallel run (total 220)...\n",
      "Building estimator 10 of 12 for this parallel run (total 220)...\n",
      "Building estimator 11 of 12 for this parallel run (total 220)...\n",
      "Building estimator 12 of 12 for this parallel run (total 220)...\n",
      "Building estimator 1 of 11 for this parallel run (total 220)...\n",
      "Building estimator 2 of 11 for this parallel run (total 220)...\n",
      "Building estimator 3 of 11 for this parallel run (total 220)...\n",
      "Building estimator 4 of 11 for this parallel run (total 220)...\n",
      "Building estimator 5 of 11 for this parallel run (total 220)...\n",
      "Building estimator 6 of 11 for this parallel run (total 220)...\n",
      "Building estimator 7 of 11 for this parallel run (total 220)...\n",
      "Building estimator 8 of 11 for this parallel run (total 220)...\n",
      "Building estimator 9 of 11 for this parallel run (total 220)...\n",
      "Building estimator 10 of 11 for this parallel run (total 220)...\n",
      "Building estimator 11 of 11 for this parallel run (total 220)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 12 for this parallel run (total 220)...\n",
      "Building estimator 2 of 12 for this parallel run (total 220)...\n",
      "Building estimator 3 of 12 for this parallel run (total 220)...\n",
      "Building estimator 4 of 12 for this parallel run (total 220)...\n",
      "Building estimator 5 of 12 for this parallel run (total 220)...\n",
      "Building estimator 6 of 12 for this parallel run (total 220)...\n",
      "Building estimator 7 of 12 for this parallel run (total 220)...\n",
      "Building estimator 8 of 12 for this parallel run (total 220)...\n",
      "Building estimator 9 of 12 for this parallel run (total 220)...\n",
      "Building estimator 10 of 12 for this parallel run (total 220)...\n",
      "Building estimator 11 of 12 for this parallel run (total 220)...\n",
      "Building estimator 12 of 12 for this parallel run (total 220)...\n",
      "Building estimator 1 of 11 for this parallel run (total 220)...\n",
      "Building estimator 2 of 11 for this parallel run (total 220)...\n",
      "Building estimator 3 of 11 for this parallel run (total 220)...\n",
      "Building estimator 4 of 11 for this parallel run (total 220)...\n",
      "Building estimator 5 of 11 for this parallel run (total 220)...\n",
      "Building estimator 6 of 11 for this parallel run (total 220)...\n",
      "Building estimator 7 of 11 for this parallel run (total 220)...\n",
      "Building estimator 8 of 11 for this parallel run (total 220)...\n",
      "Building estimator 9 of 11 for this parallel run (total 220)...\n",
      "Building estimator 10 of 11 for this parallel run (total 220)...\n",
      "Building estimator 11 of 11 for this parallel run (total 220)...\n",
      "Building estimator 1 of 11 for this parallel run (total 220)...\n",
      "Building estimator 2 of 11 for this parallel run (total 220)...\n",
      "Building estimator 3 of 11 for this parallel run (total 220)...\n",
      "Building estimator 4 of 11 for this parallel run (total 220)...\n",
      "Building estimator 5 of 11 for this parallel run (total 220)...\n",
      "Building estimator 6 of 11 for this parallel run (total 220)...\n",
      "Building estimator 7 of 11 for this parallel run (total 220)...\n",
      "Building estimator 8 of 11 for this parallel run (total 220)...\n",
      "Building estimator 9 of 11 for this parallel run (total 220)...\n",
      "Building estimator 10 of 11 for this parallel run (total 220)...\n",
      "Building estimator 11 of 11 for this parallel run (total 220)...\n",
      "Building estimator 1 of 11 for this parallel run (total 220)...\n",
      "Building estimator 2 of 11 for this parallel run (total 220)...\n",
      "Building estimator 3 of 11 for this parallel run (total 220)...\n",
      "Building estimator 4 of 11 for this parallel run (total 220)...\n",
      "Building estimator 5 of 11 for this parallel run (total 220)...\n",
      "Building estimator 6 of 11 for this parallel run (total 220)...\n",
      "Building estimator 7 of 11 for this parallel run (total 220)...\n",
      "Building estimator 8 of 11 for this parallel run (total 220)...\n",
      "Building estimator 9 of 11 for this parallel run (total 220)...\n",
      "Building estimator 10 of 11 for this parallel run (total 220)...\n",
      "Building estimator 11 of 11 for this parallel run (total 220)...\n",
      "Building estimator 1 of 11 for this parallel run (total 220)...\n",
      "Building estimator 2 of 11 for this parallel run (total 220)...\n",
      "Building estimator 3 of 11 for this parallel run (total 220)...\n",
      "Building estimator 4 of 11 for this parallel run (total 220)...\n",
      "Building estimator 5 of 11 for this parallel run (total 220)...\n",
      "Building estimator 6 of 11 for this parallel run (total 220)...\n",
      "Building estimator 7 of 11 for this parallel run (total 220)...\n",
      "Building estimator 8 of 11 for this parallel run (total 220)...\n",
      "Building estimator 9 of 11 for this parallel run (total 220)...\n",
      "Building estimator 10 of 11 for this parallel run (total 220)...\n",
      "Building estimator 11 of 11 for this parallel run (total 220)...\n",
      "Building estimator 1 of 11 for this parallel run (total 220)...\n",
      "Building estimator 2 of 11 for this parallel run (total 220)...\n",
      "Building estimator 3 of 11 for this parallel run (total 220)...\n",
      "Building estimator 4 of 11 for this parallel run (total 220)...\n",
      "Building estimator 5 of 11 for this parallel run (total 220)...\n",
      "Building estimator 6 of 11 for this parallel run (total 220)...\n",
      "Building estimator 7 of 11 for this parallel run (total 220)...\n",
      "Building estimator 8 of 11 for this parallel run (total 220)...\n",
      "Building estimator 9 of 11 for this parallel run (total 220)...\n",
      "Building estimator 10 of 11 for this parallel run (total 220)...\n",
      "Building estimator 11 of 11 for this parallel run (total 220)...\n",
      "Building estimator 1 of 11 for this parallel run (total 220)...\n",
      "Building estimator 2 of 11 for this parallel run (total 220)...\n",
      "Building estimator 3 of 11 for this parallel run (total 220)...\n",
      "Building estimator 4 of 11 for this parallel run (total 220)...\n",
      "Building estimator 5 of 11 for this parallel run (total 220)...\n",
      "Building estimator 6 of 11 for this parallel run (total 220)...\n",
      "Building estimator 7 of 11 for this parallel run (total 220)...\n",
      "Building estimator 8 of 11 for this parallel run (total 220)...\n",
      "Building estimator 9 of 11 for this parallel run (total 220)...\n",
      "Building estimator 10 of 11 for this parallel run (total 220)...\n",
      "Building estimator 11 of 11 for this parallel run (total 220)...\n"
     ]
    }
   ],
   "source": [
    "tit = pd.read_csv('test.csv')\n",
    "tit.drop(list(tit.columns)[1:], axis = 1, inplace=True) \n",
    "tit['Survived'] = pred_stack_rft\n",
    "tit.set_index('PassengerId', inplace=True)\n",
    "tit.to_csv('tit_test(stack).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69976ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
